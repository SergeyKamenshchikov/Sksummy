{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_tests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergeyKamenshchikov/Sksummy/blob/master/ner_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR8FuO7Z9Ebv"
      },
      "source": [
        "!pip3 install spacy==2.1.0\n",
        "!python3 -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvMPB8Ik_aQ5"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4typ0F_W99qL",
        "outputId": "662f4339-a1e5-4ea5-f04d-25a81e164614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import spacy\n",
        "import numpy\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('stopwords')\n",
        "eng_stopwords = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7sd5_88rqgE"
      },
      "source": [
        "CDFs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd_osFRMrpIL"
      },
      "source": [
        "def detokenizer(list_of_tokens):\n",
        "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"â€™\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
        "    return(text_str)\n",
        "\n",
        "class Score:\n",
        "    def __init__(self, f1, precision, recall, wiki,model):\n",
        "        self.f1 = f1\n",
        "        self.precision = precision\n",
        "        self.recall = recall\n",
        "        self.wiki = wiki\n",
        "        self.model = model\n",
        "\n",
        "def has_upper(word):\n",
        "  for c in word:\n",
        "    if c.isalpha():\n",
        "      if c.isupper():\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "def evaluate(n, raw_corpus, labels):\n",
        "\n",
        "  sentences = raw_corpus.split('\\n')\n",
        "  sentences=[sentence for sentence in sentences if len(sentence) > 0]\n",
        "\n",
        "  doc_len=len(sentences)//n\n",
        "\n",
        "  sentences = raw_corpus.split('\\n')\n",
        "  sentences=[sentence for sentence in sentences if len(sentence)>0]\n",
        "  doc_len=144000//n\n",
        "  documents=numpy.array_split(sentences,doc_len)\n",
        "  documents=[document.tolist() for document in documents]\n",
        "  documents=[' '.join(document) for document in documents]\n",
        "  \n",
        "  #get tokens for each document\n",
        "  tokenized_documents=[document.split(' ') for document in documents]\n",
        "\n",
        "\n",
        "  #get labels for each token\n",
        "  fully_labeled_documents=[ [token.split('|') for token in tokenized_document] for tokenized_document in tokenized_documents]\n",
        "\n",
        "  scores=[]\n",
        "  i=0\n",
        " \n",
        "  \n",
        "  nlp = spacy.load('en_core_web_lg', disable=['tagger','parser'])\n",
        "\n",
        "\n",
        "  for document in fully_labeled_documents:\n",
        "\n",
        "    doc=nlp(detokenizer([token[0] for token in document]))    \n",
        "    \n",
        "    spacy_entities=[token.text for token in doc if token.ent_type_ in labels]\n",
        "\n",
        "    corpus_entities=[token[0] for token in document if token[2] in ['I-PER','I-ORG']]\n",
        "\n",
        "    E1=set(corpus_entities)\n",
        "    E2=set(spacy_entities)\n",
        "    intersection=[ner for ner in E1 if ner in E2]\n",
        "    try:\n",
        "      Precision = len(intersection)/len(E2)\n",
        "      Recall = len(intersection)/len(E1)\n",
        "      F1 = 2*Precision*Recall/(Precision + Recall)\n",
        "      scores.append(Score(F1,Precision,Recall,E1,E2))\n",
        "    except:\n",
        "      scores.append(Score(0,0,0,E1,E2)) \n",
        "    \n",
        "  \n",
        "  print('F1:')\n",
        "  print('median = ', round(np.median([score.f1 for score in scores]),2))\n",
        "  print('standard = ', round(np.std([score.f1 for score in scores]),2))\n",
        "\n",
        "  print('\\nPrecision:')\n",
        "  print('median = ', round(np.median([score.precision for score in scores]),2))\n",
        "  print('standard = ', round(np.std([score.recall for score in scores]),2))\n",
        "  \n",
        "  print('\\nRecall:')\n",
        "  print('median = ', round(np.median([score.recall for score in scores]),2))\n",
        "  print('standard = ', round(np.std([score.recall for score in scores]),2))  \n",
        "\n",
        "score_and_len_pairs=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea8ynlHU_xOu"
      },
      "source": [
        "Create model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM7qq5rTTjyN",
        "outputId": "d58b34b0-2203-4c82-fa30-aa764b322127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
        "spacy_nlp._path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/usr/local/lib/python3.6/dist-packages/en_core_web_lg/en_core_web_lg-2.1.0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnCNwrPJ9tIC"
      },
      "source": [
        "Get document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZM1Mput9rUy"
      },
      "source": [
        "htmlString = requests.get('https://en.wikipedia.org/wiki/Higgs_boson')\n",
        "soup = BeautifulSoup(htmlString.content, 'html.parser')\n",
        "paragraphs = soup.find_all(['p', 'article', 'section', 'title', 'h1', 'h2', 'h3'])\n",
        "rawtext = ' '.join([i.text for i in paragraphs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQaqDXNGGiMq"
      },
      "source": [
        "Get entities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfNkzOOzDI_t",
        "outputId": "a7f14ef0-57a8-41ff-9830-a47ddbc37f8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "doc=spacy_nlp(rawtext)\n",
        "\n",
        "i=0\n",
        "ner_list = []\n",
        "\n",
        "for ent in doc.ents:\n",
        "  if ent.label_ in ['ORG', 'PERSON']:\n",
        "    ner_list.append(ent.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.52 s, sys: 154 ms, total: 1.68 s\n",
            "Wall time: 1.68 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOQuKZ5ZJeN6"
      },
      "source": [
        "Print top entities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm-AthFxGsi4",
        "outputId": "7c836867-4ec6-466f-9923-921fb207f12c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ners = dict(Counter(ner_list))\n",
        "ners = sorted(ners.items(), key=lambda x: x[1], reverse=True)\n",
        "ners = [i[0] for i in ners][:10]\n",
        "\n",
        "for i in ners:\n",
        "  print('+', i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ Higgs\n",
            "+ CERN\n",
            "+ LHC\n",
            "+ CMS\n",
            "+ ATLAS\n",
            "+ GHK\n",
            "+ Goldstone\n",
            "+ Lederman\n",
            "+ SU(2\n",
            "+ Peter Higgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbAWpxRV5Ka0"
      },
      "source": [
        "Get wikipedia marked corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opewJN_11zhZ"
      },
      "source": [
        "!wget https://github.com/dice-group/FOX/raw/master/input/Wikiner/aij-wikiner-en-wp2.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWwPZ1lPocCc"
      },
      "source": [
        "Unzip file here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI7titqo4XCa"
      },
      "source": [
        "!bzip2 -d /content/aij-wikiner-en-wp2.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BY1PEvNqBZm"
      },
      "source": [
        "Read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6POnXioI4sPS"
      },
      "source": [
        "f = open('/content/aij-wikiner-en-wp2', 'r')\n",
        "raw_corpus = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LEWjfA-Er_h"
      },
      "source": [
        "Get model metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwDFfe7oSz8v",
        "outputId": "50c0cc93-cf21-4b99-c654-fe4b4b58eb1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "score_and_len_pairs = evaluate(300, raw_corpus,['PERSON','ORG'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1:\n",
            "median =  0.79\n",
            "standard =  0.06\n",
            "\n",
            "Precision:\n",
            "median =  0.73\n",
            "standard =  0.04\n",
            "\n",
            "Recall:\n",
            "median =  0.87\n",
            "standard =  0.04\n",
            "CPU times: user 3min 7s, sys: 3.02 s, total: 3min 10s\n",
            "Wall time: 3min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}