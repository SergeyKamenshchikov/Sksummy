{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T12:34:23.555276Z",
     "start_time": "2020-09-29T12:34:23.532339Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from attention import AttentionLayer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "import winsound\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T14:56:15.318668Z",
     "start_time": "2020-09-29T14:56:15.300717Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'end'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString = newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T11:32:50.807744Z",
     "start_time": "2020-09-29T11:32:50.803755Z"
    }
   },
   "outputs": [],
   "source": [
    "max_text_len = 30\n",
    "max_summary_len = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T11:02:36.057458Z",
     "start_time": "2020-09-29T11:02:35.096986Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reviews/review.csv\",nrows=100000)\n",
    "df = df[['Text','Summary']]\n",
    "df = df.rename(columns={'Text': 'text', 'Summary': 'summary'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T11:02:44.093957Z",
     "start_time": "2020-09-29T11:02:43.872407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                summary\n",
       "0  I have bought several of the Vitality canned d...  Good Quality Dog Food\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      Not as Advertised"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add start and end tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T11:02:49.246049Z",
     "start_time": "2020-09-29T11:02:49.187202Z"
    }
   },
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'start '+ x + ' end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into the train and test parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T11:43:58.514860Z",
     "start_time": "2020-09-29T11:43:58.462999Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(np.array(df['text']), np.array(df['summary']), \n",
    "                                            test_size=0.1, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T11:44:13.585581Z",
     "start_time": "2020-09-29T11:44:00.972143Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer(num_words=100) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T11:44:20.640056Z",
     "start_time": "2020-09-29T11:44:17.721702Z"
    }
   },
   "outputs": [],
   "source": [
    "y_tokenizer = Tokenizer(num_words=100) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T13:28:45.272996Z",
     "start_time": "2020-09-29T13:28:43.539634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 100)      10100       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 30, 300), (N 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 30, 300), (N 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    10100       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 30, 300), (N 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 101)    60701       concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,666,001\n",
      "Trainable params: 2,666,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "enc_emb =  Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complile and introduce early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T13:28:52.911261Z",
     "start_time": "2020-09-29T13:28:52.825381Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T14:23:34.118826Z",
     "start_time": "2020-09-29T13:30:07.394939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79582 samples, validate on 8843 samples\n",
      "Epoch 1/2\n",
      "79582/79582 [==============================] - 1678s 21ms/sample - loss: 1.4033 - val_loss: 1.2871\n",
      "Epoch 2/2\n",
      "79582/79582 [==============================] - 1528s 19ms/sample - loss: 1.2526 - val_loss: 1.2247\n",
      "Wall time: 53min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:], epochs=2,\n",
    "                    callbacks=[es], batch_size=128, validation_data=([x_val,y_val[:,:-1]], \n",
    "                                                                    y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T14:49:02.158080Z",
     "start_time": "2020-09-29T14:49:02.151097Z"
    }
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T14:55:43.840898Z",
     "start_time": "2020-09-29T14:55:43.253402Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T14:56:30.097479Z",
     "start_time": "2020-09-29T14:56:18.512443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: is not only to use it only but is great the have just the of they up \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: to my the food if you have a about in that is some had and some other by them in a it was a but the like i to on \n",
      "Original summary: excellent product not \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: coffee is but not make a of coffee even when you the of \n",
      "Original summary: \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: when in this i of it but of them for the i was and they be i them so i them on and very with the product \n",
      "Original summary: one of my favorite \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i'm am that i was to find these the were in and are what a find i be \n",
      "Original summary: of \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i don't like it at all it very to me i in for a and to love if you are for the taste of coffee this is not it or \n",
      "Original summary: not to \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: good i i like the when they are in this the are but all the a and well \n",
      "Original summary: nice \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: you up for and the price is a than and even food br br br br br br br it's br are for a or to br br br more \n",
      "Original summary: great for a easy \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this was the only food my were to if it for the that they are food i would be it for them and i at some i can \n",
      "Original summary: it \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: all i just like it but all in all i do and have this tea to a and do and because i love the taste and the i'm with it \n",
      "Original summary: \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: we were at but love them we have a to are also in love will to as as the price \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: it just a like taste in the it was just too much i it and up it out br br i would not this you are a and even be \n",
      "Original summary: too bad not for a \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i love this product the them and they i have up for because i them \n",
      "Original summary: great for \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: better of a br they to very as to and for br br taste is not too so to be a all br br as with you we no on \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: these are the for my it them and they have had with \n",
      "Original summary: favorite treat for my \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: good tea i with a is to and so is a good get it on so you out and \n",
      "Original summary: great tea \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: great but i can buy it for this price in the not all are on you \n",
      "Original summary: taste is great price is a \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: but it's so well that was i was that so after if a of and have of some of this if you don't have you it but make you and \n",
      "Original summary: this stuff is so good that it's \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: for for dog of product you get to the on food for a \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: like that are and at a great price the at about \n",
      "Original summary: for my favorite \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i the other at a up so i them of it was it not out or more br br i was very very by this and will not buy it \n",
      "Original summary: a of for \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: like a but i find it is to have a good in the they in that very a time br br the price on amazon is the best i have \n",
      "Original summary: these \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this is coffee but i'm not all the other about it it is not and has some the are also a which is a to me as well \n",
      "Original summary: just ok coffee \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: the that i use it for but use and i don't and it the for me to i'm a br br so and this and it in all of them \n",
      "Original summary: i with it \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i really to like these i just don't like and that is what it like don't your i have the \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: flavor as chocolate and the were too with the the are too too and too to it up br br i would this product more and it would even better \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i love love love this i use it for and i it from amazon i have it on to all my and they love it too \n",
      "Original summary: a have \n",
      "Predicted summary:  great end\n",
      "\n",
      "\n",
      "Review: this is very and i i'm to like it because i love but i when my will it to me and me to for with it \n",
      "Original summary: in \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: these are great for i have and i love that these are in i them in because they get more than one they like them as much as the \n",
      "Original summary: great treat for dogs \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: a it really is to br br as a you can and just all the a and just use it as a a of can from it but not an \n",
      "Original summary: perfect for but \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: are great on the dog food the dog food to be about food but we have not had this with the we had with or the are a to of \n",
      "Original summary: dog food \n",
      "Predicted summary:  my dog loves it end\n",
      "\n",
      "\n",
      "Review: i have tried and this is my for a coffee some and it is a great to my i it a and my more \n",
      "Original summary: my favorite \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: a flavor but or too to find to of other and this out and is to find i get it on the other by are not as good in my \n",
      "Original summary: best ever \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: a that i have in other coffee i am by coffee and up this is my of this product from this great it from the it's it it like it \n",
      "Original summary: so and taste of the \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: tea is an product i love tea with it it is i it great i it \n",
      "Original summary: love this \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: i chocolate but love chocolate i was to and find chocolate and flavor no chocolate a on the a like the and \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: my the better than dog and the price was great to get them amazon to my \n",
      "Original summary: better than dog treats \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: good about this tea because it is a tea that is from tea it's as well as also this tea can be so this tea is also very be more \n",
      "Original summary: but very \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: use the and at but if the is it not be from so when it to this product and the of and in the of the all i can is \n",
      "Original summary: good but too \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: for br the flavor and the amazon the on the so i had to my and a little more but as we we have to do what the to do \n",
      "Original summary: the food my cat \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: and was not about that not too about a time in on the too the is a also can we some more to the amazon you to be more more \n",
      "Original summary: good with \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: about this and all the great and they are all it me of if i a and don't it really it is to make and i even this by the \n",
      "Original summary: my \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: the for a to the or the and can be to get some good your br br if for more than the taste they are than my to food in \n",
      "Original summary: and a for this \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: br i this you your dog like a he it or if it's for a dog that there are other that are much better i the and my dog it \n",
      "Original summary: \n",
      "Predicted summary:  my dog loves these end\n",
      "\n",
      "\n",
      "Review: and this one is up there with my of it's to on the taste is good because so much all as as the of the product i no this and \n",
      "Original summary: coffee tastes better you good it \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: tea it is like tea this from amazon is no more than tea at the i after get of it it after it in not just or with and the \n",
      "Original summary: have tea so good in your \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: that is of the i to what is in the food i my it is to find dog food that have in it and also this food an to the \n",
      "Original summary: my dog loves this stuff \n",
      "Predicted summary:  great food end\n",
      "\n",
      "\n",
      "Review: i have this at my food taste so good my is with and he was so to have the \n",
      "Original summary: what a great \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: not what the about these is one you buy a be \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i the taste of the and which is not but has more flavor the are this which is a good it is with \n",
      "Original summary: easy more flavor \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this is the best you will have the that you to as well are the you \n",
      "Original summary: popcorn in the \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: you more the are on the of the no what you do but just be br br get a better or they use and the if it for it for \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: other it the if you on a which i the of the is too for i would have this of had my just a is all great in that of \n",
      "Original summary: yummy \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: and i them the only with a is that you them in to them if you are not to them these are also to and a on them to as \n",
      "Original summary: and \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: these are and with a flavor they great with a of coffee for a or an \n",
      "Original summary: tasty \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: was dog food after food he to get a on it at because it and i was a than i i was very on this food it's a good product \n",
      "Original summary: dog food \n",
      "Predicted summary:  my dog loves it end\n",
      "\n",
      "\n",
      "Review: there we get them at to my only for them is that them a but my my are they the there is not even a out there not them more \n",
      "Original summary: better \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: to it the have been there is no i as well be this is a that flavor is only a i am up on chocolate and will flavor from on \n",
      "Original summary: what my favorite coffee \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: is by the best buy for in the from in i a for that taste great the price be this is my of these and be the \n",
      "Original summary: best buy great flavor \n",
      "Predicted summary:  great end\n",
      "\n",
      "\n",
      "Review: as good as in the and in good for a \n",
      "Original summary: just \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i use this for and for it has a very flavor it like but is not \n",
      "Original summary: great flavor and quality \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: at price a of there were to a in a of time in the that is for by so i it was i i about the great on at amazon \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: my is a of me not so much br but when i tried these i a also br the are and with great flavor \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this is a product and it's a good price with and i will be this \n",
      "Original summary: yummy \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this product is good but it was too for me i it and it i don't or food if so this me the br br if you food like this \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i have a i'm with so these great on and a great price \n",
      "Original summary: love \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: be well not so that i'm to it but i that be br br of it and it on you get that it really taste like and it's but will \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: to a at love the chocolate and this is the from love my by it in and just good to on \n",
      "Original summary: love \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this all of the other it and other that it a flavor a little or of to the it in a \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: these are chocolate and and make a the are but it is a little and the these br br has of with of and of br br br br \n",
      "Original summary: good snack \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: if you food or up food this product is for you it make much and a price it is much than in when you can find it \n",
      "Original summary: great \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: we a of these at a to if dog them them they are to other dog but they do a good with and as them \n",
      "Original summary: nice \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i this as a of my and was very i for a great to in your and taste great i it be too at but i up it \n",
      "Original summary: be \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: tea in a when you this there is a of and the tea not will buy \n",
      "Original summary: \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: the time i it in and all for the time br br what can i it be very and they have of other that make this food very br br \n",
      "Original summary: tasty \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: you this is it's with and it was the time and in \n",
      "Original summary: it's gluten free \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: but he he it the was but it well have been that the or and one be than not that i would on a but they were all in my \n",
      "Original summary: my them \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: but it good the can be in tried that i have with it out and it i this will be very and so will love the price for a product \n",
      "Original summary: best i have \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i really love this of coffee i am a coffee and this it is and and great you can not with this \n",
      "Original summary: i love this of coffee \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: i love all and this is just for me i like some or on the it's not like other i just love them \n",
      "Original summary: just so good \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this chocolate is very good it has just the of chocolate flavor the price is a very good and more than it \n",
      "Original summary: very good hot chocolate \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: good i'm a little about the is it as for as it is for br br even the to like them i a to to the i them too much \n",
      "Original summary: ever \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: with the but i am i will it out some good on \n",
      "Original summary: great \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: we this coffee br to be and for an it all the buy it \n",
      "Original summary: \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: also like that there are of coffee it the coffee i would that you it a i have other and for the this is very to like i it a \n",
      "Original summary: great cup of coffee \n",
      "Predicted summary:  coffee end\n",
      "\n",
      "\n",
      "Review: it is in the as as price is but for the price that you in amazon you get a of i am with this product that i will be more \n",
      "Original summary: delicious and a to healthy \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i find these i and so i to amazon to them they were and me that i to for them i will these \n",
      "Original summary: as i them love \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: my dog this it is a great product to and by the he it it taste good too \n",
      "Original summary: good for the dog \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: we in and we the of the of so not in chocolate too it was a to me these the and it's to if only these would or \n",
      "Original summary: chocolate \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: we have this and love it the are great to a and the is on no or for to and this and \n",
      "Original summary: great for \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i'm this from to this product to be and the have the the is and it's \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: this tea is a good tea i'm not much of a tea but this is the best tea had i'm a of but this one is my \n",
      "Original summary: great tea \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: really these if you have a or a dog i them little has a and so i to this not a little product your dog is to really get them \n",
      "Original summary: at \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: the or other but is of i have to it is a not be the for all but it has me a of time and in to the of this \n",
      "Original summary: for cat \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: tea is good for to the of of the very good price and would buy it \n",
      "Original summary: tea \n",
      "Predicted summary:  tea end\n",
      "\n",
      "\n",
      "Review: and they are all of and flavor a that is not to with and or it is a of when i don't have time to make my i you these \n",
      "Original summary: delicious and all what more you \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: i was at good these are also it has good and you can taste the it \n",
      "Original summary: delicious with great \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: great for make a with and a little on and with \n",
      "Original summary: good stuff \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: to this flavor in the so had to on price was the \n",
      "Original summary: \n",
      "Predicted summary:  end\n",
      "\n",
      "\n",
      "Review: would i up for in the the were more and this i don't even to to \n",
      "Original summary: what i \n",
      "Predicted summary:  end\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
