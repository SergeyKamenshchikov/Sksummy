{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shared_metasearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQRJz3Qq6YhT"
      },
      "source": [
        "##### Update drivers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PL9ZRULpXlK",
        "outputId": "68fc9ebb-4e51-47c9-a86d-85976debc96e"
      },
      "source": [
        "%%time\r\n",
        "!apt update"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,903 kB]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [289 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,383 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [549 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,718 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,334 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [879 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,154 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [42.6 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [319 kB]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [49.2 kB]\n",
            "Fetched 11.9 MB in 3s (3,568 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "CPU times: user 37.6 ms, sys: 14.2 ms, total: 51.8 ms\n",
            "Wall time: 5.86 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMzQ9frW2ypn"
      },
      "source": [
        "##### Install libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zyssm0Sn1Un",
        "outputId": "3b310e99-7c0f-44ba-e733-f1afc88b7724"
      },
      "source": [
        "%%time\n",
        "!pip3 install fuzzy-pandas\n",
        "!pip3 install webdriver_manager\n",
        "!pip3 install arxiv\n",
        "!pip3 install wikipedia-api\n",
        "!pip3 install tika\n",
        "!pip3 install pdfminer\n",
        "!pip3 install spacy==2.1.0\n",
        "!python3 -m spacy download en_core_web_lg\n",
        "!pip3 install neuralcoref --no-binary neuralcoref\n",
        "!pip3 install python-docx\n",
        "!pip3 install sumy\n",
        "!pip3 install yake\n",
        "!pip3 install transformers\n",
        "!pip3 install textstat\n",
        "!pip3 install rouge\n",
        "!pip3 install rouge-score\n",
        "!pip3 install sentencepiece\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzy-pandas\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/70/c9de848dea88eb02ecb7b4993d188789fe35857f5b3b6dc616d957c55769/fuzzy_pandas-0.1-py3-none-any.whl\n",
            "Collecting csvmatch\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/76/3ca79293188c99c509bc6d31aa44e2766f985967b892cb27b96357f17a47/csvmatch-1.20-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fuzzy-pandas) (1.1.5)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from csvmatch->fuzzy-pandas) (3.0.4)\n",
            "Collecting colorama==0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting doublemetaphone==0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/27/8df369334aac64755ca899b9a7cc4d2d60e800cca148322ef19309cdae0f/DoubleMetaphone-0.1-cp36-cp36m-manylinux1_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.3MB/s \n",
            "\u001b[?25hCollecting jellyfish==0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/80/bcacc7affb47be7279d7d35225e1a932416ed051b315a7f9df20acf04cbe/jellyfish-0.7.2.tar.gz (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 6.4MB/s \n",
            "\u001b[?25hCollecting unidecode==1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.2MB/s \n",
            "\u001b[?25hCollecting dedupe==1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/aa/23882548dd54775f3ce9b2eab2abe5c5fa2d1af8fe3f60c17ea6c5b7d990/dedupe-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.5MB/s \n",
            "\u001b[?25hCollecting tqdm==4.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/cb/a12e63ce28a0db97580fc5d18085281cd3d5393acc6d18e0aa9c3dcdd0ee/tqdm-4.18.0-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->fuzzy-pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->fuzzy-pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fuzzy-pandas) (2018.9)\n",
            "Collecting dedupe-variable-datetime\n",
            "  Downloading https://files.pythonhosted.org/packages/65/8f/d21f6acadcdfd681ee038153883b5673b8b76f790e465d791780e6b7bf60/dedupe_variable_datetime-0.1.5-py3-none-any.whl\n",
            "Collecting categorical-distance>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/b7/4f97771f52c63916f4e4d349a644c2387961592e76070e7310463b2d70a5/categorical_distance-1.9-py3-none-any.whl\n",
            "Collecting highered>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/81/00/cbd902cfd14ad1992fcdaa11a615d47b36b6136dc690e19b0afa58c7365d/highered-0.2.1-py2.py3-none-any.whl\n",
            "Collecting rlr>=2.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/02/3b1a9727a622ff4320919645ce35ceb887d90784d0bab41484756c33b7ea/rlr-2.4.5-py2.py3-none-any.whl\n",
            "Collecting fastcluster\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/0a/f408a98de142cf9b3b8721c6d6529aee775bc8a89a97a6388ae9201d3dd8/fastcluster-1.1.27-cp36-cp36m-manylinux1_x86_64.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 8.4MB/s \n",
            "\u001b[?25hCollecting affinegap>=1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/6a/91f5defe8178104449bc897208c9780b159575d16a959a5074f0bf39a6f0/affinegap-1.11-cp36-cp36m-manylinux1_x86_64.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hCollecting simplecosine>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/22/6ea3a5ab8aea06d6563eb927e706f7342a00d1849c9be6143a2a7d84ddbd/simplecosine-1.2-py2.py3-none-any.whl\n",
            "Collecting haversine>=0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/52/a13286844780c7b1740edbbee8a8f0524e2a6d51c068b59dda39a6a119f5/haversine-2.3.0-py2.py3-none-any.whl\n",
            "Collecting zope.index\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/05/16849c612ae7aaa6c33d1dab769593a9fe10868d4b25130970b5cbc2d750/zope.index-5.0.0-cp36-cp36m-manylinux2010_x86_64.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.6MB/s \n",
            "\u001b[?25hCollecting BTrees>=4.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/ba/5094e7ce0d9f104f478dc66dc122f29bfa4097ece1cb162efc3c98a4637c/BTrees-4.7.2-cp36-cp36m-manylinux2010_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 8.4MB/s \n",
            "\u001b[?25hCollecting Levenshtein-search\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/89/dc320196d10447540c95f58eab5dd316a2166310356c1d88b84724f4e793/Levenshtein_search-1.4.5-cp36-cp36m-manylinux1_x86_64.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n",
            "\u001b[?25hCollecting dedupe-hcluster\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/1f/c6f6075c2e988b3a1759fabaf91d2f8f2de59c6e607a3fd9a2e06112a0de/dedupe_hcluster-0.3.8-cp36-cp36m-manylinux1_x86_64.whl (531kB)\n",
            "\u001b[K     |████████████████████████████████| 532kB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->fuzzy-pandas) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from dedupe-variable-datetime->dedupe==1.10.0->csvmatch->fuzzy-pandas) (0.16.0)\n",
            "Collecting datetime-distance\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/98/a5eff9256ff27e3bb8030466dabd772002e5014b9237cbeb18c542050ff5/datetime_distance-0.1.3-py3-none-any.whl\n",
            "Collecting pyhacrf-datamade>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/f5/971e17a8b6686d5fc3d562e29e9c902743eb5f0f4436880b86cb11c0149c/pyhacrf_datamade-0.2.5-cp36-cp36m-manylinux1_x86_64.whl (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 41.1MB/s \n",
            "\u001b[?25hCollecting pylbfgs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/5b/b8e1ef62e5e5b034ce5ae919b64158ec8da4f64c995444aec7fd96e8ec42/PyLBFGS-0.2.0.13-cp36-cp36m-manylinux1_x86_64.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 44.4MB/s \n",
            "\u001b[?25hCollecting persistent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/bf/b581dfae685172ede99b4caa2f5afe899129987bcbd5f976296578f5715c/persistent-4.6.4-cp36-cp36m-manylinux2010_x86_64.whl (246kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 50.8MB/s \n",
            "\u001b[?25hCollecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.index->dedupe==1.10.0->csvmatch->fuzzy-pandas) (51.3.3)\n",
            "Requirement already satisfied: cffi; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from persistent->zope.index->dedupe==1.10.0->csvmatch->fuzzy-pandas) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi; platform_python_implementation == \"CPython\"->persistent->zope.index->dedupe==1.10.0->csvmatch->fuzzy-pandas) (2.20)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.7.2-cp36-cp36m-linux_x86_64.whl size=72997 sha256=9fb2bebbec9c3edbc032345659f713f3ff4c92801be67b7e76282ce7112053cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/fe/99/d8fa8f2ef7b82a625b0b77a84d319b0b50693659823c4effb4\n",
            "Successfully built jellyfish\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.18.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.18.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: colorama, doublemetaphone, jellyfish, unidecode, datetime-distance, dedupe-variable-datetime, categorical-distance, pylbfgs, pyhacrf-datamade, highered, rlr, fastcluster, affinegap, simplecosine, haversine, zope.interface, persistent, BTrees, zope.index, Levenshtein-search, dedupe-hcluster, dedupe, tqdm, csvmatch, fuzzy-pandas\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed BTrees-4.7.2 Levenshtein-search-1.4.5 affinegap-1.11 categorical-distance-1.9 colorama-0.4.3 csvmatch-1.20 datetime-distance-0.1.3 dedupe-1.10.0 dedupe-hcluster-0.3.8 dedupe-variable-datetime-0.1.5 doublemetaphone-0.1 fastcluster-1.1.27 fuzzy-pandas-0.1 haversine-2.3.0 highered-0.2.1 jellyfish-0.7.2 persistent-4.6.4 pyhacrf-datamade-0.2.5 pylbfgs-0.2.0.13 rlr-2.4.5 simplecosine-1.2 tqdm-4.18.0 unidecode-1.1.1 zope.index-5.0.0 zope.interface-5.2.0\n",
            "Collecting webdriver_manager\n",
            "  Downloading https://files.pythonhosted.org/packages/32/28/a4e7638fc497ff8f86c6670a5f9f42dc018c37a0b254caa5e51799959da5/webdriver_manager-3.3.0-py2.py3-none-any.whl\n",
            "Collecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from webdriver_manager) (2.23.0)\n",
            "Collecting crayons\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/0d/e3fad4ca1de8e70e06444e7d777a5984261e1db98758b5be3e8296c03fe9/crayons-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->webdriver_manager) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->webdriver_manager) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->webdriver_manager) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->webdriver_manager) (2.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from crayons->webdriver_manager) (0.4.3)\n",
            "Installing collected packages: configparser, crayons, webdriver-manager\n",
            "Successfully installed configparser-5.0.1 crayons-0.4.0 webdriver-manager-3.3.0\n",
            "Collecting arxiv\n",
            "  Downloading https://files.pythonhosted.org/packages/50/81/9714d5a4efc14edddb308c0b527fe2d9ac35840fcfa83684a52655d35d42/arxiv-0.5.3-py3-none-any.whl\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from arxiv) (2.23.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (2.10)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6066 sha256=6263cfee76fe239f93a5b6a2e6ee1ac41c9b812ba4e668110eced4220617d509\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-0.5.3 feedparser-6.0.2 sgmllib3k-1.0.0\n",
            "Collecting wikipedia-api\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/3d/289963bbf51f8d00cdf7483cdc2baee25ba877e8b4eb72157c47211e3b57/Wikipedia-API-0.5.4.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from wikipedia-api) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-cp36-none-any.whl size=13462 sha256=413843de5477af8b34709578c3d3621c7351d295c5370485f721bb855746668f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/40/42/ba1d497f3712281b659dd65b566fc868035c859239571a725a\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.4\n",
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (51.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.12.5)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32885 sha256=4a0d3b5c31c211a1ff4e9ebfbb9ad46b2c0848fd8b3299434bb0b23bbf0a0185\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n",
            "Collecting pdfminer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/a3/155c5cde5f9c0b1069043b2946a93f54a41fd72cc19c6c100f6f2f5bdc15/pdfminer-20191125.tar.gz (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 4.2MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/6f/7e38d7c97fbbc3987539c804282c33f56b6b07381bf2390deead696440c5/pycryptodome-3.9.9-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 313kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-cp36-none-any.whl size=6140083 sha256=b2fa768da03e9e70f4840099f33f1a2281c740b129d60b6226bbfecb4ce42871\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/00/af/720a55d74ba3615bb4709a3ded6dd71dc5370a586a0ff6f326\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.9.9\n",
            "Collecting spacy==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/39/4bde5da5f18ab0bdd525760c4fe38808b4bb03907a2aea094000d831afe1/spacy-2.1.0-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 158kB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 29.7MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.18.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.12.5)\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blis, preshed, plac, thinc, spacy\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed blis-0.2.4 plac-0.9.6 preshed-2.0.1 spacy-2.1.0 thinc-7.0.8\n",
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 1.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255077 sha256=3c200d38da9b2a70142175033e722bc1a28a8146992ccfb9a4a3786afe9d5ea4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qtlbb8gu/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Collecting neuralcoref\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/40/8db3db763077fe80b71859f57731261aeb03cc624635f97a3bcfe55ab37b/neuralcoref-4.0.tar.gz (368kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/eb/84794204fff2d6234da730da7b1cc52effea7b701cc4e51b0394b749754e/boto3-1.16.62-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.1.0)\n",
            "Collecting botocore<1.20.0,>=1.19.62\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/29/4bb69158274cd87632ecea00efa95d08f3f137de799170ab9f7651abaac4/botocore-1.19.62-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 8.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.5)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.62->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.62->boto3->neuralcoref) (1.15.0)\n",
            "Skipping wheel build for neuralcoref, due to binaries being disabled for it.\n",
            "\u001b[31mERROR: botocore 1.19.62 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "    Running setup.py install for neuralcoref ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully installed boto3-1.16.62 botocore-1.19.62 jmespath-0.10.0 neuralcoref-4.0 s3transfer-0.3.4\n",
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=66cf992b333c7978ef52341fab5af6ad127f3da944d0a955612402138b7a4673\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.10\n",
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy) (2.23.0)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy) (3.2.5)\n",
            "Collecting pycountry>=18.2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 18.1MB/s \n",
            "\u001b[?25hCollecting breadability>=0.1.20\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n",
            "Building wheels for collected packages: pycountry, breadability\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746863 sha256=75335b5ec31bf2368361c4c9a9258d3d5527bfdd489810bab18f9fdbe59e6821\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21683 sha256=c4fdef079111cc55d0c8830bacf65b449a0c3653e88007c62bfd0b800ea94214\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "Successfully built pycountry breadability\n",
            "Installing collected packages: pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n",
            "Collecting yake\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/c9/998809d67c7a6f1faba5e97b93f7deec5483a1e510443ca2892959041bb4/yake-0.4.3-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from yake) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from yake) (0.8.7)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.6/dist-packages (from yake) (7.1.2)\n",
            "Collecting segtok\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.6/dist-packages (from yake) (0.7.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from yake) (2.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from segtok->yake) (2019.12.20)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->yake) (4.4.2)\n",
            "Building wheels for collected packages: segtok\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25019 sha256=6b3af5ceef2cdc12f1156250f9924fb93a40495613cc30ad11c1ca2fa6d19b71\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "Successfully built segtok\n",
            "Installing collected packages: segtok, yake\n",
            "Successfully installed segtok-1.5.10 yake-0.4.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tqdm>=4.27\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0bc45ba127718a6bcc9a5bda80e38bb4b9af3e4fcc406ea4d206347ab9c21654\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: csvmatch 1.20 has requirement tqdm==4.18.0, but you'll have tqdm 4.56.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm, sacremoses, tokenizers, transformers\n",
            "  Found existing installation: tqdm 4.18.0\n",
            "    Uninstalling tqdm-4.18.0:\n",
            "      Successfully uninstalled tqdm-4.18.0\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 tqdm-4.56.0 transformers-4.2.2\n",
            "Collecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/b1/ab40a00b727a0d209402d1be6aa3f1bc75bd03678b59ace8507b08bf12f5/textstat-0.7.0-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.7MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.10.0 textstat-0.7.0\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge-score) (0.10.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 81.0 MB of archives.\n",
            "After this operation, 273 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\n",
            "Fetched 81.0 MB in 3s (24.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 146374 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "CPU times: user 2.18 s, sys: 607 ms, total: 2.78 s\n",
            "Wall time: 6min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn6jBAVr6kA7"
      },
      "source": [
        "##### Restart and import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j02E0CGKnkPq",
        "scrolled": true,
        "outputId": "2927e3e8-01cf-49c4-fbda-43654dd42448"
      },
      "source": [
        "# general\n",
        "import time, random\n",
        "from random import randint\n",
        "# general\n",
        "\n",
        "# process arrays and dataframes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import fuzzy_pandas as fpd\n",
        "from collections import Counter\n",
        "#/process arrays and dataframes\n",
        "\n",
        "# parallel calculations\n",
        "from tqdm import tqdm\n",
        "#/parallel calculations\n",
        "\n",
        "# web driver\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "#/web driver\n",
        "\n",
        "# web parsing\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Tag\n",
        "from requests import get\n",
        "#/web parsing\n",
        "\n",
        "# parsing libs\n",
        "import arxiv\n",
        "import wikipediaapi\n",
        "from googlesearch import search  \n",
        "#/parsing libs\n",
        "\n",
        "# read .pdf\n",
        "from tika import parser\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from io import StringIO\n",
        "#/read .pdf\n",
        "\n",
        "# text processing\n",
        "import spacy,nltk,string,re\n",
        "import neuralcoref\n",
        "import networkx as nx\n",
        "from spacy.symbols import nsubj, nsubjpass, VERB\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from more_itertools import unique_everseen\n",
        "from textblob import TextBlob\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "nlp.max_length = 50000000\n",
        "#/text processing\n",
        "\n",
        "# create .docx\n",
        "import docx\n",
        "from docx import Document\n",
        "from docx.shared import Cm\n",
        "from docx.shared import Pt\n",
        "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.shared import Pt\n",
        "#/create .docx\n",
        "\n",
        "# keywords extraction\n",
        "import yake\n",
        "#/keywords extraction\n",
        "\n",
        "# extractive summarizer\n",
        "from sumy.parsers.html import HtmlParser\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "#/extractive summarizer\n",
        "\n",
        "# abstractive summarizer\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import textstat\n",
        "import torch\n",
        "#/abstractive summarizer\n",
        "\n",
        "# many-to-many evaluation\n",
        "from rouge import Rouge\n",
        "from rouge_score import rouge_scorer\n",
        "#/many-to-many evaluation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40155833/40155833 [00:01<00:00, 39356919.17B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceKg96nOnkPt"
      },
      "source": [
        "##### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmNiV_sznkPu"
      },
      "source": [
        "UPLOAD_FOLDER = '/'\n",
        "\n",
        "threshold = 0.8\n",
        "rouge_limit = 0.5\n",
        "\n",
        "page_number = 10\n",
        "max_length = 60"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF06EzbxnkPu"
      },
      "source": [
        "##### Feature toggles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhTtAkBnkPv"
      },
      "source": [
        "filter_request = False\n",
        "paraphrase = False\n",
        "compress = True\n",
        "\n",
        "wiki_sum = True\n",
        "gogle_sum = True\n",
        "arxiv_sum = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxs0ONCSnkPv"
      },
      "source": [
        "##### CDFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cepweVInkPv"
      },
      "source": [
        "##### HTML parsing #####\n",
        "def parse_google_page(url): \n",
        "    try:\n",
        "        LANGUAGE = \"english\"\n",
        "        title = BeautifulSoup(get(url).content, 'html.parser').title.getText()\n",
        "        parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
        "        \n",
        "        summarizer = Summarizer(Stemmer(LANGUAGE))\n",
        "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "        sentences = []\n",
        "        for i in summarizer(parser.document, 1000000):\n",
        "            sentences.append(str(i))\n",
        "        txt = ' '.join(sentences)\n",
        "    except:\n",
        "        txt = ''\n",
        "        title = ''\n",
        "    \n",
        "    return txt, title\n",
        "\n",
        "def striphtml(data):\n",
        "    p = re.compile(r'<.*?>')\n",
        "    return p.sub('', data)\n",
        "\n",
        "def get_unique_text(document):\n",
        "    unique_sentences = []\n",
        "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
        "        if sentence not in unique_sentences:\n",
        "            unique_sentences.append(sentence)\n",
        "    return ' '.join(unique_sentences)\n",
        "\n",
        "def get_text(url):\n",
        "    page = urlopen(url)\n",
        "    soup = BeautifulSoup(page)\n",
        "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
        "    return fetched_text\n",
        "#####/HTML parsing #####\n",
        "\n",
        "############# Parse Wiki ############# \n",
        "def parse_wiki(google_url):\n",
        "    \n",
        "    # load driver\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    #/load driver\n",
        "    \n",
        "    # get urls  \n",
        "    driver.get(google_url)\n",
        "    time.sleep(randint(1,5))\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
        "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
        "\n",
        "    links = []\n",
        "    titles = []\n",
        "    errors = []\n",
        "\n",
        "    descriptions = []\n",
        "    for r in result_div:\n",
        "        try:\n",
        "            link = r.find('a', href=True)\n",
        "            title = None\n",
        "            title = r.find('h3')\n",
        "\n",
        "            if isinstance(title,Tag):\n",
        "                title = title.get_text()\n",
        "\n",
        "            description = None\n",
        "            description = r.find('span', attrs={'class': 'st'})\n",
        "\n",
        "            if isinstance(description, Tag):\n",
        "                description = description.get_text()\n",
        "\n",
        "            if link != '' and title != '' and description != '':\n",
        "                links.append(link['href'])\n",
        "                titles.append(title)\n",
        "                descriptions.append(description)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "    url_list = links[:(page_number)]\n",
        "    url_list = [i for i in url_list if 'https://en.wikipedia.org' in i]\n",
        "    \n",
        "    title_list = []\n",
        "    for i in url_list:\n",
        "        try:\n",
        "            if 'https://en.wikipedia.org' in i: \n",
        "                title_list.append(i.split('/')[4])  \n",
        "        except:\n",
        "            continue\n",
        "    #/ get urls\n",
        "        \n",
        "    driver.stop_client()\n",
        "    driver.close()\n",
        "    \n",
        "    return title_list     \n",
        "############# Parse Wiki ##############\n",
        "\n",
        "############# Parse Arxiv #############\n",
        "def parse_arxiv(query):\n",
        "    \n",
        "    arxivtext = ''  \n",
        "    \n",
        "    urls = []\n",
        "    titles = []\n",
        "  \n",
        "    arxiv_data = arxiv.query(query=query, max_results=page_number)\n",
        "\n",
        "    urls = [i['id'].replace('arxiv.org/', 'export.arxiv.org/') for i in arxiv_data]\n",
        "    titles = [i['title'] for i in arxiv_data]\n",
        "    abstracts = [i['summary'] for i in arxiv_data] \n",
        "\n",
        "    txts = []\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    for i in tqdm(urls):\n",
        "    \n",
        "        driver.get(i)\n",
        "        soup = BeautifulSoup(driver.page_source,'lxml')\n",
        "        result_div = soup.find_all('blockquote', attrs={'class': 'abstract mathjax'})[0]\n",
        "        abstract = result_div.get_text().replace('\\n',' ').replace('\\t',' ').strip()\n",
        "\n",
        "        file_data = parser.from_file(i.replace('abs', 'pdf'))['content']\n",
        "        content = file_data.replace('\\n',' ').replace('\\t',' ').strip()\n",
        "\n",
        "        extended_abstract = filter_text(content, abstract, threshold=0.01).replace('\\n',' ').replace('\\t',' ').strip()\n",
        "        txts.append(extended_abstract)\n",
        "    \n",
        "    driver.stop_client()\n",
        "    driver.close()\n",
        "\n",
        "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txts))\n",
        "\n",
        "    df = pd.DataFrame(list(zip(txts, urls, titles)), columns=['text','link', 'page'])\n",
        "    \n",
        "    return arxivtext, titles, df\n",
        "#############/Parse Arxiv #############\n",
        "\n",
        "############# Parse Google ###############\n",
        "def parse_google(query):   \n",
        "    \n",
        "    txt = []\n",
        "    titles = []\n",
        "    errors = []\n",
        "\n",
        "    # load driver\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    #/load driver  \n",
        "\n",
        "    # get urls\n",
        "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1)\n",
        "    \n",
        "    if filter_request == True:\n",
        "        google_url = google_url + '&searchtype=all&source=header&start=0&date-filter_by=past_' + str(months_delta)\n",
        "        google_url = google_url + '&hl=en&gl=en' + '&lr=lang_en&cr=countryGB'\n",
        "    \n",
        "    driver.get(google_url)\n",
        "    time.sleep(randint(1,5))\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
        "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
        "\n",
        "    links = []\n",
        "    titles = []\n",
        "    errors = []\n",
        "\n",
        "    descriptions = []\n",
        "    for r in result_div:\n",
        "        try:\n",
        "            link = r.find('a', href=True)\n",
        "            title = None\n",
        "            title = r.find('h3')\n",
        "\n",
        "            if isinstance(title,Tag):\n",
        "                title = title.get_text()\n",
        "\n",
        "            description = None\n",
        "            description = r.find('span', attrs={'class': 'st'})\n",
        "\n",
        "            if isinstance(description, Tag):\n",
        "                description = description.get_text()\n",
        "                \n",
        "            wikiarxiv_filter = ('wikipedia.org' not in link['href']) and ('arxiv.org' not in link['href'])\n",
        "            patent_filter = ('patents.google.com/' not in link['href'])\n",
        "\n",
        "            if wikiarxiv_filter and patent_filter and link != '' and title != '' and description != '':\n",
        "                links.append(link['href'])\n",
        "                titles.append(title)\n",
        "                descriptions.append(description)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "    url_list = list(set(links))[:(page_number)] \n",
        "    #/ get urls\n",
        "    \n",
        "    for j in tqdm(url_list):\n",
        "        delta = random.randint(1,8)\n",
        "        time.sleep(delta) \n",
        "        \n",
        "        try:  \n",
        "            if str(j).endswith('.pdf'):\n",
        "                file_data = parser.from_file(str(j))        \n",
        "                t = file_data['content'].replace('\\n','')    \n",
        "                titles.append(t[:100])\n",
        "            else:\n",
        "                t = parse_google_page(j)[0].replace('\\n','') \n",
        "                titles.append(parse_google_page(j)[1].replace('\\n',''))\n",
        "            \n",
        "            txt.append(t)\n",
        "            \n",
        "        except:\n",
        "            print('Parsing error:',str(j))\n",
        "            errors.append(str(j))\n",
        "          \n",
        "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
        "    df = df[~df['page'].str.contains('|'.join(['403','404']))]\n",
        "    df.replace('', np.nan, inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "    \n",
        "    googletext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(list(df['text'])))\n",
        "    titles = list(df['page'])\n",
        "   \n",
        "    return googletext, errors, df, titles\n",
        "#############/Parse Google ###############\n",
        "\n",
        "############## Text processing #########\n",
        "def text_from_html(body):\n",
        "    soup = BeautifulSoup(body, 'html.parser')\n",
        "    texts = soup.findAll(text=True)\n",
        "    visible_texts = filter(tag_visible, texts)\n",
        "    return u\" \".join(t.strip() for t in visible_texts)\n",
        "\n",
        "def text_normalize(txt):\n",
        "    processed_text = re.sub('[^a-zA-Z]', ' ', txt)\n",
        "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
        "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(processed_text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
        "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def tag_visible(element):\n",
        "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
        "        return False\n",
        "    if isinstance(element, Comment):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def filter_triplet(final_text):\n",
        "    \n",
        "    final_text = get_unique_text(final_text)\n",
        "    doc = nlp(final_text)\n",
        "    valid_sents = []\n",
        "\n",
        "    for s in list(doc.sents):\n",
        "        if syntax_full(s):\n",
        "            valid_sents.append(s.text)\n",
        "    \n",
        "    final_text = ' '.join(valid_sents)\n",
        "    \n",
        "    return final_text\n",
        "\n",
        "def coref_res(rawtext, coref_greedn = 0.5):\n",
        "\n",
        "    neuralcoref.add_to_pipe(nlp, greedyness = coref_greedn, store_scores=False)\n",
        "    doc = nlp(rawtext)\n",
        "\n",
        "    resolved = list(tok.text_with_ws for tok in doc)\n",
        "\n",
        "    for cluster in doc._.coref_clusters:\n",
        "        for coref in cluster:\n",
        "            if coref != cluster.main:\n",
        "                if coref.text[0].isalpha() and coref.text[0].isupper():\n",
        "\n",
        "                    main_words_list=word_tokenize(cluster.main.text)\n",
        "                    main_words_list[0]=main_words_list[0].capitalize()\n",
        "                    resolved[coref.start] = detokenizer(main_words_list) + doc[coref.end-1].whitespace_\n",
        "\n",
        "                for i in range(coref.start+1, coref.end):\n",
        "                    resolved[i] = \"\"\n",
        "            else:\n",
        "                resolved[coref.start] = cluster.main.text + doc[coref.end-1].whitespace_\n",
        "                for i in range(coref.start+1, coref.end):\n",
        "                    resolved[i] = \"\"\n",
        "\n",
        "    text_resolved = ''.join(resolved)\n",
        "    nlp.remove_pipe(\"neuralcoref\")\n",
        "\n",
        "    return text_resolved\n",
        "\n",
        "def compress(spacy_sents,sents_whitelist):\n",
        "    blacklist_tokens=[]\n",
        "    n=1\n",
        "    for sent in spacy_sents:\n",
        "        if (n in sents_whitelist):\n",
        "            for token in sent:\n",
        "                if token.dep_ in ['appos','advmod']:\n",
        "                    token_sub_tree=token.subtree\n",
        "                    for t in token_sub_tree:\n",
        "                        blacklist_tokens.append(t.i)\n",
        "\n",
        "        n=n+1\n",
        "    return(blacklist_tokens)\n",
        "\n",
        "def spacy_compress(rawtext):\n",
        "\n",
        "    doc1 = nlp(rawtext)\n",
        "    sents_whitelist = get_sents_ids_whitelist(doc1.sents)\n",
        "\n",
        "    tokens_blacklist = compress(doc1.sents,sents_whitelist)\n",
        "    sents_tokens = get_list_sents_tokens(doc1.sents,sents_whitelist,tokens_blacklist)\n",
        "    compressed_text_sents = []\n",
        "\n",
        "    for s in sents_tokens:\n",
        "        text=detokenizer(s)\n",
        "        compressed_text_sents.append(text)\n",
        "    compressed_text_sents=sentence_grammar_fix(compressed_text_sents)\n",
        "    text =' '.join(compressed_text_sents)\n",
        "\n",
        "    return(text)\n",
        "##### Text processing #####\n",
        "\n",
        "############## Get summary #############\n",
        "def get_summary(rawtext, sentences):\n",
        "    \n",
        "    stemmer = Stemmer(\"english\")\n",
        "    summarizer = Summarizer(stemmer)\n",
        "    summarizer.stop_words = get_stop_words(\"english\")\n",
        "    parser = PlaintextParser.from_string(' '.join(sent_tokenize(rawtext)[6:]), Tokenizer(\"english\"))\n",
        "\n",
        "    text_list = []\n",
        "    for sentence in summarizer(parser.document, sentences):\n",
        "        text_list.append(str(sentence))\n",
        "\n",
        "    txt = ' '.join(sent_tokenize(rawtext)[:6]) + ' '+' '.join(text_list)\n",
        "\n",
        "    z = 0\n",
        "    output = []\n",
        "    \n",
        "    for i in nltk.sent_tokenize(txt):\n",
        "        output.append(str(i) + '==')\n",
        "    \n",
        "    txt = ''.join(output)\n",
        "    \n",
        "    return txt\n",
        "##############/Get summary #############\n",
        "\n",
        "############## Get tags and entities ###########\n",
        "def graph_keys(final_text, top_number):\n",
        "    \n",
        "    bigrams = list(nltk.ngrams(text_normalize(final_text.lower()),2))\n",
        "    bigrams = [' '.join(i) for i in bigrams if (i[0]!=i[1])] \n",
        "    bigram_counts = collections.Counter(bigrams)\n",
        "    \n",
        "    df = pd.DataFrame(bigram_counts.most_common(len(bigram_counts)), columns=['bigram', 'count'])[:top_number]\n",
        "    df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
        "    keys = ', '.join(list(df['bigram'].astype(str)))\n",
        "\n",
        "    return keys\n",
        "\n",
        "def yake_keys(text, keys_number):\n",
        "    сustom_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=2, top=keys_number)\n",
        "    keywords = сustom_kw_extractor.extract_keywords(text)\n",
        "    keywords = ', '.join([i[1] for i in keywords])\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def get_entities(rawtext, tops):\n",
        "    spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
        "    nlp.max_length = 1000000000000\n",
        "    doc = spacy_nlp(rawtext)\n",
        "\n",
        "    ners = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG', 'PERSON']:\n",
        "            ners.append(ent.text)\n",
        "   \n",
        "    ner_counts = collections.Counter(ners)\n",
        "\n",
        "    try:\n",
        "        df = pd.DataFrame(ner_counts.most_common(len(ner_counts)), columns=['ner_names', 'count'])[:tops]\n",
        "        df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
        "        keys = ', '.join(list(df['ner_names'].astype(str)))\n",
        "    except:\n",
        "        keys = ''\n",
        "    \n",
        "    return keys\n",
        "############## Get tags and entities #############\n",
        "\n",
        "############## Add keyurls ################\n",
        "def add_keyurls(final_keys, query):\n",
        "    url_keys = []\n",
        "    for i in final_keys.split(','):\n",
        "        url = 'https://www.google.com/search?q=' + '+'.join(re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip().split()) + '+' + query + '/keyword/' + i \n",
        "        url_keys.append(url)\n",
        "        \n",
        "    return url_keys     \n",
        "##############/Add urls ###################\n",
        "\n",
        "##### Abstractive summarization #############\n",
        "def get_response(input_text,num_return_sequences):\n",
        "    \n",
        "    batch = tokenizer.prepare_seq2seq_batch([input_text], truncation=True, padding='longest', max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "    translated = model.generate(**batch, max_length=60, num_beams=10, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "    \n",
        "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    \n",
        "    return tgt_text\n",
        "#####/Abstractive summarization #############\n",
        "\n",
        "############# Doc preparation ##########\n",
        "def add_hyperlink(paragraph, text, url, flag):\n",
        "    part = paragraph.part\n",
        "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
        "\n",
        "    # Create the w:hyperlink tag and add needed values\n",
        "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
        "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
        "\n",
        "    # Create a w:r element and a new w:rPr element\n",
        "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
        "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
        "\n",
        "    # Join all the xml elements together add add the required text to the w:r element\n",
        "    new_run.append(rPr)\n",
        "    new_run.text = text\n",
        "    hyperlink.append(new_run)\n",
        "\n",
        "    # Create a new Run object and add the hyperlink into it\n",
        "    r = paragraph.add_run()\n",
        "    r._r.append (hyperlink) \n",
        "\n",
        "    # A workaround for the lack of a hyperlink style (doesn't go purple after using the link)\n",
        "    # Delete this if using a template that has the hyperlink style in it\n",
        "    r.font.color.theme_color = MSO_THEME_COLOR_INDEX.HYPERLINK\n",
        "    r.font.underline = flag\n",
        "\n",
        "    return hyperlink\n",
        "\n",
        "def save_doc(final_summary, summary, query, score, compression):\n",
        "    \n",
        "    sent_list = list(final_summary.split(sep='<hr>'))\n",
        "    doc = Document()\n",
        "    style = doc.styles['Normal']\n",
        "    \n",
        "    font = style.font\n",
        "    font.name = 'Times New Roman'\n",
        "    font.size = Pt(12)\n",
        "\n",
        "    hd = doc.add_paragraph()\n",
        "    hd.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
        "    hd.add_run('Summary').bold = True\n",
        "\n",
        "    if query != 'none':\n",
        "        hd = doc.add_paragraph('Request: ' + \"''\" + query + \"''\")\n",
        "\n",
        "    hd = doc.add_paragraph('Information: ' + str(score))\n",
        "    hd = doc.add_paragraph('Word compression: ' + str(compression))\n",
        "    hd = doc.add_paragraph('Model efficiency: ' + str(round((score/compression),2)))  \n",
        "    \n",
        "    r = hd.add_run()\n",
        "    for i in sent_list:\n",
        "        hd.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "\n",
        "        if query != 'none':\n",
        "            try:\n",
        "                link = re.search(r\"<a href=(.*?)target='_blank'\", str(i)).group(1).replace(' ','')\n",
        "                hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))               \n",
        "                add_hyperlink(hd, 'More', link, True).add_run()\n",
        "            except:\n",
        "                link = ''\n",
        "        if query == 'none':\n",
        "            hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))    \n",
        "         \n",
        "    doc.save(summary + '.docx')\n",
        "    \n",
        "    return True\n",
        "#############/Doc preparation ##########\n",
        "\n",
        "############## Sandbox functions ##########\n",
        "def longest_common_substring(s1, s2):\n",
        "  m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
        "  longest, x_longest = 0, 0\n",
        "  for x in range(1, 1 + len(s1)):\n",
        "    for y in range(1, 1 + len(s2)):\n",
        "      if s1[x - 1] == s2[y - 1]:\n",
        "        m[x][y] = m[x - 1][y - 1] + 1\n",
        "        if m[x][y] > longest:\n",
        "          longest = m[x][y]\n",
        "          x_longest = x\n",
        "      else:\n",
        "        m[x][y] = 0\n",
        "  return s1[x_longest - longest: x_longest]\n",
        "\n",
        "def longest_common_sentence(s1, s2):\n",
        "    s1_words = s1.split(' ')\n",
        "    s2_words = s2.split(' ')\n",
        "    return ' '.join(longest_common_substring(s1_words, s2_words))\n",
        "\n",
        "def css(a,b):\n",
        "    if len(a.split()) > 0:\n",
        "        score = len(longest_common_sentence(a,b).split())/len(a.split())\n",
        "    else:    \n",
        "        score = 0\n",
        "    return score\n",
        "\n",
        "def readingTime(mytext):\n",
        "    total_words = len(word_tokenize(mytext))\n",
        "    estimatedTime = round(total_words/200.0,1)\n",
        "    return estimatedTime\n",
        "\n",
        "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
        "\n",
        "def tags(x):\n",
        "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
        "\n",
        "def syntax_full(spacy_sentence):\n",
        "    result=[]\n",
        "    for token in spacy_sentence:\n",
        "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
        "            result.append(token.head)\n",
        "    if result:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def check_min_num_of_clauses(spacy_sentence, n):\n",
        "    result=[]\n",
        "    for token in spacy_sentence:\n",
        "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
        "            result.append(token.head.text)\n",
        "    if len(result)>=n:\n",
        "        return True\n",
        "\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def get_sents_ids_whitelist(spacy_sents):\n",
        "    whitelist=[]\n",
        "    i=1\n",
        "    sents_texts=[]\n",
        "    for sent in spacy_sents:\n",
        "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
        "            whitelist.append(i)\n",
        "            sents_texts.append(sent.text)\n",
        "        i=i+1\n",
        "    return(whitelist)\n",
        "\n",
        "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
        "    sents_tokens=[]\n",
        "    n=1\n",
        "    for sent in spacy_sents:\n",
        "        sent_tokens=[]\n",
        "        if (n in sents_whitelist):\n",
        "            for token in sent:\n",
        "                if (token.i not in blacklist_tokens):\n",
        "                    sent_tokens.append(token.text)\n",
        "            sents_tokens.append(sent_tokens)\n",
        "            sent_tokens=[]\n",
        "\n",
        "        n=n+1\n",
        "    return(sents_tokens)\n",
        "\n",
        "def detokenizer(list_of_tokens):\n",
        "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
        "    return(text_str)\n",
        "\n",
        "def sentence_grammar_fix(sentences):\n",
        "    fixed=[]\n",
        "    for sent in sentences:\n",
        "\n",
        "        sent=sent.strip()\n",
        "        sent=sent.replace('\\n','')\n",
        "        sent=sent.replace('()','')\n",
        "\n",
        "        sent=re.sub('\\s+',' ',sent)\n",
        "        sent=sent+'.'\n",
        "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
        "\n",
        "        if len(sent)>1:\n",
        "            if sent[0] in ['.',',','-','—']:\n",
        "                sent=sent[1:]\n",
        "        sent=sent.strip()\n",
        "\n",
        "        if len(sent)>1:\n",
        "            if sent[0].isalpha():\n",
        "                sent=sent[0].upper()+sent[1:]\n",
        "        fixed.append(sent)\n",
        "\n",
        "    return(fixed)\n",
        "\n",
        "def get_scores(report_summary, final_text):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
        "    scores = scorer.score(' '.join(text_normalize(report_summary)).lower(), ' '.join(text_normalize(final_text)).lower())\n",
        "    scores = round(list(list(scores.values())[0])[2],2)\n",
        "    \n",
        "    return scores \n",
        "##############/Sandbox functions ##########\n",
        "\n",
        "############## Extend abstract ##########\n",
        "def get_ngrams(text): \n",
        "    grams = nltk.ngrams(text.split(), 2)\n",
        "    grams_list = []\n",
        "    for i in grams:\n",
        "        grams_list.append(i)\n",
        "    \n",
        "    return grams_list \n",
        "\n",
        "def get_jaccard_sim(a,b):\n",
        "    a, b = set(get_ngrams(a)), set(get_ngrams(b)) \n",
        "    c = a.intersection(b)\n",
        "\n",
        "    return round(float(len(c)/len(a)), 2)\n",
        "\n",
        "def filter_text(content, abstract, threshold=0.5, content_type='arxiv'): \n",
        "    \n",
        "    content_list = []   \n",
        "    \n",
        "    for j in content.split('.'):\n",
        "        try:\n",
        "            sim_score = get_jaccard_sim(j, abstract)\n",
        "        except:\n",
        "            sim_score = 0\n",
        "            \n",
        "        if sim_score > threshold:\n",
        "            content_list.append(j)    \n",
        "        \n",
        "        if content_type == 'wiki':\n",
        "            reduced_list = [i for i in content_list if i not in list(abstract.split('.'))]\n",
        "            final_list = list(dict.fromkeys(abstract.split('.') + reduced_list)) \n",
        "        else:\n",
        "            final_list = content_list \n",
        "                        \n",
        "    return '. '.join(final_list)\n",
        "##############/Extend abstract #########"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRRSLY_gnkP5"
      },
      "source": [
        "##### Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA7gIToCnkP_",
        "scrolled": false,
        "outputId": "3fb6c9ec-15b2-4dc4-bb50-ae5c5f44dac8"
      },
      "source": [
        "query = input()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pla polylactic acid problems risks issues constraints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Pi7Fxcp9Kn"
      },
      "source": [
        "##### Parse Wiki:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6J0jcdCnkQA",
        "scrolled": true,
        "outputId": "27c10a26-0d24-4627-fc1e-eff293c96462"
      },
      "source": [
        "%%time\n",
        "\n",
        "wikitext = ''\n",
        "wikikeys = ''\n",
        "\n",
        "df_wiki = pd.DataFrame()\n",
        "\n",
        "if wiki_sum == True: \n",
        "    \n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "    \n",
        "    red_query = \"https://www.google.com/search?q=\" + 'site:https://en.wikipedia.org ' + query + \"&num=\" + str(page_number+1)\n",
        "    red_query = red_query + '&searchtype=all&source=header'\n",
        "    \n",
        "    wiki_titles = parse_wiki(red_query)\n",
        "\n",
        "    txts = []\n",
        "    titles = []\n",
        "\n",
        "    for i in tqdm(wiki_titles): \n",
        "        \n",
        "        page_sum = wiki_wiki.page(i).summary\n",
        "        page_txt = wiki_wiki.page(i).text\n",
        "        sent_list = filter_text(page_txt, page_sum, threshold=threshold, content_type='wiki')\n",
        "       \n",
        "        titles.append(i)\n",
        "        txts.append(''.join(sent_list).replace('\\n', ''))        \n",
        "    \n",
        "    wikitext = ''.join(txts).replace('\\n','') \n",
        "\n",
        "    if compress == True:\n",
        "        wikitext = coref_res(filter_triplet(wikitext))\n",
        "\n",
        "    url_list = [str('https://en.wikipedia.org/wiki/' + i)  for i in wiki_titles] \n",
        "    \n",
        "    df_wiki = pd.DataFrame(list(zip(txts, url_list, titles)), columns=['text','link', 'page'])\n",
        "    df_wiki.replace('', np.nan, inplace=True)\n",
        "    df_wiki.dropna(inplace=True)\n",
        "    \n",
        "    random_num = randint(1,len(df_wiki))  \n",
        "    \n",
        "    print(df_wiki['page'][random_num-1]+'\\n')\n",
        "    print(df_wiki['text'][random_num-1][:1000]+'...'+'\\n')\n",
        "    print(wikitext[:2000] + '...'+'\\n')   "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  5.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Biodegradable_plastic\n",
            "\n",
            "Biodegradable plastics are plastics that can be decomposed by the action of living organisms, usually microbes, into water, carbon dioxide, and biomass.  Biodegradable plastics are commonly produced with renewable raw materials, micro-organisms, petrochemicals, or combinations of all three. While the words \"bioplastic\" and \"biodegradable plastic\" are similar, they are not synonymous.  Not all bioplastics (plastics derived partly or entirely from biomass) are biodegradable. ...\n",
            "\n",
            "Biodegradable plastics are plastics that can be decomposed by the action of living organisms, usually microbes, into water, carbon dioxide, and biomass.  Biodegradable plastics are commonly produced with renewable raw materials, micro-organisms, petrochemicals, or combinations of all three. While the words \"bioplastic\" and \"biodegradable plastic\" are similar, they are not synonymous.  Not all bioplastics (plastics derived partly or entirely from biomass) are biodegradable. Tissue engineering is a biomedical engineering discipline that uses a combination of cells, engineering, materials methods, and suitable biochemical and physicochemical factors to restore, maintain, improve, or replace different types of biological tissues.  Tissue engineering often involves the use of cells placed on tissue scaffolds in the formation of new viable tissue for a medical purpose but is not limited to applications involving cells and tissue scaffolds.  While it was once categorized as a sub-field of biomaterials, having grown in scope and importance it can be considered as a field in its own. While most definitions of tissue engineering cover a broad range of applications, in practice the term is closely associated with applications that repair or replace portions of or whole tissues (i. e. , bone, cartilage, blood vessels, bladder, skin, muscle etc. ).  Often, the tissues involved require certain mechanical and structural properties for proper functioning.  The term has also been applied to efforts to perform specific biochemical functions using cells within an artificially-created support system (e. g.  an artificial pancreas, or a bio artificial liver).  The term regenerative medicine is often used synonymously with tissue engineering, although those involved in regenerative medicine place more emphasis on the use of stem cells or progenitor cells to produce tissues. Plastics are a wide range of synthetic or semi-synthetic materials, that usually use polymers as a main ingredient....\n",
            "\n",
            "CPU times: user 318 ms, sys: 30 ms, total: 348 ms\n",
            "Wall time: 3.99 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1l11mWCnkQB"
      },
      "source": [
        "##### Parse Arxiv:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEBlFcZFnkQB",
        "scrolled": true,
        "outputId": "4142f2ef-f4fb-4bf3-ca96-9a98564c3ebe"
      },
      "source": [
        "%%time\n",
        "\n",
        "arxivtext = ''\n",
        "arxivkeys = ''\n",
        "\n",
        "df_arxiv = pd.DataFrame()\n",
        "\n",
        "if arxiv_sum == True:\n",
        "    \n",
        "    try:\n",
        "        df_arxiv = parse_arxiv(query)[2] \n",
        "        df_arxiv.replace('', np.nan, inplace=True)\n",
        "        df_arxiv.dropna(inplace=True)   \n",
        "        \n",
        "        arxivtext = ''.join(list(df_arxiv['text'])) \n",
        "    \n",
        "        if compress == True:\n",
        "            arxivtext = coref_res(filter_triplet(arxivtext))\n",
        "    \n",
        "        random_num = randint(1, len(df_arxiv)) \n",
        "        \n",
        "        print(df_arxiv['page'][random_num-1]+'\\n')\n",
        "        print(df_arxiv['text'][random_num-1][:1000]+'...'+'\\n')\n",
        "    \n",
        "    except:\n",
        "        print('No data')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]2021-01-29 08:01:18,262 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/0802.3074v1 to /tmp/pdf-0802.3074v1.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:612: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            " 10%|█         | 1/10 [00:02<00:21,  2.39s/it]2021-01-29 08:01:19,715 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/2012.07210v1 to /tmp/pdf-2012.07210v1.\n",
            " 20%|██        | 2/10 [00:06<00:28,  3.52s/it]2021-01-29 08:01:23,995 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/1605.00777v1 to /tmp/pdf-1605.00777v1.\n",
            " 30%|███       | 3/10 [00:09<00:20,  2.99s/it]2021-01-29 08:01:26,338 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/1909.02083v2 to /tmp/pdf-1909.02083v2.\n",
            " 40%|████      | 4/10 [00:31<01:04, 10.73s/it]2021-01-29 08:01:49,103 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/2001.03430v1 to /tmp/pdf-2001.03430v1.\n",
            " 50%|█████     | 5/10 [00:33<00:37,  7.46s/it]2021-01-29 08:01:50,612 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/1910.13760v1 to /tmp/pdf-1910.13760v1.\n",
            " 60%|██████    | 6/10 [00:36<00:23,  5.99s/it]2021-01-29 08:01:53,792 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/1707.03677v1 to /tmp/pdf-1707.03677v1.\n",
            " 70%|███████   | 7/10 [00:37<00:13,  4.45s/it]2021-01-29 08:01:55,028 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/1907.02634v1 to /tmp/pdf-1907.02634v1.\n",
            " 80%|████████  | 8/10 [00:41<00:08,  4.16s/it]2021-01-29 08:01:58,647 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/2006.02848v1 to /tmp/pdf-2006.02848v1.\n",
            " 90%|█████████ | 9/10 [00:44<00:03,  3.74s/it]2021-01-29 08:02:01,377 [MainThread  ] [INFO ]  Retrieving http://export.arxiv.org/pdf/1704.02017v1 to /tmp/pdf-1704.02017v1.\n",
            "100%|██████████| 10/10 [00:46<00:00,  4.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Biodegradable Polylactic Acid (PLA) Microstructures for Scaffold\n",
            "  Applications\n",
            "\n",
            "tw                                                            ABSTRACT    In this research, we present a simple and cost effective  soft lithographic process to fabricate PLA scaffolds for  tissue engineering.  In which, the negative photoresist JSR  THB-120N was spun on a glass subtract followed by  conventional UV lithographic processes to fabricate the  master to cast the PDMS elastomeric mold.  A thin  poly(vinyl alcohol) (PVA) layer was used as a mode  release such that the PLA scaffold can be easily peeled  off.  The PLA precursor solution was then cast onto the  PDMS mold to form the PLA microstructures.  After  evaporating the solvent, the PLA microstructures can be  easily peeled off from the PDMS mold.  Experimental  results show that the desired microvessels scaffold can be  successfully transferred to the biodegradable polymer  PLA.  INTRODUCTION     Biodegradable polymers are currently used in a   number of biomedical applications, including sutures,  screws, surgical adhe...\n",
            "\n",
            "CPU times: user 2.06 s, sys: 456 ms, total: 2.52 s\n",
            "Wall time: 47.9 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmVdkOknkQC"
      },
      "source": [
        "##### Parse Google:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dxn_sIvnkQC",
        "scrolled": true,
        "outputId": "2c1468fe-d85c-4c35-d377-21c8e2fd9892"
      },
      "source": [
        "%%time\n",
        "\n",
        "googletext = ''\n",
        "df_google = pd.DataFrame()\n",
        "\n",
        "if gogle_sum == True:\n",
        "    \n",
        "    try:\n",
        "        df_google = parse_google(query)[2]\n",
        "        df_google.replace('', np.nan, inplace=True)\n",
        "        \n",
        "        googletext = ''.join(list(df_google['text']))\n",
        "        \n",
        "        if compress == True:\n",
        "            googletext = coref_res(filter_triplet(googletext))\n",
        "            \n",
        "        random_num = randint(1,len(df_google)) \n",
        "        \n",
        "        print(list(df_google['page'])[random_num-1]+'\\n')\n",
        "        print(list(df_google['text'])[random_num-1])\n",
        "\n",
        "    except:\n",
        "        print('No data')   "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:20<00:00,  8.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PLA - ACS Publications - American Chemical Society\n",
            "\n",
            "PLA is suitable to process using injection molding at a compression ratio of 2.5–3. PLA is suitable for a hot runner process, while products like drinking cups which have thin-walled design require subsequent optimization besides following the general setting as recommended by Natureworks (2011) in Table 10.2. In addition, the addition of colorants and a slip agent needs to be done in the masterbatch. Premixing a using tumble mixer is favorable in order to obtain homogeneous results. However, due to the PLA masterbatch seldom being found in the market, it is recommended users produce their own masterbatch using PLA as the carrier. There are limitations to producing a masterbatch using PLA carriers, for instance, minerals such as calcium carbonate and additives such as zinc stearate, which are inherently hydroscopic, can release water when processed at elevated temperatures, and subsequently the water can react with the PLA causing depolymerization degradation and damaging the PLA properties. Table 10.2. General Setting for an Injection Molding Machine to Process Poly(Lactic Acid). ZoneBarrel Condition SettingFeed throat20°CFeed section165°CCompression section195°CMetering section200°C–230°CHot runner205°C–230°CNozzle tips205°C–230°CMold temperature25°CScrew speed50–100 rpmBack pressure150–200 psi Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9780128144725000108 PLA filament has a wide range of 3D printing in medical applications, as it also offers biodegradability. Made by polymerizing sugar cane and potato starch, it has a low-toxicity. PLA has the ability to degrade into lactic acid in the body and due to this property, it has been used in medical suturing and surgical implants. Surgically implanted screws, pins, rods or mesh naturally break down in the body between 6 and 24 months. However, there have been a number of side effects resulting from implantation because of foreign body reactions [26]. Therefore, it should not be used for surgical transplantation (Fig. 3.16). Figure 3.16. 3D Printed PLA in the shape of atlas vertebra. This model features a rough surface as a result of the layer resolution. This model was 3D printed using a 1mm layer height. PLA is a key material for printing using an office-based 3D printer and is environment-sustainable. Because PLA only shrinks by 1.5% on cooling so there is no requirement for a heated bed. PLA can be procured in both 1.75 and 3 mm diameter filaments. There are also further derivative PLA materials that are crystal clear, soft, and impact resistant and is great for the manufacture of a wide range of 3D printed medical models and surgical instruments. PLA requires a 3D print temperature range: 195–215°C. PLA is the best material for medical professionals who want to start printing a range of devices, models, and instruments. Because of its affinity to attract water it can become difficult to print with as it gets older. It is important to ensure PLA is stored in a cool dry place when it is not being used. Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9780081025420000038 While PLA has found a plurality of applications in several technical areas such as textiles and packaging, it presents significant performance limitations when used in building and construction applications. In particular, PLA’s sensitivity to high temperatures, and particularly its low heat deflection temperature (HDT), has rendered PLA and PLA composites unsuitable for use in most building construction components. In addition, PLA is prone to degradation by hydrolysis, especially at elevated temperatures and humidity levels. This inherent aspect of PLA makes it prone to deterioration over time and has further limited its use as a component of structural or decorative members that will be exposed to the elements. Also, the adhesion of PLA to other polymers or coatings is problematic (2011, WO2011116122 A1, ANDERSEN CORP.). PLA can be crystallized, and increased crystallinity can raise the HDT of the biopolymer. However, the rate of crystallization of PLA is relatively low, which may limit its value in situation where high throughput, coupled with high crystallinity, that is to say rapid crystallization, is needed. This combination of requirements can occur, for example, where structural and decorative members are to be produced by extrusion, and the required crystallization rate is therefore dictated by the line speed of the extrusion process. Such line speeds typically have been far too fast to allow PLA to crystallize before cooling. An additional problem can arise due to the fact that PLA has a rather sharply defined melting temperature (T m ). This can place strict requirements on temperature control and die design in an extruder, and it may be difficult to produce an extruded member from PLA that retains its shape well during cooling (2011, WO2011116122 A1, ANDERSEN CORP.). PLA is also flammable and in order to be used for parts which require a flame retardancy, its flammability has to be reduced. Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9780323353991000107 PLA is derived from renewable resources, such as corn starch or sugarcanes. PLA polymers are considered biodegradable and compostable. PLA is a thermoplastic, high-strength, high-modulus polymer that can be made from annually renewable sources to yield articles for use in either the industrial packaging field or the biocompatible/bioabsorbable medical device market. Bacterial fermentation is used to make lactic acid, which is then converted to the lactide dimer to remove the water molecule that would otherwise limit the ability to make high-molecular-weight polymer. The lactide dimer, after the water is removed, can be polymerized without the production of the water. This process is shown in Fig. 13.14. The PLA CAS number is 9002-97-5. Figure 13.14. Conversion of lactic acid to polylactic acid. Manufacturers and trade names : FKur Bio-Flex ® , Cereplast Inc. Compostables ® , Mitsubishi Chemical Fozeas ® , NatureEorks LLC Ingeo ™ , Alcan Packaging Ceramis ® -PLA. Applications and uses : Biomedical applications, such as sutures, stents, dialysis media, and drug delivery devices. It is also being evaluated as a material for tissue engineering; loose-fill packaging, compost bags, food packaging, and disposable tableware. PLA can be in the form of fibers and nonwoven textiles. Potential uses: upholstery, disposable garments, awnings, feminine hygiene products. See also Figs. 13.15–13.19. Figure 13.15. Permeation coefficient of methane vs. temperature through linear polylactic acid film. 14 Figure 13.16. Permeation coefficient of carbon dioxide vs. temperature through linear polylactic acid film. 14 Figure 13.17. Permeation coefficient of nitrogen vs. temperature through linear polylactic acid film. 14 Figure 13.18. Permeation coefficient of oxygen vs. temperature through linear polylactic acid film. 14 Figure 13.19. Permselectivity of carbon dioxide/methane vs. temperature through linear polylactic acid film. 14 Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B978143773469010013X PLA can be synthesized by different routes starting from lactic acid, an α-hydroxy acid existing in either L(+) or D(−) stereoisomer. Its properties, namely the maximum crystallinity, melting temperature and glass transition temperature depend upon their molecular weight and stereochemistry. Both poly(L-lactic acid) (PLLA) and poly(D-lactic acid) (PDLA) are semicrystalline, while the presence of significant amounts of one form within a sequence of the other, giving poly(D,L-lactic acid), can result in an amorphous polymer (Steinbuchel and Doi, 2002). PLA can be processed by using conventional technologies such as extrusion, injection moulding, injection stretch blow moulding, casting, blown film thermoforming, foaming, blending, fibre spinning and compounding (Lim and coworkers, 2008). The glass transition temperature ( T g ) of amorphous PLA lies between 55 to 60 °C and is a function of the PLA molecular weight and stereochemistry. In semicrystalline PLA, the T g is higher (60–80 °C) and depends on the crystallization conditions that determine both the morphology of the crystalline/amorphous phases and the degree of crystallinity. The amorphous phase in semicrystalline PLA is characterized by the presence of two phases, a mobile fraction and a rigid fraction characterized by lower mobility and higher T g (Iannace and Nicolais, 1997). The amount of rigid fraction depends upon the crystallization conditions and it can reach values of about 25–30% when PLA is isothermally crystallized in the range of 90–110 °C. PLA crystallizes when cooled from melt (melt crystallization) or when heated above its T g (cold crystallization) in the range 80–150 °C, but its fastest rate of crystallization occurs between 95 °C and 115 °C. The melting temperature ( T m ) of PLA occurs between 130 °C and 180 °C according to the L-lactide content and the type of crystals formed during the crystallization. The selection of the processing window aimed at exploring foaming behaviour in the presence of blowing agents is based on the thermal properties of PLA. In particular, porous structures can be generated within a PLA matrix by promoting bubble nucleation and growth during a controlled cooling from the melt, typically in extrusion and injection-moulding-based technologies, or in the ‘solid state’ above the T g and below the T m . In both methods, one must take into account that in presence of a physical blowing agent, the T g , the crystallization temperatures, and the T m will shift to lower values. Carbon dioxide has been used as an efficient plasticizer and foaming agent for the fabrication of 3D scaffolds based on PLA and poly(lactic acid-co-glycolic acid) (PLGA). CO 2 can be also used in combination with N 2 to improve the porous morphology of PLA and PLA-based nanocomposite matrices (Di et al ., 2005a, 2005b). The effect of foaming conditions from the melt on foam architecture was investigated by Mathieu et al. (2005). Samples of PLA were placed in a pressure vessel and saturated at 100 and 250 bar at 195 °C. Foam expansion was then achieved by sudden gas release, with additional water cooling. Neat PLA foams with interconnected pores with a diameter of 200-400 μm, and compressive moduli between 10 and 180 MPa for porosities from 78% to 92%, were obtained (Fig. 6.3a). 6.3. Porous morphologies of PLA foams prepared with different techniques and processing conditions: (a) PLA saturated with CO 2 at 195 °C, 15 MPa, cooling at 5.1 °C/s (Mathieu et al ., 2005); (b) PLA saturated with a mixture of CO 2 and N 2 (20:80) at 170 °C, 16 MPa, T foam = 110 °C (Di et al ., 2005b); (c) PLA saturated with CO 2 at 100 °C, 5 MPa, after ultrasound (Wang et al ., 2006); (d) PLA/nanoclay (1%) saturated with a mixture of CO 2 and N 2 (20:80) at 170 °C, 16 MPa, T foam = 110 °C (Di et al ., 2005a). Foaming from melt can be also used to develop composite scaffolds based on PLA matrix and ceramic fillers, hydroxyapatite (HA) or β -tricalcium phosphate ( β -TCP) (Mathieu et al ., 2006; Montjovent et al ., 2007). The rate of cooling has a significant effect on the porous structure: cooling too rapidly will result in small closed pores, whereas a very slow cooling will not allow freezing of the structure, which will finally collapse. An intermediate cooling rate must be found which allows interconnections to be created, while still stabilizing the morphology before it collapses. To control the morphology of supercritically foamed scaffolds, it is essential to study the interactions of polymers with CO 2 and the consequent solubility of CO 2 in the polymers, as well as the viscosity of the plasticized polymers. Tai et al. (2010) have shown that the viscosities of the CO 2 - plasticized polymers at 35 °C and 100 bar were comparable to the values for the polymer melts at 140 °C. The PLA/gas solutions can therefore be foamed at relatively low temperature, and this allows the incorporation of biologically active guest species into polymer host with limited loss or change of activity. Based on the above considerations, the solid-state foaming process has been studied to generate microporous foams (usually termed microcellular foams) for biomedical applications by using gases such as CO 2 and N 2 . Singh et al. (2004) have used this method with an amorphous PLGA to generate microcellular structures with pore sizes ranging from sub-micrometres to a few hundred micrometres at 35 °C. However, the disadvantage of the process is that the foams it produces are mostly close-pored and not suitable for tissue engineering applications. For this reason, solidstate gas foaming of PLA-based scaffolds is often coupled with particulate leaching to generate open-pore structures (Mooney et al ., 1996; Harris et al ., 1998; Nam et al .,2000). Another method to break the pore walls of the solid-state foams is to use ultrasound. Wang et al. (2006) have used semicrystalline PLA. The samples were first foamed in the solid-state foaming process at temperature below the melting point (100-150 °C) to achieve suitable pore sizes. Then the foamed samples were processed using ultrasound to enhance the inter-pore connectivity of the solid-state foams (Fig. 6.3c). Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9780857096968500061 PGA and PLA are the most widely used synthetic, biodegradable polymers. PGA is the simplest linear polyester. It is highly crystalline and consequently it presents a high melting point and low solubility in organic solvents. It has been processed as a material for sutures (Dexon ® ). The limitation of these sutures is that they tend to lose their mechanical properties over a period of 2–4 weeks after implantation. To overcome this problem, the use of copolymers of PGA and PLA has been studied. The introduction of the more hydrophobic PLA into the polymeric structure limits the water uptake and reduces the rate of backbone hydrolysis. Lactic acid is a chiral molecule (i.e. a molecule that is not superimposable on its mirror image), it exists in two stereoisomeric forms that give rise to four morphologically different polymers: d-PLA, l-PLA, Dl-PLA and meso-PLA. The crystalline properties differ from polymer to polymer. Both d and l polymers are semicrystalline materials, whereas the optically inactive dL-PLA is always amorphous. For this reason, l-PLA is preferred for application where high mechanical strength and toughness are needed. Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9781845693855500088 PLA is derived from renewable resources, such as cornstarch or sugarcanes. PLA polymers are considered biodegradable and compostable. PLA is a thermoplastic, high-strength, high-modulus polymer that can be made from annually renewable sources to yield articles for use in either the industrial packaging field or the biocompatible/bioabsorbable medical device market. Bacterial fermentation is used to make lactic acid, which is then converted to the lactide dimer to remove the water molecule that would otherwise limit the ability to make high molecular weight polymer. The lactide dimer, after the water is removed, can be polymerized without the production of the water. This process is shown in Fig. 12.7. The PLA CAS number is 9002-97-5. Figure 12.7. Conversion of lactic acid to polylactic acid. The dimer can be isolated into three forms: the optically active l-lactide, the optically active d-lactide, and the optically inactive dl mixture dl-lactide. These stereoisomer structures are shown in Fig. 12.8. The enantiomeric ratio of the dimer can be controlled. Fermentation-derived lactic acid is 95% l-isomer. Figure 12.8. Stereoisomeric structures of the lactide dimer. Manufacturers and trade names: FKur Bio-Flex®, Cereplast Inc. Compostables®, Mitsubishi Chemical Fozeas®, Natureworks LLC Ingeo™, Alcan Packaging Ceramis®-PLA, Pro-Tech Biologische und Technische Produkte GmbH BioMat, and Melitta Haushaltsprodukte GmbH & Co KG SWIRL. Applications and uses: Biomedical applications, such as sutures, stents, dialysis media, and drug delivery devices. It is also being evaluated as a material for tissue engineering, loose-fill packaging, compost bags, food packaging, and disposable tableware. PLA can be in the form of fibers and nonwoven textiles; potential uses: upholstery, disposable garments, awnings, and feminine hygiene products. Data for PLA plastics are found in Figs 12.9–12.16. Figure 12.9. The effect of gamma irradiation dose on the tensile strength of Pro-Tech Biologische und Technische Produkte GmbH BioMat PLA film. 5 Figure 12.10. The effect of gamma irradiation dose on the retention of tensile strength of Melitta Haushaltsprodukte GmbH &amp; Co KG SWIRL PLA film. 5 Figure 12.11. The effect of gamma irradiation dose on the retention of elongation of Pro-Tech Biologische und Technische Produkte GmbH BioMat PLA film. 5 Figure 12.12. The effect of gamma irradiation dose on the retention of elongation of Melitta Haushaltsprodukte GmbH &amp; Co KG SWIRL PLA film. 5 Figure 12.13. The effect of electron beam sterilization on the molecular weight of polylactic acid. 6 Figure 12.14. The effect of electron beam sterilization on the retention of several properties of polylactic acid. 6 Figure 12.15. The effect of EtO sterilization on retention of several properties of poly L-lactic acid. Figure 12.16. The effect of steam autoclave sterilization under various conditions on the retention of several properties of poly L-lactic acid. Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9781455725984000125 PLA is a semicrystalline biodegradable aliphatic PES that can be obtained from renewable resources. PLA has a glass transition temperature between 50 and 80 °C and a melting temperature between 170 and 180 °C depending on the amount of residual monomer. It is expected to have wide applications because of its excellent mechanical and biodegradable properties, as well as its adjustable hydrolyzability. 101 The synthesis of high-molecular-weight PLA is generally carried out by the ring-opening polymerization of lactide or by direct polycondensation of lactic acid. Tin-based catalysts are typically used in both cases, either alone or in combination with p -toluenesulfonic acid. 102 In the methods using lactide, the final product of the melt polymerization usually contains a certain amount of the cyclic monomer, which increases with polymerization temperature. This residual lactide has been known to deteriorate mechanical properties of the polymer to cause corrosion of the processing machines and to increase the degradation rate of PLA. Solid-state polymerization has been applied to PLA as a method for monomer removal, considering that during SSP, crystallization also occurs, which may result in excluding reactive ends and monomer in the amorphous regions, thus reaching polymerization conversion to 100%. 103 In the pertinent study, a two-step method and a one-step method were carried out in the presence of stannous 2-ethylhexanoate (octoate) as catalyst. In the former technique, melt polymerization of lactide was first performed at 140 and 170 °C for 1 h and then the postpolymerization continued for 9 h at the crystallization temperature of PLA, which was predetermined by DSC to be 120–140 °C. Optimum reaction conditions were found for the two stages in order to obtain crystalline PLA free of monomer. In the one-step process, a mixture of monomer and catalyst was subjected to polymerization at a constant temperature around the crystallization point of PLA to allow the system to transform into the solid state during the reaction. Indeed, this was achieved at temperatures up to 140 °C, while a further increase did not induce crystallization and monomer conversion. In both techniques, however, the MW of PLA did not increase due to the ester interchange reactions and the formation of oligomers. A two-step method was also applied to the direct polycondensation of lactic acid, 104,105 where again a ring-chain equilibrium with the formation of cyclic monomer can occur and may reduce the product yield during the melt polymerization. In the pertinent studies, a binary catalyst system comprising tin dichloride hydrate and p -toluenesulfonic acid was used in a process comprising melt polymerization, prepolymer crystallization, and the solid-state polymerization. In particular, melt polycondensation was performed at 180 °C for 5 h, crystallization at 105 °C for 1 or 2 h, and SSP at 140 or 150 °C for 10–30 h. The suggested route resulted in high MW, above 500 000, and again crystallization of the prepolymer was a key process step and the crystallinity of the PLA product was well correlated with the increase in the MW. Regarding the MW, the effect of crystallization on the SSP of PLA has been investigated when the starting form was lactic acid. 106 In particular, the crystallization temperature was set at 105 °C for various times (15–90 min) and SSP was then performed in vacuo at 135 °C for 15–50 h. The optimum conditions for a prepolymer of MW 18 000 were found, namely a crystallization time of 30 min at 105 °C and then the solid-state polymerization at 135 °C for 35 h. Higher crystallization times for the prepolymer had a negative effect on the SSP rate due to the formation of large crystals and thus to the higher limitation to by-product diffusion. On the other hand, higher SSP times were detrimental to MW due to the occurrence of degradation reactions. Finally, in a very recent publication, PLA nanocomposites have been prepared through solid-state polymerization. 107 A prepolymer was first synthesized from both lactic acid and lactide in the presence of different organoclays and SSP followed. In the former case, a MW of 138 000 was reached after 10 h of SSP at 150 °C, while the presence of hydroxy groups in the organoclay modification inhibited a MW increase due to prepolymer end-groups binding to clay surface. When using lactide, an lactide–clay intercalated mixture was prepared by ring-opening polymerization and MW was further increased to 127 000 through SSP. Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9780444533494001266 Poly(lactic acid) (PLA) is produced from the monomer of lactic acid (LA). PLA can be produced by two well-known processes—the direct polycondensation (DP) route and the ring-opening polymerization (ROP) route. Although DP is simpler than ROP for the production of PLA, ROP can produce a low-molecular-weight brittle form of PLA. Generally, several substances are involved in the production of PLA, and these relationships are summarized in Fig. 2.1. The LA for the process is obtained from the fermentation of sugar. LA is converted to lactide and eventually to PLA. It should be noted that there are two different terms, “poly(lactic acid)” and “polylactide,” for the polymer of LA. Both terms are used interchangeably; however, scientifically there is a difference because polylactide is produced through the ROP route whereas PLA is generated using the DP route. Generally speaking, the term “poly(lactic acid)” is widely used to mean the polymer that is produced from LA. (The explanation regarding the difference between PLA and polylactide is given here to help readers’ understanding.) Figure 2.1. General routes of poly(lactic acid) production. Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9780128144725000029 Poly(lactic acid) (PLA) is known to be an environment-friendly polymer due to its biodegradability as well as it being produced from renewable sources. However, besides these two advantages, there are also other factors that contribute to its carbon footprint. This chapter aims to provide additional perspectives through the results of life cycle analysis and compare PLA with other polymers in order to discover the pros and cons when selecting environment-friendly polymers. Finally, lists of international standards on evaluation of the biodegradability of polymers are provided to enable readers to select the most appropriate standard to evaluate biodegradability of polymers. Read full chapter URL: https://www.sciencedirect.com/science/article/pii/B9780128144725000121 Composites Part B: Engineering Materials & Design Journal of the Mechanical Behavior of Biomedical Materials Biomaterials Browse Journals & Books\n",
            "CPU times: user 23 s, sys: 2.35 s, total: 25.3 s\n",
            "Wall time: 1min 27s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF5f-RonnkQD"
      },
      "source": [
        "##### Concatenate dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "VKW1FL4GnkQD",
        "outputId": "bcce746b-bdf1-4c6c-f347-ca7076a0db15"
      },
      "source": [
        "df = df_wiki.append(df_google).append(df_arxiv)\n",
        "df.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>link</th>\n",
              "      <th>page</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Biodegradable plastics are plastics that can b...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Biodegradable_pl...</td>\n",
              "      <td>Biodegradable_plastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tissue engineering is a biomedical engineering...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Tissue_engineering</td>\n",
              "      <td>Tissue_engineering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Plastics are a wide range of synthetic or semi...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Plastic</td>\n",
              "      <td>Plastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Green chemistry, also called sustainable chemi...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Green_chemistry</td>\n",
              "      <td>Green_chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A thermoplastic, or thermosoftening plastic, i...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Thermoplastic</td>\n",
              "      <td>Thermoplastic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                   page\n",
              "0  Biodegradable plastics are plastics that can b...  ...  Biodegradable_plastic\n",
              "1  Tissue engineering is a biomedical engineering...  ...     Tissue_engineering\n",
              "2  Plastics are a wide range of synthetic or semi...  ...                Plastic\n",
              "3  Green chemistry, also called sustainable chemi...  ...        Green_chemistry\n",
              "4  A thermoplastic, or thermosoftening plastic, i...  ...          Thermoplastic\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KuFgn2RnkQD"
      },
      "source": [
        "##### Define optimal compression rate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ywNX2dInkQE",
        "outputId": "a3d8c5c3-bb90-4d84-dd95-31d54c8b211e"
      },
      "source": [
        "%%time\n",
        "final_text = (wikitext + arxivtext + googletext)\n",
        "\n",
        "for i in tqdm(range(50, 10000, 20)): \n",
        "    \n",
        "    report_summary = get_summary(final_text, i)\n",
        "    scores = get_scores(report_summary, final_text)\n",
        "    \n",
        "    print('Volume:', i)\n",
        "    print('Score:', scores, '\\n')\n",
        "    \n",
        "    if scores > rouge_limit:\n",
        "        print('Optimal volume:', i, 'sentences', '\\n')\n",
        "        sent_number = i\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/498 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q9BhtLZnkQE"
      },
      "source": [
        "Enter the sentence number:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlbtTliunkQE",
        "outputId": "9125e44f-370b-4242-9413-d12e11bc5a82"
      },
      "source": [
        "%%time\r\n",
        "final_text = (wikitext + arxivtext + googletext)\r\n",
        "report_summary = get_summary(final_text, sent_number)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.9 s, sys: 1.01 s, total: 21.9 s\n",
            "Wall time: 15.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA9TOssynkQF"
      },
      "source": [
        "Get extractive summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P26DjtYOnkQF",
        "outputId": "88c1a4dd-84f0-4fb2-dc84-84c2b0115d91"
      },
      "source": [
        "%%time\n",
        "final_text = (wikitext + arxivtext + googletext)\n",
        "report_summary = get_summary(final_text, sent_number)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.8 s, sys: 1.07 s, total: 21.9 s\n",
            "Wall time: 15.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieZZN9fcnkQF"
      },
      "source": [
        "Summary metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glqGe6CLnkQF",
        "outputId": "d72af7fc-e0db-45bb-9e20-cf1a0017cc82"
      },
      "source": [
        "compression = round(len(word_tokenize(report_summary))/len(word_tokenize(final_text)),2) \n",
        "scores = get_scores(report_summary, final_text)\n",
        "\n",
        "print('Bigrams extracted:', str(scores))\n",
        "print('\\nCompression:', compression, '\\n')\n",
        "print('Efficiency:', str(round((scores/compression),2)), '\\n')  \n",
        "\n",
        "print((report_summary[:1000])+'...', '\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bigrams extracted: 0.33\n",
            "\n",
            "Compression: 0.2 \n",
            "\n",
            "Efficiency: 1.65 \n",
            "\n",
            "Biodegradable plastics are plastics that can be decomposed by the action of living organisms, usually microbes, into water, carbon dioxide, and biomass.==Biodegradable plastics are commonly produced with renewable raw materials, micro-organisms, petrochemicals, or combinations of all three.==While the words \"bioplastic\" and \"biodegradable plastic\" are similar, they are not synonymous.==Not all bioplastics (plastics derived partly or entirely from biomass) are biodegradable.==tw                                                            ABSTRACT    In this research, we present a simple and cost effective  soft lithographic process to fabricate PLA scaffolds for  tissue engineering.==In which, the negative photoresist JSR  THB-120N was spun on a glass subtract followed by  conventional UV lithographic processes to fabricate the  master to cast the PDMS elastomeric mold.==INTRODUCTION     Biodegradable polymers are currently used in a   number of biomedical applications, including sutures... \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia7YmdJCnkQG"
      },
      "source": [
        "Paraphrase generation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1bsi7zhnkQG",
        "scrolled": false,
        "outputId": "f0437609-9fcf-4c97-b824-fb19eb221638"
      },
      "source": [
        "%%time\n",
        "\n",
        "if paraphrase == True:\n",
        "    \n",
        "    model_name = 'tuner007/pegasus_paraphrase'\n",
        "    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "    \n",
        "    counter = 0\n",
        "    summ_list = []\n",
        "    \n",
        "    for i in report_summary.split('==')[:-1]:\n",
        "        summ_list.append('=='+ get_response(i,1)[0])\n",
        "    \n",
        "    summary = ' '.join(summ_list)\n",
        "\n",
        "    scores = scorer.score(summary, report_summary)\n",
        "    scores = round(100*list(list(scores.values())[0])[2])\n",
        "    report_summary = summary \n",
        "\n",
        "    print('Plagiarism:', (str(scores) + ' %'))\n",
        "\n",
        "    winsound.Beep(2500, 1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.15 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUgrzrICnkQH"
      },
      "source": [
        "Create keys with urls:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIN3lSCXnkQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af64c9c-9c3f-4bd5-f668-e59950802471"
      },
      "source": [
        "%%time\n",
        "ref_list = []\n",
        "pdf_list = []\n",
        "\n",
        "sent_list = list(report_summary.split(sep='=='))[:-1]\n",
        "\n",
        "for i in sent_list:\n",
        "    try:\n",
        "        df_score = df.copy()\n",
        "        df_score['score'] = df_score['text'].apply(lambda x: css(i,x))\n",
        "        df_score = df_score.sort_values(by=['score'], ascending=False)\n",
        "        \n",
        "        if str(df_score['link'].iloc[0]):\n",
        "            pdf_list.append(str(i))\n",
        "            ref_list.append(str(df_score['link'].iloc[0]))\n",
        "    except:\n",
        "        pdf_list.append('')\n",
        "\n",
        "pdf_summary = ''.join(pdf_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 23.2 s, sys: 21.1 ms, total: 23.2 s\n",
            "Wall time: 23.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rRQoiWunkQH"
      },
      "source": [
        "Create dataframe from tags and urls:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vethXX0nkQH",
        "outputId": "2b4de5bc-dbd5-40fc-b1ec-05e7971b82b2"
      },
      "source": [
        "%%time\n",
        "df_merged = pd.DataFrame(list(zip(ref_list, pdf_list)), columns=['link', 'text'])\n",
        "df_merged = df_merged.sort_index(ascending=True).groupby('link', as_index=True).agg(lambda x: ' '.join(x))\n",
        "df_merged = df_merged.reindex(list(unique_everseen(ref_list))).reset_index()\n",
        "\n",
        "df_merged.replace('', np.nan, inplace=True)\n",
        "df_merged.dropna(inplace=True) \n",
        "\n",
        "df_merged.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22.6 ms, sys: 917 µs, total: 23.5 ms\n",
            "Wall time: 33.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSN1RAWEnkQI"
      },
      "source": [
        "Add new sources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA4E0THqnkQI",
        "outputId": "aa6100ee-413e-4c47-adb0-f37f650b524e"
      },
      "source": [
        "%%time\n",
        "ref_list = []\n",
        "pdf_list = []\n",
        "\n",
        "trc = 0\n",
        "for i in range(len(df_merged)):\n",
        "    trc = trc + 1\n",
        "    \n",
        "    pdf_list.append(str(trc) + '. ...' + str(str(df_merged['text'].iloc[i])) + \" <u><a href=\" + str(df_merged['link'].iloc[i]) + \" target='_blank'>\" + \"More\" + \"</a></u>\" + \"<hr>\")\n",
        "    ref_list.append(str(df_merged['link'].iloc[i]))\n",
        "\n",
        "pdf_summary = ''.join(pdf_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.34 ms, sys: 0 ns, total: 1.34 ms\n",
            "Wall time: 6.54 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fqGvwEInkQJ"
      },
      "source": [
        "Save docx:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsLgM4F7nkQJ",
        "outputId": "0569abc0-8e16-455c-e634-855e8ac72122"
      },
      "source": [
        "save_doc(pdf_summary, 'summary', query, scores, compression)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}