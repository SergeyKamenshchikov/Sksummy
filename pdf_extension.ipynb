{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T19:02:21.225488Z",
     "start_time": "2021-01-08T19:01:58.184327Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import time, winsound, random\n",
    "from random import randint\n",
    "# general\n",
    "\n",
    "# process arrays and dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import fuzzy_pandas as fpd\n",
    "from collections import Counter\n",
    "#/process arrays and dataframes\n",
    "\n",
    "# parallel calculations\n",
    "from tqdm import tqdm\n",
    "#/parallel calculations\n",
    "\n",
    "# web parsing\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import chromedriver_binary\n",
    "from requests import get\n",
    "#/web parsing\n",
    "\n",
    "# parsing libs\n",
    "import arxiv\n",
    "from googlesearch import search \n",
    "#/parsing libs\n",
    "\n",
    "# read .pdf\n",
    "from tika import parser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "#/read .pdf\n",
    "\n",
    "# text processing\n",
    "import spacy,nltk,string,re\n",
    "import neuralcoref\n",
    "import networkx as nx\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from more_itertools import unique_everseen\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000\n",
    "#/text processing\n",
    "\n",
    "# extractive summarizer\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "#/extractive summarizer\n",
    "\n",
    "# many-to-many evaluation\n",
    "from rouge import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "#/many-to-many evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T19:02:25.688505Z",
     "start_time": "2021-01-08T19:02:25.645629Z"
    }
   },
   "outputs": [],
   "source": [
    "############## Extend abstract ##########\n",
    "def get_ngrams(text): \n",
    "    grams = nltk.ngrams(text.split(), 2)\n",
    "    grams_list = []\n",
    "    for i in grams:\n",
    "        grams_list.append(i)\n",
    "    \n",
    "    return grams_list \n",
    "\n",
    "def get_jaccard_sim(a,b):\n",
    "    a, b = set(get_ngrams(a)), set(get_ngrams(b)) \n",
    "    c = a.intersection(b)\n",
    "\n",
    "    return round(float(len(c)/len(a)), 2)\n",
    "\n",
    "def filter_text(content, abstract, threshold=0.5): \n",
    "    content_list = []   \n",
    "    \n",
    "    for j in content.split('.'):\n",
    "        try:\n",
    "            sim_score = get_jaccard_sim(j, abstract)\n",
    "        except:\n",
    "            sim_score = 0\n",
    "            \n",
    "        if sim_score > threshold:\n",
    "            content_list.append(j)    \n",
    "        \n",
    "        final_list = list(dict.fromkeys(abstract.split('.') + content_list))    \n",
    "             \n",
    "    return '. '.join(final_list)\n",
    "##############/Extend abstract ##########\n",
    "\n",
    "##### HTML parsing #####\n",
    "def parse_google_page(url): \n",
    "    try:\n",
    "        title = BeautifulSoup(get(url).content, 'html.parser').title.getText()\n",
    "        parser = HtmlParser.from_url(url, Tokenizer(\"English\"))\n",
    "        \n",
    "        summarizer = Summarizer(Stemmer(\"English\"))\n",
    "        summarizer.stop_words = get_stop_words(\"English\")\n",
    "\n",
    "        sentences = []\n",
    "        for i in summarizer(parser.document, 1000000):\n",
    "            sentences.append(str(i))\n",
    "        txt = ' '.join(sentences)\n",
    "    except:\n",
    "        txt = ''\n",
    "        title = ''\n",
    "    \n",
    "    return txt, title\n",
    "\n",
    "def parse_patent_page(url):\n",
    "    try:\n",
    "        soup = BeautifulSoup(get(url).text, 'html.parser')\n",
    "        \n",
    "        title = re.sub('[^A-Za-z0-9.]+', ' ', soup.title.getText()).replace('Google Patents','').strip()\n",
    "        descr = soup.find('section', attrs={'itemprop': 'description'}).getText().replace('\\n',' ').strip()\n",
    "        claims = soup.find('section', {'itemprop':'claims'}).getText().replace('\\n',' ').strip()\n",
    "        abstract = soup.abstract.getText().replace('\\n',' ').strip() \n",
    "        \n",
    "        abstract = re.sub('[^A-Za-z0-9.]+', ' ', abstract).replace('Google Patents','').strip()\n",
    "        descr = re.sub('[^A-Za-z0-9.]+', ' ', descr).replace('Google Patents','').strip()\n",
    "        claims = re.sub('[^A-Za-z0-9.]+', ' ', claims).replace('Google Patents','').strip()\n",
    "        \n",
    "        paragraphs = abstract + '; ' + descr + '; ' + claims \n",
    "        \n",
    "    except:\n",
    "        paragraphs = ''\n",
    "        title = ''  \n",
    "\n",
    "    return paragraphs, title\n",
    "\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "#####/HTML parsing #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T19:02:36.571173Z",
     "start_time": "2021-01-08T19:02:36.567211Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://arxiv.org/abs/2006.10213'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend test article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T19:02:46.956681Z",
     "start_time": "2021-01-08T19:02:40.782221Z"
    }
   },
   "outputs": [],
   "source": [
    "# load driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#/load driver\n",
    "    \n",
    "# get urls  \n",
    "driver.get(url)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "result_div = soup.find_all('blockquote', attrs={'class': 'abstract mathjax'})[0]\n",
    "abstract = result_div.get_text().replace('\\n',' ').replace('\\t',' ').strip()\n",
    "\n",
    "print(abstract)\n",
    "\n",
    "driver.stop_client()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect .pdf data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T19:03:55.541410Z",
     "start_time": "2021-01-08T19:03:51.239428Z"
    }
   },
   "outputs": [],
   "source": [
    "file_data = parser.from_file(url.replace('abs', 'pdf'))['content']\n",
    "content = file_data.replace('\\n',' ').replace('\\t',' ').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T19:04:44.230408Z",
     "start_time": "2021-01-08T19:04:44.118720Z"
    }
   },
   "outputs": [],
   "source": [
    "extended_abstract = filter_text(content, abstract, threshold=0.1).replace('\\n',' ').replace('\\t',' ').strip()\n",
    "print(extended_abstract)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
