{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:08:58.664487Z",
     "start_time": "2020-10-04T10:08:37.610758Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wikipediaapi\n",
    "import string\n",
    "    \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "from docx.shared import Pt\n",
    "import concurrent.futures\n",
    "from flask import Flask, render_template, request\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt\n",
    "import docx\n",
    "import time\n",
    "\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "from reportlab import platypus\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tika import parser\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from requests import get\n",
    "import random\n",
    "import html\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from random import randint\n",
    "import winsound\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:08:58.677461Z",
     "start_time": "2020-10-04T10:08:58.667480Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'docs/'\n",
    "ALLOWED_EXTENSIONS = set(['pdf', 'docx'])\n",
    "\n",
    "readtime = 2\n",
    "page_number = 5\n",
    "keys_number = 10\n",
    "\n",
    "process_time = 0\n",
    "\n",
    "wiki_FT = True\n",
    "google_FT = True\n",
    "arxiv_FT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:08:58.872990Z",
     "start_time": "2020-10-04T10:08:58.683435Z"
    }
   },
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "\n",
    "    return text\n",
    "\n",
    "def text_normalize(txt):\n",
    "    processed_text = (re.sub('[^a-zA-Z]', ' ', txt)).lower()\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def readingTime(mytext):\n",
    "    total_words = len(word_tokenize(mytext))\n",
    "    estimatedTime = round(total_words/200.0,1)\n",
    "    return estimatedTime\n",
    "\n",
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "\n",
    "def create_graph(text, common=keys_number):\n",
    "    tokens = text_normalize(text)\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(common), columns=['bigram', 'count'])\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "    F = nx.Graph()\n",
    "    for k, v in d[0].items():\n",
    "            F.add_edge(k[0], k[1], weight=(v*10))\n",
    "            pos = nx.spring_layout(F, iterations=500)\n",
    "    return F, pos\n",
    "\n",
    "def graph_keys(final_text, top_number):\n",
    "    F,pos = create_graph(final_text.lower())\n",
    "    nodes = []\n",
    "    degree = []\n",
    "\n",
    "    for i in F.nodes():\n",
    "        nodes.append(i)\n",
    "        degree.append(F.degree(i))\n",
    "\n",
    "    x = dict(zip(nodes, degree))\n",
    "    key_nodes = list({k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)})\n",
    "    tokens = text_normalize(final_text.lower())\n",
    "\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(100), columns=['bigram', 'count'])\n",
    "    key_bigrams = [' '.join(i) for i in list(bigram_df['bigram'])]\n",
    "\n",
    "    keys = [i for i in key_bigrams if (((i.split()[0] in key_nodes) or (i.split()[1] in key_nodes)) and (i.split()[0]!=i.split()[1]))][:top_number]\n",
    "    key_bigrams = ', '.join(keys)\n",
    "\n",
    "    return key_bigrams\n",
    "\n",
    "def highlight(txt, keys):\n",
    "    for k in keys:\n",
    "        txt = txt.replace(\" \" + k + \" \", \" \" + \"<mark style='background: #A7D1F2; font-size: 105%'><b>\" + k+ \"</b></mark>\" + \" \")\n",
    "\n",
    "    txt = html.unescape(txt)\n",
    "    return txt\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def wiki_scrape(topic_name, verbose=True):\n",
    "    def wiki_link(link):\n",
    "        try:\n",
    "            page = wiki_api.page(link)\n",
    "            if page.exists():\n",
    "                d = {'page': link, 'text': page.text, 'link': page.fullurl,\n",
    "                     'categories': list(page.categories.keys())}\n",
    "                return d\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    wiki_api = wikipediaapi.Wikipedia(language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(topic_name)\n",
    "    if not page_name.exists():\n",
    "        return\n",
    "    page_links = list(page_name.links.keys())\n",
    "    progress = tqdm(desc='Links Scraped', unit='', total=len(page_links)) if verbose else None\n",
    "    sources = [{'page': topic_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
    "                'categories': list(page_name.categories.keys())}]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_link = {executor.submit(wiki_link, link): link for link in page_links}\n",
    "        for future in concurrent.futures.as_completed(future_link):\n",
    "            data = future.result()\n",
    "            progress.update(1) if verbose else None\n",
    "            if data:\n",
    "                sources.append(data)\n",
    "    progress.close() if verbose else None\n",
    "    blacklist = ('Template', 'Help:', 'Category:', 'Portal:', 'Wikipedia:', 'Talk:')\n",
    "    sources = pd.DataFrame(sources)\n",
    "    sources = sources[(len(sources['text']) > 20)\n",
    "                      & ~(sources['page'].str.startswith(blacklist))]\n",
    "    sources['categories'] = sources.categories.apply(lambda x: [y[9:] for y in x])\n",
    "    sources['topic'] = topic_name\n",
    "    return sources\n",
    "\n",
    "def wiki_page(page_name):\n",
    "    wiki_api = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(page_name)\n",
    "    if not page_name.exists():\n",
    "        print('page does not exist')\n",
    "        return\n",
    "    page_data = {'page': page_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
    "                 'categories': [[y[9:] for y in list(page_name.categories.keys())]]}\n",
    "    page_data_df = pd.DataFrame(page_data)\n",
    "    return page_data_df\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def word_cloud(margin, background_color, tokens):\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    bigrams_list = list(nltk.bigrams(tokens))\n",
    "    dictionary = [' '.join(tup) for tup in bigrams_list]\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "    bag_of_words = vectorizer.fit_transform(dictionary)\n",
    "    words_freq = [(word, bag_of_words.sum(axis=0)[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    wordcloud = WordCloud(stopwords=stop_words, width=250, height=250, random_state=42, max_words=100, prefer_horizontal=1, margin=margin, background_color=background_color, max_font_size=30)\n",
    "    wordcloud.generate_from_frequencies(dict(words_freq))\n",
    "    wordcloud.to_file('images/cloud.png')\n",
    "\n",
    "    return True\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "def filter_triplet(final_text):\n",
    "    final_text = get_unique_text(final_text)\n",
    "    doc = nlp(final_text)\n",
    "    valid_sents=[]\n",
    "\n",
    "    for s in list(doc.sents):\n",
    "        if syntax_full(s):\n",
    "            valid_sents.append(s.text)\n",
    "    final_text = ' '.join(valid_sents)\n",
    "    return final_text\n",
    "\n",
    "def coref_res(rawtext, coref_greedn = 0.5):\n",
    "\n",
    "    neuralcoref.add_to_pipe(nlp, greedyness = coref_greedn, store_scores=False)\n",
    "    doc = nlp(rawtext)\n",
    "\n",
    "    resolved = list(tok.text_with_ws for tok in doc)\n",
    "\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        for coref in cluster:\n",
    "            if coref != cluster.main:\n",
    "                if coref.text[0].isalpha() and coref.text[0].isupper():\n",
    "\n",
    "                    main_words_list=word_tokenize(cluster.main.text)\n",
    "                    main_words_list[0]=main_words_list[0].capitalize()\n",
    "                    resolved[coref.start] = detokenizer(main_words_list) + doc[coref.end-1].whitespace_\n",
    "\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "            else:\n",
    "                resolved[coref.start] = cluster.main.text + doc[coref.end-1].whitespace_\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "\n",
    "    text_resolved = ''.join(resolved)\n",
    "    nlp.remove_pipe(\"neuralcoref\")\n",
    "\n",
    "    return text_resolved\n",
    "\n",
    "def check_min_num_of_clauses(spacy_sentence, n):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
    "            result.append(token.head.text)\n",
    "    if len(result)>=n:\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_sents_ids_whitelist(spacy_sents):\n",
    "    whitelist=[]\n",
    "    i=1\n",
    "    sents_texts=[]\n",
    "    for sent in spacy_sents:\n",
    "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
    "            whitelist.append(i)\n",
    "            sents_texts.append(sent.text)\n",
    "        i=i+1\n",
    "    return(whitelist)\n",
    "\n",
    "def compress(spacy_sents,sents_whitelist):\n",
    "    blacklist_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if token.dep_ in ['appos','advmod']:\n",
    "                    token_sub_tree=token.subtree\n",
    "                    for t in token_sub_tree:\n",
    "                        blacklist_tokens.append(t.i)\n",
    "\n",
    "        n=n+1\n",
    "    return(blacklist_tokens)\n",
    "\n",
    "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
    "    sents_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        sent_tokens=[]\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if (token.i not in blacklist_tokens):\n",
    "                    sent_tokens.append(token.text)\n",
    "            sents_tokens.append(sent_tokens)\n",
    "            sent_tokens=[]\n",
    "\n",
    "        n=n+1\n",
    "    return(sents_tokens)\n",
    "\n",
    "def detokenizer(list_of_tokens):\n",
    "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
    "    return(text_str)\n",
    "\n",
    "def sentence_grammar_fix(sentences):\n",
    "    fixed=[]\n",
    "    for sent in sentences:\n",
    "\n",
    "        sent=sent.strip()\n",
    "        sent=sent.replace('\\n','')\n",
    "        sent=sent.replace('()','')\n",
    "\n",
    "        sent=re.sub('\\s+',' ',sent)\n",
    "        sent=sent+'.'\n",
    "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0] in ['.',',','-','—']:\n",
    "                sent=sent[1:]\n",
    "        sent=sent.strip()\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0].isalpha():\n",
    "                sent=sent[0].upper()+sent[1:]\n",
    "        fixed.append(sent)\n",
    "\n",
    "    return(fixed)\n",
    "\n",
    "def spacy_compress(rawtext):\n",
    "\n",
    "    doc1 = nlp(rawtext)\n",
    "    sents_whitelist = get_sents_ids_whitelist(doc1.sents)\n",
    "\n",
    "    tokens_blacklist = compress(doc1.sents,sents_whitelist)\n",
    "    sents_tokens = get_list_sents_tokens(doc1.sents,sents_whitelist,tokens_blacklist)\n",
    "    compressed_text_sents = []\n",
    "\n",
    "    for s in sents_tokens:\n",
    "        text=detokenizer(s)\n",
    "        compressed_text_sents.append(text)\n",
    "    compressed_text_sents=sentence_grammar_fix(compressed_text_sents)\n",
    "    text =' '.join(compressed_text_sents)\n",
    "\n",
    "    return(text)\n",
    "\n",
    "def get_jaccard_sim(a,b):\n",
    "\n",
    "    a = set(a.split())\n",
    "    b = set(b.split())\n",
    "    c = a.intersection(b)\n",
    "\n",
    "    return float(len(c)/len(a))\n",
    "\n",
    "def parse_wiki(query):\n",
    "    wikitext = ''\n",
    "    df = pd.DataFrame()\n",
    "    try:\n",
    "        df = wiki_scrape(query, verbose=False)[:page_number]\n",
    "        df = df[['text','link','page']]\n",
    "\n",
    "        wikitext = '; '.join(list(df['text']))\n",
    "        wiki_keys = list(frozenset(list(df['page'])))\n",
    "        wiki_keys = ', '.join([i.lower() for i in wiki_keys])\n",
    "    except:\n",
    "        wikitext = ''\n",
    "\n",
    "    return wikitext, wiki_keys, df\n",
    "\n",
    "def parse_arxiv(query):\n",
    "    arxivtext = ''\n",
    "    urls = []\n",
    "    titles = []\n",
    "\n",
    "    closest_value = 200\n",
    "    req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value)+'&searchtype=all&source=header&start=0'\n",
    "    htmlString = get(req)\n",
    "\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib')  \n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "\n",
    "    titles = list(soup.find_all('p', {'class' : 'title is-5 mathjax'}))[:page_number]\n",
    "    titles_r = [i.text.replace('\\n','').replace('  ','') for i in titles]\n",
    "    titles = ', '.join(titles_r)\n",
    "\n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])\n",
    "\n",
    "    txt = []\n",
    "    for i in urls[:page_number]:\n",
    "        time.sleep(random.randint(1,8))\n",
    "        soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "        \n",
    "        abstract = ' '.join(soup.find('blockquote').text.replace('\\n','').replace('  ',' ').split())       \n",
    "        abstract = abstract.replace('abstract','').replace('Abstract','')       \n",
    "        txt.append(abstract)\n",
    "\n",
    "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, urls, titles_r)), columns=['text','link', 'page'])\n",
    "\n",
    "    return arxivtext, titles, df\n",
    "\n",
    "# common string #\n",
    "def longest_common_substring(s1, s2):\n",
    "  m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "  longest, x_longest = 0, 0\n",
    "  for x in range(1, 1 + len(s1)):\n",
    "    for y in range(1, 1 + len(s2)):\n",
    "      if s1[x - 1] == s2[y - 1]:\n",
    "        m[x][y] = m[x - 1][y - 1] + 1\n",
    "        if m[x][y] > longest:\n",
    "          longest = m[x][y]\n",
    "          x_longest = x\n",
    "      else:\n",
    "        m[x][y] = 0\n",
    "  return s1[x_longest - longest: x_longest]\n",
    "\n",
    "def longest_common_sentence(s1, s2):\n",
    "    s1_words = s1.split(' ')\n",
    "    s2_words = s2.split(' ')\n",
    "    return ' '.join(longest_common_substring(s1_words, s2_words))\n",
    "\n",
    "def css(a,b):\n",
    "    if len(a.split()) > 0:\n",
    "        score = len(longest_common_sentence(a,b).split())/len(a.split())\n",
    "    else:    \n",
    "        score = 0\n",
    "    return score \n",
    "#/common string #\n",
    "\n",
    "def parse_page(url):\n",
    "    htmlString = get(url).text\n",
    "    soup = BeautifulSoup(htmlString, 'html.parser')\n",
    "    paragraphs = soup.find('body')\n",
    "    txt = text_from_html(str(paragraphs))\n",
    "\n",
    "    return txt\n",
    "\n",
    "def parse_google(query, keys_number, page_number):   \n",
    "    # load driver\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    #/load driver\n",
    "\n",
    "    # get urls\n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1)\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    #/ get urls\n",
    "\n",
    "    # load content \n",
    "    txt = []\n",
    "    titles = []\n",
    "    for j in tqdm(url_list):\n",
    "        delta = random.randint(1,1)\n",
    "        time.sleep(delta)\n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'): \n",
    "                file_data = parser.from_file(str(j))           \n",
    "                t = file_data['content']\n",
    "            else:\n",
    "                t = parse_page(j)\n",
    "            txt.append(t)\n",
    "            titles.append(''.join(sent_tokenize(t)[:3]))\n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "       \n",
    "    googletext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "   \n",
    "    return googletext, errors, df\n",
    "\n",
    "def add_hyperlink(paragraph, text, url):\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element and a new w:rPr element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    # Create a new Run object and add the hyperlink into it\n",
    "    r = paragraph.add_run ()\n",
    "    r._r.append (hyperlink)\n",
    "\n",
    "    # A workaround for the lack of a hyperlink style (doesn't go purple after using the link)\n",
    "    # Delete this if using a template that has the hyperlink style in it\n",
    "    r.font.color.theme_color = MSO_THEME_COLOR_INDEX.HYPERLINK\n",
    "    r.font.underline = True\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "def save_doc(final_summary, summary, query, docs_number, readtime):\n",
    "    sent_list = list(final_summary.split(sep='<hr>'))\n",
    "    doc = Document()\n",
    "    style = doc.styles['Normal']\n",
    "    font = style.font\n",
    "    font.name = 'Times New Roman'\n",
    "    font.size = Pt(12)\n",
    "\n",
    "    hd = doc.add_paragraph()\n",
    "    hd.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "    hd.add_run('Summary').bold = True\n",
    "\n",
    "    if query != 'none':\n",
    "        hd = doc.add_paragraph('Request: ' + query)\n",
    "\n",
    "    hd = doc.add_paragraph('Read time: ' + str(readtime) + ' min')\n",
    "    hd = doc.add_paragraph('Documents: ' + str(docs_number))\n",
    "    r = hd.add_run()\n",
    "\n",
    "    for i in sent_list:\n",
    "        hd.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "        if query != 'none':\n",
    "            try:\n",
    "                link = re.search(r\"<a href=(.*?)target='_blank'\", str(i)).group(1).replace(' ','')\n",
    "                hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))\n",
    "                add_hyperlink(hd, 'More', link)\n",
    "            except:\n",
    "                link = ''\n",
    "        if query == 'none':\n",
    "            hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))\n",
    "\n",
    "    doc.save('docs/' + summary + '.docx')\n",
    "    return True\n",
    "\n",
    "def get_summary(rawtext, readtime):\n",
    "    sentences = int(readtime/(np.median([len(i.split()) for i in nltk.sent_tokenize(rawtext)])/200))\n",
    "\n",
    "    stemmer = Stemmer(\"english\")\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(\"english\")\n",
    "    parser = PlaintextParser.from_string(' '.join(sent_tokenize(rawtext)[6:]), Tokenizer(\"english\"))\n",
    "\n",
    "    text_list = []\n",
    "    for sentence in summarizer(parser.document, sentences):\n",
    "        text_list.append(str(sentence))\n",
    "\n",
    "    txt = ' '.join(sent_tokenize(rawtext)[:6]) + ' '+' '.join(text_list)\n",
    "\n",
    "    z = 0\n",
    "    output = []\n",
    "    pdf_output = []\n",
    "\n",
    "    for i in nltk.sent_tokenize(txt):\n",
    "        z = z+1\n",
    "        output.append('\\n\\n<hr>' + str(z) + '. ' + str(i))\n",
    "        pdf_output.append('<hr>' + str(z) + '. ' + str(i))\n",
    "\n",
    "    txt = ''.join(output) + '<hr>'\n",
    "    pdf_txt = ''.join(pdf_output) + '<hr>'\n",
    "    return txt, pdf_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:09:15.946335Z",
     "start_time": "2020-10-04T10:09:08.761503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer development\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Wiki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:09:36.783686Z",
     "start_time": "2020-10-04T10:09:19.655212Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wikitext = ''\n",
    "wikikeys = ''\n",
    "wiki_time = 0\n",
    "df_wiki = pd.DataFrame()\n",
    "\n",
    "if wiki_FT == True:\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        wikitext = parse_wiki(query)[0]\n",
    "        wikikeys = parse_wiki(query)[1]\n",
    "        df_wiki = parse_wiki(query)[2]  \n",
    "    except:\n",
    "        print('Parsing error')\n",
    "    \n",
    "    end = time.time()\n",
    "    wiki_time = end - start\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Arxiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:11:45.629784Z",
     "start_time": "2020-10-04T10:10:25.896751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arxivtext = ''\n",
    "arxivkeys = ''\n",
    "arxiv_time = 0\n",
    "df_arxiv = pd.DataFrame()\n",
    "start = time.time()\n",
    "\n",
    "if arxiv_FT == True:\n",
    "    try:\n",
    "        arxivtext = parse_arxiv(query)[0]\n",
    "        arxivkeys = graph_keys(parse_arxiv(query)[1], keys_number)\n",
    "        df_arxiv = parse_arxiv(query)[2]\n",
    "    except:\n",
    "        print('Parsing error')\n",
    "    \n",
    "end = time.time()\n",
    "arxiv_time = end - start\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:12:05.084213Z",
     "start_time": "2020-10-04T10:11:45.633757Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:444: DeprecationWarning: use options instead of chrome_options\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "googletext = ''\n",
    "googlekeys = ''\n",
    "google_time = 0\n",
    "df_google = pd.DataFrame()\n",
    "\n",
    "if google_FT == True:\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        gp = parse_google(query, keys_number, page_number)\n",
    "        googlekeys = graph_keys(gp[0], keys_number)\n",
    "        googletext = gp[0]   \n",
    "        df_google = gp[2]\n",
    "    except:\n",
    "        print('Parsing error')\n",
    "    \n",
    "    end = time.time()\n",
    "    google_time = end - start\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:12:05.102164Z",
     "start_time": "2020-10-04T10:12:05.089200Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_wiki.append(df_arxiv).append(df_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:12:53.753683Z",
     "start_time": "2020-10-04T10:12:05.105157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "\n",
    "final_text = wikitext + ' ' + arxivtext + ' ' + googletext\n",
    "final_text = filter_triplet(final_text)\n",
    "final_text = coref_res(spacy_compress(filter_triplet(final_text)))\n",
    "\n",
    "end = time.time()\n",
    "filter_time = end - start\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:12:56.913275Z",
     "start_time": "2020-10-04T10:12:53.774627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "report_summary = get_summary(final_text, readtime)[1]\n",
    "end = time.time()\n",
    "summary_time = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:01.905002Z",
     "start_time": "2020-10-04T10:12:56.917264Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_list = []\n",
    "sent_list = list(report_summary.split(sep='<hr>'))[1:]\n",
    "\n",
    "for i in sent_list[:-1]:\n",
    "    try:\n",
    "        df['score'] = df['text'].apply(lambda x: css(i,x))\n",
    "        df = df.sort_values(by=['score'], ascending=False)\n",
    "        if str(df['link'].iloc[0]):\n",
    "            pdf_list.append(str(str(i) + \" <u><a href=\" + str(df['link'].iloc[0]) + \" target='_blank'>\" + \"More\" + \"</a></u>\" + \"<hr>\"))\n",
    "    except:\n",
    "        pdf_list.append('')\n",
    "\n",
    "pdf_summary = ''.join(pdf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save docx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:02.151376Z",
     "start_time": "2020-10-04T10:13:01.910973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_doc(pdf_summary, 'summary', query, len(df), readtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:03.173849Z",
     "start_time": "2020-10-04T10:13:02.156321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precessing time: 2 min\n",
      "Memory size: 76 mb\n",
      "Number of documents: 15\n"
     ]
    }
   ],
   "source": [
    "total_time = summary_time + filter_time + google_time + arxiv_time + wiki_time\n",
    "memory_size = int(len(final_text.encode('utf-8'))/1024)\n",
    "total_time = int(total_time/60)\n",
    "docs_number = len(df)\n",
    "\n",
    "print('Precessing time:', total_time, 'min')\n",
    "print('Memory size:', memory_size, 'mb')\n",
    "print('Number of documents:', docs_number)\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
