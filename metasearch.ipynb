{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:10:36.227316Z",
     "start_time": "2020-11-29T10:10:08.205602Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wikipediaapi\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import fuzzy_pandas as fpd\n",
    "from collections import Counter\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "    \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "from docx.shared import Pt\n",
    "import concurrent.futures\n",
    "from flask import Flask, render_template, request\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt\n",
    "import docx\n",
    "import time\n",
    "\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "from reportlab import platypus\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import numpy as np\n",
    "from tika import parser\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from requests import get\n",
    "import random\n",
    "import html\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from random import randint\n",
    "import winsound\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:09:46.035256Z",
     "start_time": "2020-11-29T10:09:45.776947Z"
    }
   },
   "outputs": [],
   "source": [
    "##### HTML parsing #####\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "#####/HTML parsing #####\n",
    "\n",
    "##### Text preprocessing #####\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def text_normalize(txt):\n",
    "    processed_text = (re.sub('[^a-zA-Z]', ' ', txt)).lower()\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "##### Text preprocessing #####\n",
    "\n",
    "def readingTime(mytext):\n",
    "    total_words = len(word_tokenize(mytext))\n",
    "    estimatedTime = round(total_words/200.0,1)\n",
    "    return estimatedTime\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "\n",
    "def graph_keys(final_text, top_number):\n",
    "    \n",
    "    bigrams = list(nltk.ngrams(text_normalize(final_text),2))\n",
    "    bigrams = [' '.join(i) for i in bigrams if (i[0]!=i[1])] \n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    \n",
    "    df = pd.DataFrame(bigram_counts.most_common(len(bigram_counts)), columns=['bigram', 'count'])[:top_number]\n",
    "    df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
    "    keys = ', '.join(list(df['bigram'].astype(str) + ' (' + df['count'].astype(int).astype(str) + '%)'))\n",
    "\n",
    "    return keys\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def filter_triplet(final_text):\n",
    "    final_text = get_unique_text(final_text)\n",
    "    doc = nlp(final_text)\n",
    "    valid_sents=[]\n",
    "\n",
    "    for s in list(doc.sents):\n",
    "        if syntax_full(s):\n",
    "            valid_sents.append(s.text)\n",
    "    final_text = ' '.join(valid_sents)\n",
    "    return final_text\n",
    "\n",
    "def coref_res(rawtext, coref_greedn = 0.5):\n",
    "\n",
    "    neuralcoref.add_to_pipe(nlp, greedyness = coref_greedn, store_scores=False)\n",
    "    doc = nlp(rawtext)\n",
    "\n",
    "    resolved = list(tok.text_with_ws for tok in doc)\n",
    "\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        for coref in cluster:\n",
    "            if coref != cluster.main:\n",
    "                if coref.text[0].isalpha() and coref.text[0].isupper():\n",
    "\n",
    "                    main_words_list=word_tokenize(cluster.main.text)\n",
    "                    main_words_list[0]=main_words_list[0].capitalize()\n",
    "                    resolved[coref.start] = detokenizer(main_words_list) + doc[coref.end-1].whitespace_\n",
    "\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "            else:\n",
    "                resolved[coref.start] = cluster.main.text + doc[coref.end-1].whitespace_\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "\n",
    "    text_resolved = ''.join(resolved)\n",
    "    nlp.remove_pipe(\"neuralcoref\")\n",
    "\n",
    "    return text_resolved\n",
    "\n",
    "def check_min_num_of_clauses(spacy_sentence, n):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
    "            result.append(token.head.text)\n",
    "    if len(result)>=n:\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_sents_ids_whitelist(spacy_sents):\n",
    "    whitelist=[]\n",
    "    i=1\n",
    "    sents_texts=[]\n",
    "    for sent in spacy_sents:\n",
    "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
    "            whitelist.append(i)\n",
    "            sents_texts.append(sent.text)\n",
    "        i=i+1\n",
    "    return(whitelist)\n",
    "\n",
    "def compress(spacy_sents,sents_whitelist):\n",
    "    blacklist_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if token.dep_ in ['appos','advmod']:\n",
    "                    token_sub_tree=token.subtree\n",
    "                    for t in token_sub_tree:\n",
    "                        blacklist_tokens.append(t.i)\n",
    "\n",
    "        n=n+1\n",
    "    return(blacklist_tokens)\n",
    "\n",
    "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
    "    sents_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        sent_tokens=[]\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if (token.i not in blacklist_tokens):\n",
    "                    sent_tokens.append(token.text)\n",
    "            sents_tokens.append(sent_tokens)\n",
    "            sent_tokens=[]\n",
    "\n",
    "        n=n+1\n",
    "    return(sents_tokens)\n",
    "\n",
    "def detokenizer(list_of_tokens):\n",
    "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
    "    return(text_str)\n",
    "\n",
    "def sentence_grammar_fix(sentences):\n",
    "    fixed=[]\n",
    "    for sent in sentences:\n",
    "\n",
    "        sent=sent.strip()\n",
    "        sent=sent.replace('\\n','')\n",
    "        sent=sent.replace('()','')\n",
    "\n",
    "        sent=re.sub('\\s+',' ',sent)\n",
    "        sent=sent+'.'\n",
    "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0] in ['.',',','-','—']:\n",
    "                sent=sent[1:]\n",
    "        sent=sent.strip()\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0].isalpha():\n",
    "                sent=sent[0].upper()+sent[1:]\n",
    "        fixed.append(sent)\n",
    "\n",
    "    return(fixed)\n",
    "\n",
    "def spacy_compress(rawtext):\n",
    "\n",
    "    doc1 = nlp(rawtext)\n",
    "    sents_whitelist = get_sents_ids_whitelist(doc1.sents)\n",
    "\n",
    "    tokens_blacklist = compress(doc1.sents,sents_whitelist)\n",
    "    sents_tokens = get_list_sents_tokens(doc1.sents,sents_whitelist,tokens_blacklist)\n",
    "    compressed_text_sents = []\n",
    "\n",
    "    for s in sents_tokens:\n",
    "        text=detokenizer(s)\n",
    "        compressed_text_sents.append(text)\n",
    "    compressed_text_sents=sentence_grammar_fix(compressed_text_sents)\n",
    "    text =' '.join(compressed_text_sents)\n",
    "\n",
    "    return(text)\n",
    "\n",
    "############# Parse Wiki ############# \n",
    "def parse_wiki(google_url):\n",
    "    \n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver\n",
    "    \n",
    "    # get urls  \n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    url_list = [i for i in url_list if 'https://en.wikipedia.org' in i]\n",
    "    url_list = [i.split('/')[4] for i in url_list if 'https://en.wikipedia.org' in i]\n",
    "     #/ get urls\n",
    "        \n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "    \n",
    "    return url_list     \n",
    "############# Parse Wiki ############# \n",
    "\n",
    "############# Parse Arxiv #############\n",
    "def parse_arxiv(query, delta_months):\n",
    "    \n",
    "    arxivtext = ''\n",
    "    urls = []\n",
    "    titles = []\n",
    "\n",
    "    closest_value = 100\n",
    "    req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value)\n",
    "    req = req + '&searchtype=all&source=header&start=0&date-filter_by=past_' + str(delta_months) \n",
    "    \n",
    "    htmlString = get(req)\n",
    "\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib')\n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "\n",
    "    titles = list(soup.find_all('p', {'class' : 'title is-5 mathjax'}))[:page_number]\n",
    "    titles_r = [i.text.replace('\\n','').replace('  ','') for i in titles]\n",
    "    titles = ', '.join(titles_r)\n",
    "\n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])\n",
    "\n",
    "    txt = []\n",
    "    for i in urls[:page_number]:\n",
    "        time.sleep(random.randint(1,8))\n",
    "        soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "        abstract = ' '.join(soup.find('blockquote').text.replace('  ',' ').split())\n",
    "        txt.append(abstract)\n",
    "\n",
    "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, urls, titles_r)), columns=['text','link', 'page'])\n",
    "\n",
    "    return arxivtext, titles, df\n",
    "#############/Parse Arxiv #############\n",
    "\n",
    "def longest_common_substring(s1, s2):\n",
    "  m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "  longest, x_longest = 0, 0\n",
    "  for x in range(1, 1 + len(s1)):\n",
    "    for y in range(1, 1 + len(s2)):\n",
    "      if s1[x - 1] == s2[y - 1]:\n",
    "        m[x][y] = m[x - 1][y - 1] + 1\n",
    "        if m[x][y] > longest:\n",
    "          longest = m[x][y]\n",
    "          x_longest = x\n",
    "      else:\n",
    "        m[x][y] = 0\n",
    "  return s1[x_longest - longest: x_longest]\n",
    "\n",
    "def longest_common_sentence(s1, s2):\n",
    "    s1_words = s1.split(' ')\n",
    "    s2_words = s2.split(' ')\n",
    "    return ' '.join(longest_common_substring(s1_words, s2_words))\n",
    "\n",
    "def css(a,b):\n",
    "    if len(a.split()) > 0:\n",
    "        score = len(longest_common_sentence(a,b).split())/len(a.split())\n",
    "    else:    \n",
    "        score = 0\n",
    "    return score \n",
    "#/common string #\n",
    "\n",
    "def parse_page(url,tag,cls=''): \n",
    "    \n",
    "    htmlString = get(url).text\n",
    "    soup = BeautifulSoup(htmlString, 'html.parser')\n",
    "    paragraphs = soup.find(tag,cls)\n",
    "    txt = text_from_html(str(paragraphs))\n",
    "\n",
    "    return txt\n",
    "\n",
    "############# Parse Google #############\n",
    "def parse_google(query, keys_number, page_number, tag, D1, D2, cls=''):   \n",
    "    \n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver\n",
    "\n",
    "    # get urls\n",
    "    timed_query = query +' after:' + str(D1) +' before:' + str(D2) \n",
    "    google_url = \"https://www.google.com/search?q=\" + timed_query + \"&num=\" + str(page_number+1)\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    #/ get urls\n",
    "\n",
    "    # load content \n",
    "    txt = []\n",
    "    titles = []\n",
    "    for j in tqdm(url_list):\n",
    "        delta = random.randint(1,1)\n",
    "        time.sleep(delta)\n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'): \n",
    "                file_data = parser.from_file(str(j))           \n",
    "                t = file_data['content']\n",
    "            else:\n",
    "                t = parse_page(j,tag,cls)\n",
    "            txt.append(t)\n",
    "            titles.append(''.join(sent_tokenize(t)[:3]))\n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "       \n",
    "    googletext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    \n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "   \n",
    "    return googletext, errors, df\n",
    "#############/Parse Google #############\n",
    "\n",
    "############# Doc preparation ##########\n",
    "def add_hyperlink(paragraph, text, url, flag):\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element and a new w:rPr element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    # Create a new Run object and add the hyperlink into it\n",
    "    r = paragraph.add_run()\n",
    "    r._r.append (hyperlink) \n",
    "\n",
    "    # A workaround for the lack of a hyperlink style (doesn't go purple after using the link)\n",
    "    # Delete this if using a template that has the hyperlink style in it\n",
    "    r.font.color.theme_color = MSO_THEME_COLOR_INDEX.HYPERLINK\n",
    "    r.font.underline = flag\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "\n",
    "def save_doc(final_summary, summary, query, docs_number, readtime, url_keys, url_marks):\n",
    "    \n",
    "    sent_list = list(final_summary.split(sep='<hr>'))\n",
    "    doc = Document()\n",
    "    style = doc.styles['Normal']\n",
    "    \n",
    "    font = style.font\n",
    "    font.name = 'Times New Roman'\n",
    "    font.size = Pt(12)\n",
    "\n",
    "    hd = doc.add_paragraph()\n",
    "    hd.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "    hd.add_run('Summary').bold = True\n",
    "\n",
    "    if query != 'none':\n",
    "        hd = doc.add_paragraph('Request: ' + \"''\" + query + \"''\")\n",
    "\n",
    "    hd = doc.add_paragraph('Read time: ' + str(readtime) + ' min')\n",
    "    hd = doc.add_paragraph('Documents: ' + str(docs_number))\n",
    "    \n",
    "    hd = doc.add_paragraph('')\n",
    "    \n",
    "    hd.add_run('Keys:\\n').underline = True\n",
    "    \n",
    "    for j in url_keys:\n",
    "        if j != url_keys[-1]:\n",
    "            add_hyperlink(hd, (str(j.split('/keyword/')[1]) + ', '), str(j.split('/keyword/')[0]), False)\n",
    "        else:\n",
    "            add_hyperlink(hd, (str(j.split('/keyword/')[1])), str(j.split('/keyword/')[0]), False)\n",
    "        \n",
    "    hd.add_run('\\n\\nBenchmarks:\\n').underline = True\n",
    "    \n",
    "    for j in url_marks:\n",
    "        if j != url_marks[-1]:\n",
    "            add_hyperlink(hd, str(j.split('/keyword/')[1]) + ', ', str(j.split('/keyword/')[0]), False)\n",
    "        else:\n",
    "            add_hyperlink(hd, str(j.split('/keyword/')[1]), str(j.split('/keyword/')[0]), False)\n",
    "    \n",
    "    r = hd.add_run()\n",
    "    for i in sent_list:\n",
    "        hd.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "        if query != 'none':\n",
    "            try:\n",
    "                link = re.search(r\"<a href=(.*?)target='_blank'\", str(i)).group(1).replace(' ','')\n",
    "                hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))               \n",
    "                add_hyperlink(hd, 'More', link, True).add_run()\n",
    "            except:\n",
    "                link = ''\n",
    "        if query == 'none':\n",
    "            hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))    \n",
    "         \n",
    "    doc.save('docs/' + summary + '.docx')\n",
    "    \n",
    "    return True\n",
    "#############/Doc preparation ##########\n",
    "\n",
    "def get_summary(rawtext, readtime):\n",
    "    sentences = int(readtime/(np.median([len(i.split()) for i in nltk.sent_tokenize(rawtext)])/200))\n",
    "\n",
    "    stemmer = Stemmer(\"english\")\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(\"english\")\n",
    "    parser = PlaintextParser.from_string(' '.join(sent_tokenize(rawtext)[6:]), Tokenizer(\"english\"))\n",
    "\n",
    "    text_list = []\n",
    "    for sentence in summarizer(parser.document, sentences):\n",
    "        text_list.append(str(sentence))\n",
    "\n",
    "    txt = ' '.join(sent_tokenize(rawtext)[:6]) + ' '+' '.join(text_list)\n",
    "\n",
    "    z = 0\n",
    "    output = []\n",
    "    pdf_output = []\n",
    "\n",
    "    for i in nltk.sent_tokenize(txt):\n",
    "        z = z+1\n",
    "        output.append('\\n\\n<hr>' + str(z) + '. ' + str(i))\n",
    "        pdf_output.append('<hr>' + str(z) + '. ' + str(i))\n",
    "\n",
    "    txt = ''.join(output) + '<hr>'\n",
    "    pdf_txt = ''.join(pdf_output) + '<hr>'\n",
    "    return txt, pdf_txt\n",
    "\n",
    "def get_entities(rawtext, tops):\n",
    "    \n",
    "    spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
    "    doc = spacy_nlp(rawtext)\n",
    "\n",
    "    ners = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PERSON']:\n",
    "            ners.append(ent.text)\n",
    "   \n",
    "    ner_counts = collections.Counter(ners)\n",
    "\n",
    "    df = pd.DataFrame(ner_counts.most_common(len(ner_counts)), columns=['ner_names', 'count'])[:tops]\n",
    "    df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
    "    keys = ', '.join(list(df['ner_names'].astype(str) + ' (' + df['count'].astype(int).astype(str) + '%)'))\n",
    "    \n",
    "    return keys\n",
    "\n",
    "def get_jaccard_sim(a,b):\n",
    "\n",
    "    a = set(a.split())\n",
    "    b = set(b.split())\n",
    "    c = a.intersection(b)\n",
    "\n",
    "    return float(len(c)/len(a))\n",
    "\n",
    "def filter_text(page_txt, page_sum, threshold=0.5): \n",
    "    sent_list = []  \n",
    "    for j in page_txt.split('.'):\n",
    "        try:\n",
    "            sim_score = get_jaccard_sim(j, page_sum)\n",
    "        except:\n",
    "            sim_score = 0\n",
    "            \n",
    "        if sim_score > threshold:\n",
    "            sent_list.append(j + '.')\n",
    "            \n",
    "    return sent_list        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:10:36.255351Z",
     "start_time": "2020-11-29T10:10:36.237398Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'docs/'\n",
    "\n",
    "readtime = 5\n",
    "\n",
    "page_number = 10\n",
    "keys_number = 10\n",
    "\n",
    "process_time = 0\n",
    "delta_months = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:11:47.225637Z",
     "start_time": "2020-11-29T10:11:41.144359Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:11:52.059117Z",
     "start_time": "2020-11-29T10:11:52.053132Z"
    }
   },
   "outputs": [],
   "source": [
    "D1 = (dt.now() - relativedelta(months=delta_months)).strftime('%Y-%m-%d')\n",
    "D2 = dt.now().strftime('%Y-%m-%d')\n",
    "\n",
    "red_query = (query + \" before:\" + str(D2) + \" after:\" + str(D1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Wiki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:12:35.082174Z",
     "start_time": "2020-11-29T10:11:56.168678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Trying to download new driver from http://chromedriver.storage.googleapis.com/87.0.4280.20/chromedriver_win32.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\skamenshchikov\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.20]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: machine algorithm (14%), computer vision (11%), deep network (11%), deep belief (11%), speech recognition (11%), belief network (9%), study computer (7%), field study (7%), computer science (7%), machine model (7%) \n",
      "\n",
      "|Entities|: ASR (18%), MDP (9%), Deep (9%), DefinitionDeep (9%), OverviewMost (9%), Boltzmann (9%), InterpretationsDeep (9%), Cresceptron (9%), Neocognitron (9%), ANNs (9%) \n",
      "\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wikitext = ''\n",
    "wikikeys = ''\n",
    "\n",
    "df_wiki = pd.DataFrame()\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "\n",
    "red_query = \"https://www.google.com/search?q=\" + 'site:https://en.wikipedia.org ' + query + \"&num=\" + str(page_number+1)\n",
    "wiki_titles = parse_wiki(red_query)\n",
    "\n",
    "txts = []\n",
    "titles = []\n",
    "\n",
    "for i in tqdm(wiki_titles):  \n",
    "    \n",
    "    page_sum = wiki_wiki.page(i).summary\n",
    "    page_txt = wiki_wiki.page(i).text\n",
    "    sent_list = filter_text(page_txt, page_sum, threshold=0.5)\n",
    "            \n",
    "    titles.append(i)\n",
    "    txts.append(' '.join(sent_list).replace('\\n', ''))        \n",
    "    \n",
    "wikitext = ''.join(txts).replace('\\n','')\n",
    "wikikeys = graph_keys(wikitext, keys_number)\n",
    "wiki_entities = get_entities(wikitext, keys_number)\n",
    "\n",
    "url_list = [str('https://en.wikipedia.org/wiki/' + i)  for i in wiki_titles] \n",
    "df_wiki = pd.DataFrame(list(zip(txts, url_list, titles)), columns=['text','link', 'page'])\n",
    "\n",
    "print('|Keywords|:', wikikeys, '\\n')\n",
    "print('|Entities|:', wiki_entities, '\\n')\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Arxiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:14:38.109846Z",
     "start_time": "2020-11-29T10:13:08.238648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: subclass label (13%), training data (13%), knowledge distillation (13%), action distribution (10%), classification task (10%), state art (10%), world agent (10%), abstract world (6%), hidden stratification (6%), image classification (6%) \n",
      "\n",
      "|Entities|: GEORGE (16%), LQG (11%), MFG (11%), ImageNet (11%), PermaKey (11%), Atari (11%), ICLR (11%), Generative Adversarial Networks (5%), Clustered Optimization of LAtent (5%), COLA (5%) \n",
      "\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arxivtext = ''\n",
    "arxivkeys = ''\n",
    "\n",
    "df_arxiv = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    arxiv_data = parse_arxiv(query, delta_months)\n",
    "    arxivtext = arxiv_data[0]\n",
    "    \n",
    "    arxiv_entities = get_entities(arxivtext, keys_number)\n",
    "    arxivkeys = graph_keys(arxivtext, keys_number)\n",
    "    \n",
    "    print('|Keywords|:', arxivkeys, '\\n')\n",
    "    print('|Entities|:', arxiv_entities, '\\n')\n",
    "    \n",
    "    df_arxiv = arxiv_data[2]\n",
    "except:\n",
    "    print('No data')\n",
    "    \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:19:13.570349Z",
     "start_time": "2020-11-29T10:18:23.847586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Driver [C:\\Users\\skamenshchikov\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.20\\chromedriver.exe] found in cache\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:19<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: machine tool (12%), data fabric (12%), customer story (12%), quick link (11%), story try (11%), link data (10%), google cloud (9%), hybrid cloud (6%), cloud automl (5%), cloud service (5%) \n",
      "\n",
      "|Entities|: NetApp (24%), DZone (10%), GPU (10%), Watson (10%), ML (8%), Google Cloud (8%), Microsoft (8%), IBM (8%), GUI (5%), Scikit (5%) \n",
      "\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "googletext = ''\n",
    "googlekeys = ''\n",
    "\n",
    "df_google = pd.DataFrame()\n",
    "    \n",
    "try:\n",
    "    google_data = parse_google(query, keys_number, page_number, 'body', D1, D2)\n",
    "    googletext = google_data[0]\n",
    "        \n",
    "    google_entities = get_entities(googletext, keys_number)\n",
    "    googlekeys = graph_keys(googletext, keys_number)\n",
    "    \n",
    "    print('|Keywords|:', googlekeys, '\\n')\n",
    "    print('|Entities|:', google_entities, '\\n')\n",
    "  \n",
    "    df_google = google_data[2]\n",
    "except:\n",
    "    print('No data')\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google Patents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:29:22.774057Z",
     "start_time": "2020-11-29T10:28:31.427474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Driver [C:\\Users\\skamenshchikov\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.20\\chromedriver.exe] found in cache\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:21<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: body feature (19%), antibody affinity (13%), client device (11%), affinity antigen (8%), fault machine (8%), determination data (8%), cognitive intelligence (8%), update matrix (8%), measurement body (8%), method system (5%) \n",
      "\n",
      "|Entities|: GPU (75%), IoT (25%) \n",
      "\n",
      "Wall time: 51.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "patenttext = ''\n",
    "patentkeys = ''\n",
    "\n",
    "df_patent = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    z = parse_google(\"site:https://patents.google.com \" + query, keys_number, page_number, 'div', D1, D2, 'abstract')\n",
    "    patent_text = z[0] \n",
    "    \n",
    "    patent_entities = get_entities(patent_text, keys_number)\n",
    "    patentkeys = graph_keys(patent_text, keys_number)\n",
    "    \n",
    "    print('|Keywords|:', patentkeys, '\\n')\n",
    "    print('|Entities|:', patent_entities, '\\n')\n",
    "    \n",
    "    df_patent = z[2]\n",
    "except:\n",
    "    print('No data')\n",
    "    \n",
    "df_patent.replace('', np.nan, inplace=True)\n",
    "df_patent.dropna(inplace=True)  \n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T19:26:52.351226Z",
     "start_time": "2020-11-28T19:26:27.228292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Trend keys|: DenseNet, LC KSVD, MIAS, CNN, RCNN, Recursive Convolutional Neural Network R, BreakHis, RNA, CAD, CMS \n",
      "\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "a = (arxivtext + patent_text) \n",
    "b = (googletext + wikitext) \n",
    "\n",
    "specific_keys = get_entities(a, keys_number)\n",
    "common_keys = get_entities(b, keys_number)\n",
    "\n",
    "specific_keys = [re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip() for i in specific_keys.split(',')]\n",
    "common_keys = [re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip() for i in common_keys.split(',')]\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(specific_keys)), columns =['specific'])\n",
    "df2 = pd.DataFrame(list(zip(common_keys)), columns =['known'])\n",
    "\n",
    "matches = fpd.fuzzy_merge(df1, df2, left_on=['specific'], right_on=['known'], ignore_case=True, keep='match', \n",
    "                         method='levenshtein', threshold=0.8, join='inner')\n",
    "\n",
    "specific_reqs = list(set(specific_keys).difference(set(list(matches['specific']))))\n",
    "specific_reqs = [i for i in specific_reqs if i[0].isupper()] + [i for i in specific_reqs if i[0].islower()]\n",
    "specific_keys = ', '.join([i for i in specific_reqs if i])\n",
    "\n",
    "print('|Trend keys|:', specific_keys, '\\n')\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:39:03.476610Z",
     "start_time": "2020-11-29T10:39:03.459656Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_wiki.append(df_google).append(df_arxiv).append(df_patent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:40:42.328348Z",
     "start_time": "2020-11-29T10:40:23.283705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_text = wikitext + googletext + arxivtext + patenttext\n",
    "\n",
    "final_text = filter_triplet(final_text)\n",
    "final_text = coref_res(final_text)\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:41:27.344172Z",
     "start_time": "2020-11-29T10:41:02.363369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "report_summary = get_summary(final_text, readtime)[1]\n",
    "\n",
    "final_entities = get_entities(final_text, keys_number)\n",
    "final_keys = graph_keys(final_text, keys_number)\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create keys with urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:41:58.315000Z",
     "start_time": "2020-11-29T10:41:58.302034Z"
    }
   },
   "outputs": [],
   "source": [
    "url_keys = []\n",
    "for i in final_keys.split(','):\n",
    "    url = 'https://www.google.com/search?q=' + '+'.join(re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip().split()) + '+' + query + '/keyword/' + i \n",
    "    url_keys.append(url) \n",
    "    \n",
    "mark_keys = []\n",
    "for i in final_entities.split(','):\n",
    "    url = 'https://www.google.com/search?q=' + '+'.join(re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip().split()) + '+' + query + '/keyword/' + i\n",
    "    mark_keys.append(url)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:42:06.801639Z",
     "start_time": "2020-11-29T10:42:02.063053Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_list = []\n",
    "pdf_list = []\n",
    "\n",
    "sent_list = list(report_summary.split(sep='<hr>'))[1:]\n",
    "\n",
    "for i in sent_list[:-1]:\n",
    "    try:\n",
    "        df['score'] = df['text'].apply(lambda x: css(i,x))\n",
    "        df = df.sort_values(by=['score'], ascending=False)\n",
    "        \n",
    "        if str(df['link'].iloc[0]):\n",
    "            pdf_list.append(str(str(i) + \" <u><a href=\" + str(df['link'].iloc[0]) + \" target='_blank'>\" + \"More\" + \"</a></u>\" + \"<hr>\"))\n",
    "            ref_list.append(str(df['link'].iloc[0]))\n",
    "    except:\n",
    "        pdf_list.append('')\n",
    "\n",
    "ref_list = list(frozenset(ref_list))\n",
    "pdf_summary = ''.join(pdf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save docx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:42:18.024478Z",
     "start_time": "2020-11-29T10:42:17.808058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_doc(pdf_summary, 'summary', query, len(df), readtime, url_keys, mark_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:42:26.815727Z",
     "start_time": "2020-11-29T10:42:25.779531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory size: 33 mb\n",
      "Number of documents: 34\n"
     ]
    }
   ],
   "source": [
    "memory_size = int(len(final_text.encode('utf-8'))/1024)\n",
    "docs_number = len(df)\n",
    "\n",
    "print('Memory size:', memory_size, 'mb')\n",
    "print('Number of documents:', docs_number)\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
