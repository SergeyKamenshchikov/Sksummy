{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIbraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T09:57:15.241862Z",
     "start_time": "2021-01-10T09:56:34.721041Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import time, winsound, random\n",
    "from random import randint\n",
    "# general\n",
    "\n",
    "# process arrays and dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import fuzzy_pandas as fpd\n",
    "from collections import Counter\n",
    "#/process arrays and dataframes\n",
    "\n",
    "# parallel calculations\n",
    "from tqdm import tqdm\n",
    "#/parallel calculations\n",
    "\n",
    "# web parsing\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import chromedriver_binary\n",
    "from requests import get\n",
    "#/web parsing\n",
    "\n",
    "# parsing libs\n",
    "import arxiv\n",
    "import wikipediaapi\n",
    "from googlesearch import search  \n",
    "#/parsing libs\n",
    "\n",
    "# read .pdf\n",
    "from tika import parser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "#/read .pdf\n",
    "\n",
    "# text processing\n",
    "import spacy,nltk,string,re\n",
    "import neuralcoref\n",
    "import networkx as nx\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from more_itertools import unique_everseen\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000\n",
    "#/text processing\n",
    "\n",
    "# create .docx\n",
    "import docx\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "from docx.shared import Pt\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt\n",
    "#/create .docx\n",
    "\n",
    "# create .pdf\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "from reportlab import platypus\n",
    "#/create .pdf\n",
    "\n",
    "# keywords extraction\n",
    "import yake\n",
    "#/keywords extraction\n",
    "\n",
    "# extractive summarizer\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "#/extractive summarizer\n",
    "\n",
    "# abstractive summarizer\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import textstat\n",
    "import torch\n",
    "#/abstractive summarizer\n",
    "\n",
    "# many-to-many evaluation\n",
    "from rouge import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "#/many-to-many evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T09:58:38.626442Z",
     "start_time": "2021-01-10T09:58:38.621425Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'docs/'\n",
    "\n",
    "sent_number = 50 # total sentences\n",
    "threshold = 0.8 #extension boundary\n",
    "\n",
    "months_delta = 12 #actual dates\n",
    "\n",
    "page_number = 10 # page volume\n",
    "keys_number = 10 #number of tags\n",
    "\n",
    "max_length = 60 #number of tokens\n",
    "title_num = 5 #number of titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature toggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T09:58:44.545571Z",
     "start_time": "2021-01-10T09:58:44.540612Z"
    }
   },
   "outputs": [],
   "source": [
    "compress = True\n",
    "paraphrase = False\n",
    "\n",
    "wiki_sum = True\n",
    "gogle_sum = True\n",
    "arxiv_sum = True\n",
    "patent_sum = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T09:59:02.533033Z",
     "start_time": "2021-01-10T09:59:02.316877Z"
    }
   },
   "outputs": [],
   "source": [
    "##### HTML parsing #####\n",
    "def parse_google_page(url): \n",
    "    try:\n",
    "        title = BeautifulSoup(get(url).content, 'html.parser').title.getText()\n",
    "        parser = HtmlParser.from_url(url, Tokenizer(\"English\"))\n",
    "        \n",
    "        summarizer = Summarizer(Stemmer(\"English\"))\n",
    "        summarizer.stop_words = get_stop_words(\"English\")\n",
    "\n",
    "        sentences = []\n",
    "        for i in summarizer(parser.document, 1000000):\n",
    "            sentences.append(str(i))\n",
    "        txt = ' '.join(sentences)\n",
    "    except:\n",
    "        txt = ''\n",
    "        title = ''\n",
    "    \n",
    "    return txt, title\n",
    "\n",
    "def parse_patent_page(url):\n",
    "    try:\n",
    "        soup = BeautifulSoup(get(url).text, 'html.parser')\n",
    "        \n",
    "        title = re.sub('[^A-Za-z0-9.]+', ' ', soup.title.getText()).replace('Google Patents','').strip()\n",
    "        descr = soup.find('section', attrs={'itemprop': 'description'}).getText().replace('\\n',' ').strip()\n",
    "        claims = soup.find('section', {'itemprop':'claims'}).getText().replace('\\n',' ').strip()\n",
    "        abstract = soup.abstract.getText().replace('\\n',' ').strip() \n",
    "        \n",
    "        abstract = re.sub('[^A-Za-z0-9.]+', ' ', abstract).replace('Google Patents','').strip()\n",
    "        descr = re.sub('[^A-Za-z0-9.]+', ' ', descr).replace('Google Patents','').strip()\n",
    "        claims = re.sub('[^A-Za-z0-9.]+', ' ', claims).replace('Google Patents','').strip()\n",
    "        \n",
    "        paragraphs = (abstract) + '; ' + (claims) \n",
    "        \n",
    "    except:\n",
    "        paragraphs = ''\n",
    "        title = ''  \n",
    "\n",
    "    return paragraphs, title\n",
    "\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "#####/HTML parsing #####\n",
    "\n",
    "############# Parse Wiki ############# \n",
    "def parse_wiki(google_url):\n",
    "    \n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver\n",
    "    \n",
    "    # get urls  \n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    url_list = [i for i in url_list if 'https://en.wikipedia.org' in i]\n",
    "    \n",
    "    title_list = []\n",
    "    for i in url_list:\n",
    "        try:\n",
    "            if 'https://en.wikipedia.org' in i: \n",
    "                title_list.append(i.split('/')[4])  \n",
    "        except:\n",
    "            continue\n",
    "    #/ get urls\n",
    "        \n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "    \n",
    "    return title_list     \n",
    "############# Parse Wiki ##############\n",
    "\n",
    "############## Extend abstract ##########\n",
    "def get_ngrams(text): \n",
    "    grams = nltk.ngrams(text.split(), 2)\n",
    "    grams_list = []\n",
    "    for i in grams:\n",
    "        grams_list.append(i)\n",
    "    \n",
    "    return grams_list \n",
    "\n",
    "def get_jaccard_sim(a,b):\n",
    "    a, b = set(get_ngrams(a)), set(get_ngrams(b)) \n",
    "    c = a.intersection(b)\n",
    "\n",
    "    return round(float(len(c)/len(a)), 2)\n",
    "\n",
    "def filter_text(content, abstract, threshold=0.5): \n",
    "\n",
    "    content_list = []   \n",
    "    for j in content.split('.'):\n",
    "        try:\n",
    "            sim_score = get_jaccard_sim(j, abstract)\n",
    "        except:\n",
    "            sim_score = 0\n",
    "            \n",
    "        if sim_score > threshold:\n",
    "            content_list.append(j)    \n",
    "        \n",
    "        final_list = list(dict.fromkeys(abstract.split('.') + content_list))    \n",
    "             \n",
    "    return '. '.join(final_list)\n",
    "##############/Extend abstract #########\n",
    "\n",
    "############# Parse Arxiv #############\n",
    "def parse_arxiv(query, delta_months):\n",
    "    \n",
    "    arxivtext = ''\n",
    "    urls = []\n",
    "    titles = []\n",
    "\n",
    "    closest_value = 100\n",
    "    req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value)\n",
    "    req = req + '&searchtype=all&source=header&start=0&date-filter_by=past_' + str(delta_months) \n",
    "    \n",
    "    htmlString = get(req)\n",
    "\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib')\n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "\n",
    "    titles = list(soup.find_all('p', {'class' : 'title is-5 mathjax'}))[:page_number]\n",
    "    titles_r = [i.text.replace('\\n','').replace('  ','') for i in titles]\n",
    "    titles = ', '.join(titles_r)\n",
    "\n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])\n",
    "\n",
    "    txt = []\n",
    "    for i in urls[:page_number]:\n",
    "        time.sleep(random.randint(1,8))\n",
    "        soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "        abstract = ' '.join(soup.find('blockquote').text.replace('  ',' ').split())\n",
    "        txt.append(abstract)\n",
    "\n",
    "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, urls, titles_r)), columns=['text','link', 'page'])\n",
    "\n",
    "    return arxivtext, titles, df\n",
    "#############/Parse Arxiv #############\n",
    "\n",
    "############# Parse Google ###############\n",
    "def parse_google(query):   \n",
    "    \n",
    "    txt = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver \n",
    "\n",
    "    # get urls\n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1)\n",
    "    google_url = google_url + '&hl=en&gl=en' + '&lr=lang_en&cr=countryGB'\n",
    "    \n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = list(set(links))[:(page_number)] \n",
    "    #/ get urls\n",
    "    \n",
    "    for j in tqdm(url_list):\n",
    "        delta = random.randint(1,8)\n",
    "        time.sleep(delta) \n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'):\n",
    "                file_data = parser.from_file(str(j))        \n",
    "                t = file_data['content'].replace('\\n','')    \n",
    "                titles.append(t[:100])\n",
    "            else:\n",
    "                t = parse_google_page(j)[0].replace('\\n','') \n",
    "                titles.append(parse_google_page(j)[1].replace('\\n',''))\n",
    "            \n",
    "            txt.append(t)\n",
    "            \n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "          \n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    df = df[~df['page'].str.contains('|'.join(['403','404']))]\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    googletext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(list(df['text'])))\n",
    "    titles = list(df['page'])\n",
    "   \n",
    "    return googletext, errors, df, titles\n",
    "#############/Parse Google ###############\n",
    "\n",
    "############# Parse Patents #############\n",
    "def parse_patents(query, keys_number, page_number):   \n",
    "    \n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver\n",
    "\n",
    "    # get urls \n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1)\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '' and ('/en' in link['href']):\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    #/ get urls\n",
    "\n",
    "    # load content \n",
    "    txt = []\n",
    "    titles = []\n",
    "    for j in tqdm(url_list):\n",
    "        delta = random.randint(1,8)\n",
    "        time.sleep(delta)\n",
    "         \n",
    "        txt.append(re.sub('[^A-Za-z0-9.]+', ' ', parse_patent_page(j)[0]))\n",
    "        titles.append(re.sub('[^A-Za-z0-9.]+', ' ', parse_patent_page(j)[1].split('\\n')[0]))         \n",
    "       \n",
    "    patenttext = txt\n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    \n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "   \n",
    "    return patenttext, errors, df \n",
    "#############/Parse Patents #############\n",
    "\n",
    "##### Text processing #####\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def text_normalize(txt):\n",
    "    processed_text = re.sub('[^a-zA-Z]', ' ', txt)\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_triplet(final_text):\n",
    "    \n",
    "    final_text = get_unique_text(final_text)\n",
    "    doc = nlp(final_text)\n",
    "    valid_sents = []\n",
    "\n",
    "    for s in list(doc.sents):\n",
    "        if syntax_full(s):\n",
    "            valid_sents.append(s.text)\n",
    "    \n",
    "    final_text = ' '.join(valid_sents)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "def coref_res(rawtext, coref_greedn = 0.5):\n",
    "\n",
    "    neuralcoref.add_to_pipe(nlp, greedyness = coref_greedn, store_scores=False)\n",
    "    doc = nlp(rawtext)\n",
    "\n",
    "    resolved = list(tok.text_with_ws for tok in doc)\n",
    "\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        for coref in cluster:\n",
    "            if coref != cluster.main:\n",
    "                if coref.text[0].isalpha() and coref.text[0].isupper():\n",
    "\n",
    "                    main_words_list=word_tokenize(cluster.main.text)\n",
    "                    main_words_list[0]=main_words_list[0].capitalize()\n",
    "                    resolved[coref.start] = detokenizer(main_words_list) + doc[coref.end-1].whitespace_\n",
    "\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "            else:\n",
    "                resolved[coref.start] = cluster.main.text + doc[coref.end-1].whitespace_\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "\n",
    "    text_resolved = ''.join(resolved)\n",
    "    nlp.remove_pipe(\"neuralcoref\")\n",
    "\n",
    "    return text_resolved\n",
    "\n",
    "def compress(spacy_sents,sents_whitelist):\n",
    "    blacklist_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if token.dep_ in ['appos','advmod']:\n",
    "                    token_sub_tree=token.subtree\n",
    "                    for t in token_sub_tree:\n",
    "                        blacklist_tokens.append(t.i)\n",
    "\n",
    "        n=n+1\n",
    "    return(blacklist_tokens)\n",
    "\n",
    "def spacy_compress(rawtext):\n",
    "\n",
    "    doc1 = nlp(rawtext)\n",
    "    sents_whitelist = get_sents_ids_whitelist(doc1.sents)\n",
    "\n",
    "    tokens_blacklist = compress(doc1.sents,sents_whitelist)\n",
    "    sents_tokens = get_list_sents_tokens(doc1.sents,sents_whitelist,tokens_blacklist)\n",
    "    compressed_text_sents = []\n",
    "\n",
    "    for s in sents_tokens:\n",
    "        text=detokenizer(s)\n",
    "        compressed_text_sents.append(text)\n",
    "    compressed_text_sents=sentence_grammar_fix(compressed_text_sents)\n",
    "    text =' '.join(compressed_text_sents)\n",
    "\n",
    "    return(text)\n",
    "##### Text processing #####\n",
    "\n",
    "############## Get summary #############\n",
    "def get_summary(rawtext, sentences):\n",
    "    \n",
    "    stemmer = Stemmer(\"english\")\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(\"english\")\n",
    "    parser = PlaintextParser.from_string(' '.join(sent_tokenize(rawtext)[6:]), Tokenizer(\"english\"))\n",
    "\n",
    "    text_list = []\n",
    "    for sentence in summarizer(parser.document, sentences):\n",
    "        text_list.append(str(sentence))\n",
    "\n",
    "    txt = ' '.join(sent_tokenize(rawtext)[:6]) + ' '+' '.join(text_list)\n",
    "\n",
    "    z = 0\n",
    "    output = []\n",
    "    \n",
    "    for i in nltk.sent_tokenize(txt):\n",
    "        output.append(str(i) + '==')\n",
    "    \n",
    "    txt = ''.join(output)\n",
    "    \n",
    "    return txt\n",
    "##############/Get summary #############\n",
    "\n",
    "############## Get tags and entities ###########\n",
    "def graph_keys(final_text, top_number):\n",
    "    \n",
    "    bigrams = list(nltk.ngrams(text_normalize(final_text.lower()),2))\n",
    "    bigrams = [' '.join(i) for i in bigrams if (i[0]!=i[1])] \n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    \n",
    "    df = pd.DataFrame(bigram_counts.most_common(len(bigram_counts)), columns=['bigram', 'count'])[:top_number]\n",
    "    df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
    "    keys = ', '.join(list(df['bigram'].astype(str)))\n",
    "\n",
    "    return keys\n",
    "\n",
    "def yake_keys(text, keys_number):\n",
    "    сustom_kw_extractor = yake.KeywordExtractor(lan=\"en\", n=2, top=keys_number)\n",
    "    keywords = сustom_kw_extractor.extract_keywords(text)\n",
    "    keywords = ', '.join([i[1] for i in keywords])\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def get_entities(rawtext, tops):\n",
    "    spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
    "    nlp.max_length = 1000000000000\n",
    "    doc = spacy_nlp(rawtext)\n",
    "\n",
    "    ners = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PERSON']:\n",
    "            ners.append(ent.text)\n",
    "   \n",
    "    ner_counts = collections.Counter(ners)\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame(ner_counts.most_common(len(ner_counts)), columns=['ner_names', 'count'])[:tops]\n",
    "        df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
    "        keys = ', '.join(list(df['ner_names'].astype(str)))\n",
    "    except:\n",
    "        keys = ''\n",
    "    \n",
    "    return keys\n",
    "############## Get tags and entities #############\n",
    "\n",
    "############## Add keyurls ################\n",
    "def add_keyurls(final_keys, query):\n",
    "    url_keys = []\n",
    "    for i in final_keys.split(','):\n",
    "        url = 'https://www.google.com/search?q=' + '+'.join(re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip().split()) + '+' + query + '/keyword/' + i \n",
    "        url_keys.append(url)\n",
    "        \n",
    "    return url_keys     \n",
    "##############/Add urls ###################\n",
    "\n",
    "##### Abstractive summarization #############\n",
    "def get_response(input_text,num_return_sequences):\n",
    "    \n",
    "    batch = tokenizer.prepare_seq2seq_batch([input_text], truncation=True, padding='longest', max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch, max_length=60, num_beams=10, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    \n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    \n",
    "    return tgt_text\n",
    "#####/Abstractive summarization #############\n",
    "\n",
    "############# Doc preparation ##########\n",
    "def add_hyperlink(paragraph, text, url, flag):\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element and a new w:rPr element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    # Create a new Run object and add the hyperlink into it\n",
    "    r = paragraph.add_run()\n",
    "    r._r.append (hyperlink) \n",
    "\n",
    "    # A workaround for the lack of a hyperlink style (doesn't go purple after using the link)\n",
    "    # Delete this if using a template that has the hyperlink style in it\n",
    "    r.font.color.theme_color = MSO_THEME_COLOR_INDEX.HYPERLINK\n",
    "    r.font.underline = flag\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "def save_doc(final_summary, summary, query, docs_number, sent_number, url_keys, url_marks, score):\n",
    "    \n",
    "    sent_list = list(final_summary.split(sep='<hr>'))\n",
    "    doc = Document()\n",
    "    style = doc.styles['Normal']\n",
    "    \n",
    "    font = style.font\n",
    "    font.name = 'Times New Roman'\n",
    "    font.size = Pt(12)\n",
    "\n",
    "    hd = doc.add_paragraph()\n",
    "    hd.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "    hd.add_run('Summary').bold = True\n",
    "\n",
    "    if query != 'none':\n",
    "        hd = doc.add_paragraph('Request: ' + \"''\" + query + \"''\")\n",
    "\n",
    "    hd = doc.add_paragraph('Sentences: ' + str(sent_number))\n",
    "    hd = doc.add_paragraph('Bigrams: ' + str(score) + '%')\n",
    "    hd = doc.add_paragraph('Documents: ' + str(docs_number))\n",
    "    \n",
    "    hd = doc.add_paragraph('')\n",
    "    \n",
    "    hd.add_run('Keys:\\n').underline = True\n",
    "    \n",
    "    for j in url_keys:\n",
    "        if j != url_keys[-1]:\n",
    "            add_hyperlink(hd, (str(j.split('/keyword/')[1]) + ', '), str(j.split('/keyword/')[0]), False)\n",
    "        else:\n",
    "            add_hyperlink(hd, (str(j.split('/keyword/')[1])), str(j.split('/keyword/')[0]), False)\n",
    "        \n",
    "    hd.add_run('\\n\\nBenchmarks:\\n').underline = True\n",
    "    \n",
    "    for j in url_marks:\n",
    "        if j != url_marks[-1]:\n",
    "            add_hyperlink(hd, str(j.split('/keyword/')[1]) + ', ', str(j.split('/keyword/')[0]), False)\n",
    "        else:\n",
    "            add_hyperlink(hd, str(j.split('/keyword/')[1]), str(j.split('/keyword/')[0]), False)\n",
    "    \n",
    "    r = hd.add_run()\n",
    "    for i in sent_list:\n",
    "        hd.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "        if query != 'none':\n",
    "            try:\n",
    "                link = re.search(r\"<a href=(.*?)target='_blank'\", str(i)).group(1).replace(' ','')\n",
    "                hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))               \n",
    "                add_hyperlink(hd, 'More', link, True).add_run()\n",
    "            except:\n",
    "                link = ''\n",
    "        if query == 'none':\n",
    "            hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))    \n",
    "         \n",
    "    doc.save('docs/' + summary + '.docx')\n",
    "    \n",
    "    return True\n",
    "#############/Doc preparation ##########\n",
    "\n",
    "############## Sandbox functions ##########\n",
    "def longest_common_substring(s1, s2):\n",
    "  m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "  longest, x_longest = 0, 0\n",
    "  for x in range(1, 1 + len(s1)):\n",
    "    for y in range(1, 1 + len(s2)):\n",
    "      if s1[x - 1] == s2[y - 1]:\n",
    "        m[x][y] = m[x - 1][y - 1] + 1\n",
    "        if m[x][y] > longest:\n",
    "          longest = m[x][y]\n",
    "          x_longest = x\n",
    "      else:\n",
    "        m[x][y] = 0\n",
    "  return s1[x_longest - longest: x_longest]\n",
    "\n",
    "def longest_common_sentence(s1, s2):\n",
    "    s1_words = s1.split(' ')\n",
    "    s2_words = s2.split(' ')\n",
    "    return ' '.join(longest_common_substring(s1_words, s2_words))\n",
    "\n",
    "def css(a,b):\n",
    "    if len(a.split()) > 0:\n",
    "        score = len(longest_common_sentence(a,b).split())/len(a.split())\n",
    "    else:    \n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def readingTime(mytext):\n",
    "    total_words = len(word_tokenize(mytext))\n",
    "    estimatedTime = round(total_words/200.0,1)\n",
    "    return estimatedTime\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_min_num_of_clauses(spacy_sentence, n):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
    "            result.append(token.head.text)\n",
    "    if len(result)>=n:\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_sents_ids_whitelist(spacy_sents):\n",
    "    whitelist=[]\n",
    "    i=1\n",
    "    sents_texts=[]\n",
    "    for sent in spacy_sents:\n",
    "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
    "            whitelist.append(i)\n",
    "            sents_texts.append(sent.text)\n",
    "        i=i+1\n",
    "    return(whitelist)\n",
    "\n",
    "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
    "    sents_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        sent_tokens=[]\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if (token.i not in blacklist_tokens):\n",
    "                    sent_tokens.append(token.text)\n",
    "            sents_tokens.append(sent_tokens)\n",
    "            sent_tokens=[]\n",
    "\n",
    "        n=n+1\n",
    "    return(sents_tokens)\n",
    "\n",
    "def detokenizer(list_of_tokens):\n",
    "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
    "    return(text_str)\n",
    "\n",
    "def sentence_grammar_fix(sentences):\n",
    "    fixed=[]\n",
    "    for sent in sentences:\n",
    "\n",
    "        sent=sent.strip()\n",
    "        sent=sent.replace('\\n','')\n",
    "        sent=sent.replace('()','')\n",
    "\n",
    "        sent=re.sub('\\s+',' ',sent)\n",
    "        sent=sent+'.'\n",
    "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0] in ['.',',','-','—']:\n",
    "                sent=sent[1:]\n",
    "        sent=sent.strip()\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0].isalpha():\n",
    "                sent=sent[0].upper()+sent[1:]\n",
    "        fixed.append(sent)\n",
    "\n",
    "    return(fixed)\n",
    "##############/Sandbox functions ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T09:59:11.435465Z",
     "start_time": "2021-01-10T09:59:10.036073Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum computer\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse web sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Wiki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T10:07:46.921645Z",
     "start_time": "2021-01-10T10:07:17.975044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Driver [C:\\Users\\skamenshchikov\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.88\\chromedriver.exe] found in cache\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: quantum computing, quantum computing, superconducting quantum computing, timeline of quantum computing and communication, shor%27s algorithm, quantum algorithm \n",
      "\n",
      "|Entities|: Quantum, RSA, Turing, Paul Benioff, Richard Feynman, Yuri Manin, Peter Shor, Google AI, National Aeronautics and Space Administration, NASA \n",
      "\n",
      "Qubit\n",
      "\n",
      "In quantum computing, a qubit () or quantum bit (sometimes qbit) is the basic unit of quantum information—the quantum version of the classical binary bit physically realized with a two-state device.   A qubit is a two-state (or two-level) quantum-mechanical system, one of the simplest quantum systems displaying the peculiarity of quantum mechanics.   Examples include: the spin of the electron in which the two levels can be taken as spin up and spin down; or the polarization of a single photon in which the two states can be taken to be the vertical polarization and the horizontal polarization.  In a classical system, a bit would have to be in one state or the other.  However, quantum mechanics allows the qubit to be in a coherent superposition of both states simultaneously, a property which is fundamental to quantum mechanics and quantum computing. ...\n",
      "\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wikitext = ''\n",
    "wikikeys = ''\n",
    "\n",
    "df_wiki = pd.DataFrame()\n",
    "\n",
    "if wiki_sum == True: \n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "\n",
    "    red_query = \"https://www.google.com/search?q=\" + 'site:https://en.wikipedia.org ' + query + \"&num=\" + str(page_number+1)\n",
    "    \n",
    "    red_query = red_query + '&searchtype=all&source=header&start=0&date-filter_by=past_' + str(months_delta)\n",
    "    \n",
    "    wiki_titles = parse_wiki(red_query)\n",
    "\n",
    "    txts = []\n",
    "    titles = []\n",
    "\n",
    "    for i in tqdm(wiki_titles): \n",
    "        \n",
    "        page_sum = wiki_wiki.page(i).summary\n",
    "        page_txt = wiki_wiki.page(i).text\n",
    "        sent_list = filter_text(page_txt, page_sum, threshold=threshold)\n",
    "       \n",
    "        titles.append(i)\n",
    "        txts.append(''.join(sent_list).replace('\\n', ''))        \n",
    "    \n",
    "    wikitext = ''.join(txts).replace('\\n','') \n",
    "\n",
    "    if compress == True:\n",
    "        wikitext = coref_res(filter_triplet(wikitext))\n",
    "\n",
    "    wikikeys = ', '.join(titles).lower().replace('_',' ')\n",
    "    wikikeys = ', '.join([i for i in wikikeys.split(', ') if len(i.split()) > 1][:keys_number])\n",
    "    wiki_entities = get_entities(wikitext, keys_number)\n",
    "\n",
    "    url_list = [str('https://en.wikipedia.org/wiki/' + i)  for i in wiki_titles] \n",
    "    \n",
    "    df_wiki = pd.DataFrame(list(zip(txts, url_list, titles)), columns=['text','link', 'page'])\n",
    "    df_wiki.replace('', np.nan, inplace=True)\n",
    "    df_wiki.dropna(inplace=True)\n",
    "    \n",
    "    random_num = randint(1,len(df_wiki)) \n",
    "\n",
    "    print('|Keywords|:', wikikeys, '\\n')\n",
    "    print('|Entities|:', wiki_entities, '\\n')  \n",
    "    \n",
    "    print(df_wiki['page'][random_num-1]+'\\n')\n",
    "    print(df_wiki['text'][random_num-1][:1000]+'...'+'\\n')\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T10:09:34.615120Z",
     "start_time": "2021-01-10T10:09:34.542279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quantum computing is the use of quantum phenom...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum_computing</td>\n",
       "      <td>Quantum_computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quantum computing is the use of quantum phenom...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum_computing</td>\n",
       "      <td>Quantum_computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Superconducting quantum computing is an implem...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Superconducting_...</td>\n",
       "      <td>Superconducting_quantum_computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In quantum computing, a qubit () or quantum bi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Qubit</td>\n",
       "      <td>Qubit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a timeline of quantum computing.</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Timeline_of_quan...</td>\n",
       "      <td>Timeline_of_quantum_computing_and_communication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Quantum computing is the use of quantum phenom...   \n",
       "1  Quantum computing is the use of quantum phenom...   \n",
       "2  Superconducting quantum computing is an implem...   \n",
       "3  In quantum computing, a qubit () or quantum bi...   \n",
       "4          This is a timeline of quantum computing.    \n",
       "\n",
       "                                                link  \\\n",
       "0    https://en.wikipedia.org/wiki/Quantum_computing   \n",
       "1    https://en.wikipedia.org/wiki/Quantum_computing   \n",
       "2  https://en.wikipedia.org/wiki/Superconducting_...   \n",
       "3                https://en.wikipedia.org/wiki/Qubit   \n",
       "4  https://en.wikipedia.org/wiki/Timeline_of_quan...   \n",
       "\n",
       "                                              page  \n",
       "0                                Quantum_computing  \n",
       "1                                Quantum_computing  \n",
       "2                Superconducting_quantum_computing  \n",
       "3                                            Qubit  \n",
       "4  Timeline_of_quantum_computing_and_communication  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Arxiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:22:47.393408Z",
     "start_time": "2021-01-09T20:21:34.384257Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "arxivtext = ''\n",
    "arxivkeys = ''\n",
    "\n",
    "df_arxiv = pd.DataFrame()\n",
    "\n",
    "if arxiv_sum == True:\n",
    "    \n",
    "    try:\n",
    "        df_arxiv = parse_arxiv(query, months_delta)[2] \n",
    "        df_arxiv.replace('', np.nan, inplace=True)\n",
    "        df_arxiv.dropna(inplace=True)   \n",
    "        \n",
    "        arxivtext = ''.join(list(df_arxiv['text'])) \n",
    "    \n",
    "        if compress == True:\n",
    "            arxivtext = coref_res(filter_triplet(arxivtext))\n",
    "    \n",
    "        arxiv_entities = get_entities(arxivtext, keys_number)\n",
    "        arxiv_titles = '; '.join(list(df_arxiv['page'])[:title_num]).replace('\\n','')\n",
    "        arxivkeys = ', '.join([i for i in yake_keys(arxivtext, keys_number).split(', ') if len(i.split()) > 1][:keys_number])\n",
    "        \n",
    "        random_num = randint(1, len(df_arxiv)) \n",
    "    \n",
    "        print('|Keywords|:', arxivkeys, '\\n')\n",
    "        print('|Titles|:', arxiv_titles, '\\n')\n",
    "        print('|Entities|:', arxiv_entities, '\\n')\n",
    "        \n",
    "        print(df_arxiv['page'][random_num-1]+'\\n')\n",
    "        print(df_arxiv['text'][random_num-1][:1000]+'...'+'\\n')\n",
    "    \n",
    "    except:\n",
    "        print('No data')\n",
    "     \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:10:16.423050Z",
     "start_time": "2021-01-09T20:08:13.653385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "googletext = ''\n",
    "googlekeys = ''\n",
    "\n",
    "df_google = pd.DataFrame()\n",
    "\n",
    "if gogle_sum == True:\n",
    "    \n",
    "    try:\n",
    "        df_google = parse_google(query)[2]\n",
    "        df_google.replace('', np.nan, inplace=True)\n",
    "         \n",
    "        googletext = ''.join(list(df_google['text']))\n",
    "    \n",
    "        if compress == True:\n",
    "            googletext = coref_res(filter_triplet(googletext))\n",
    "    \n",
    "        google_entities = get_entities(googletext, keys_number)\n",
    "        google_titles = '; '.join(list(df_google['page'])).replace('\\n','')\n",
    "        googlekeys = ', '.join([i for i in yake_keys(googletext, keys_number).split(', ') if len(i.split()) > 1][:keys_number])\n",
    "        \n",
    "        random_num = randint(1,len(df_google)) \n",
    "    \n",
    "        print('|Keywords|:', googlekeys, '\\n')\n",
    "        print('|Titles|:', google_titles, '\\n')\n",
    "        print('|Entities|:', google_entities, '\\n')\n",
    "        \n",
    "        print(df_google['page'][random_num-1]+'\\n')\n",
    "        print(df_google['text'][random_num-1][:1000]+'...'+'\\n')\n",
    "\n",
    "    \n",
    "    except:\n",
    "        print('No data')\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google Patents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:11:44.580637Z",
     "start_time": "2021-01-09T20:10:35.512940Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "patenttext = ''\n",
    "patentkeys = ''\n",
    "\n",
    "df_patent = pd.DataFrame()\n",
    "\n",
    "if patent_sum == True:\n",
    "    \n",
    "    z = parse_patents(\"site:https://patents.google.com \" + query, keys_number, page_number) \n",
    "    patenttext = ''.join(z[0])\n",
    "    \n",
    "    df_patent = z[2]\n",
    "    df_patent.replace('', np.nan, inplace=True)\n",
    "    df_patent.dropna(inplace=True)\n",
    "    \n",
    "    patent_entities = get_entities(patenttext, keys_number)\n",
    "    patent_titles = '; '.join(list(df_patent['page'])).replace('\\n','')\n",
    "    patentkeys = ', '.join([i for i in yake_keys(patenttext, keys_number).split(', ') if len(i.split()) > 1][:keys_number])\n",
    "    \n",
    "    random_num = randint(1, len(df_patent)) \n",
    "    \n",
    "    print('|Keywords|:', patentkeys, '\\n')\n",
    "    print('|Titles|:', patent_titles, '\\n')\n",
    "    print('|Entities|:', patent_entities, '\\n')\n",
    "    \n",
    "    print(df_patent['page'][random_num-1]+'\\n')\n",
    "    print(df_patent['text'][random_num-1][:1000]+'...'+'\\n')\n",
    "    \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:22:55.672672Z",
     "start_time": "2021-01-09T20:22:55.656749Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_wiki.append(df_google).append(df_arxiv).append(df_patent)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get text and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:23:17.911235Z",
     "start_time": "2021-01-09T20:22:59.588207Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "final_text = (wikitext + arxivtext + googletext + patenttext)\n",
    "final_entities = get_entities(final_text, keys_number)\n",
    "\n",
    "final_keys = list(frozenset(googlekeys.split(', ') + wikikeys.split(', ') + arxivkeys.split(', ') + patentkeys.split(', ')))\n",
    "final_keys = ', '.join(final_keys) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get extractive summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:24:47.189639Z",
     "start_time": "2021-01-09T20:23:17.914228Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "report_summary = get_summary(final_text, sent_number)\n",
    "scores = scorer.score(report_summary, final_text)\n",
    "scores = round(100*list(list(scores.values())[0])[2])\n",
    "\n",
    "print('Information extracted:', (str(scores) + ' %'))\n",
    "print('\\n', (report_summary[:1000])+'...', '\\n')\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paraphrase generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T13:29:35.998877Z",
     "start_time": "2021-01-08T13:29:35.976936Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if paraphrase == True:\n",
    "    \n",
    "    model_name = 'tuner007/pegasus_paraphrase'\n",
    "    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "    \n",
    "    counter = 0\n",
    "    summ_list = []\n",
    "    \n",
    "    for i in report_summary.split('==')[:-1]:\n",
    "        summ_list.append('=='+ get_response(i,1)[0])\n",
    "    \n",
    "    summary = ' '.join(summ_list)\n",
    "\n",
    "    scores = scorer.score(summary, report_summary)\n",
    "    scores = round(100*list(list(scores.values())[0])[2])\n",
    "    report_summary = summary \n",
    "\n",
    "    print('Plagiarism:', (str(scores) + ' %'))\n",
    "\n",
    "    winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create keys with urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:25:06.229163Z",
     "start_time": "2021-01-09T20:24:47.194629Z"
    }
   },
   "outputs": [],
   "source": [
    "url_keys = add_keyurls(final_keys, query)\n",
    "mark_keys = add_keyurls(final_entities, query)  \n",
    "\n",
    "ref_list = []\n",
    "pdf_list = []\n",
    "\n",
    "sent_list = list(report_summary.split(sep='=='))[:-1]\n",
    "\n",
    "for i in sent_list:\n",
    "    try:\n",
    "        df_score = df.copy()\n",
    "        df_score['score'] = df_score['text'].apply(lambda x: css(i,x))\n",
    "        df_score = df_score.sort_values(by=['score'], ascending=False)\n",
    "        \n",
    "        if str(df_score['link'].iloc[0]):\n",
    "            pdf_list.append(str(i))\n",
    "            ref_list.append(str(df_score['link'].iloc[0]))\n",
    "    except:\n",
    "        pdf_list.append('')\n",
    "\n",
    "pdf_summary = ''.join(pdf_list)\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe from tags and urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:25:06.272859Z",
     "start_time": "2021-01-09T20:25:06.231956Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged = pd.DataFrame(list(zip(ref_list, pdf_list)), columns=['link', 'text'])\n",
    "df_merged = df_merged.sort_index(ascending=True).groupby('link', as_index=True).agg(lambda x: ' '.join(x))\n",
    "df_merged = df_merged.reindex(list(unique_everseen(ref_list))).reset_index()\n",
    "\n",
    "df_merged.replace('', np.nan, inplace=True)\n",
    "df_merged.dropna(inplace=True) \n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:25:06.287807Z",
     "start_time": "2021-01-09T20:25:06.275839Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_list = []\n",
    "pdf_list = []\n",
    "\n",
    "trc = 0\n",
    "for i in range(len(df_merged)):\n",
    "    trc = trc + 1\n",
    "    \n",
    "    pdf_list.append(str(trc) + '. ...' + str(str(df_merged['text'].iloc[i])) + \" <u><a href=\" + str(df_merged['link'].iloc[i]) + \" target='_blank'>\" + \"More\" + \"</a></u>\" + \"<hr>\")\n",
    "    ref_list.append(str(df_merged['link'].iloc[i]))\n",
    "\n",
    "pdf_summary = ''.join(pdf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:25:06.302765Z",
     "start_time": "2021-01-09T20:25:06.291796Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pdf_summary[:1000] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save docx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:25:11.392344Z",
     "start_time": "2021-01-09T20:25:06.305759Z"
    }
   },
   "outputs": [],
   "source": [
    "save_doc(pdf_summary, 'summary', query, len(df_merged), sent_number, url_keys, mark_keys, scores)\n",
    "winsound.Beep(2500, 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
