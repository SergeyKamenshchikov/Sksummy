{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:52:22.642359Z",
     "start_time": "2020-12-21T19:51:47.809527Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wikipediaapi\n",
    "import string\n",
    "\n",
    "from rouge import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from  more_itertools import unique_everseen\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fuzzy_pandas as fpd\n",
    "from collections import Counter\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import os, time, random\n",
    "import docx, html, re, nltk\n",
    "import collections\n",
    "\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "from docx.shared import Pt\n",
    "\n",
    "import concurrent.futures\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt\n",
    "\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "from reportlab import platypus\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from tika import parser\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from requests import get\n",
    "\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from random import randint\n",
    "import winsound\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T20:45:03.013704Z",
     "start_time": "2020-12-21T20:45:02.801250Z"
    }
   },
   "outputs": [],
   "source": [
    "##### HTML parsing #####\n",
    "def parse_page(url, tag, cls=''): \n",
    "    soup = BeautifulSoup(get(url).text, 'html.parser')\n",
    "    paragraphs = soup.find('body', cls='')\n",
    "    \n",
    "    title = BeautifulSoup(get(url).text, 'html.parser').title.getText()\n",
    "    \n",
    "    parser = HtmlParser.from_url(url, Tokenizer(\"English\"))\n",
    "    summarizer = Summarizer(Stemmer(\"English\"))\n",
    "    summarizer.stop_words = get_stop_words(\"English\")\n",
    "\n",
    "    sentences = []\n",
    "    for i in summarizer(parser.document, 1000000):\n",
    "        sentences.append(str(i))\n",
    "    \n",
    "    txt = ' '.join(sentences)  \n",
    "    \n",
    "    return txt, title\n",
    "\n",
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "#####/HTML parsing #####\n",
    "\n",
    "##### Text preprocessing #####\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def text_normalize(txt):\n",
    "    processed_text = re.sub('[^a-zA-Z]', ' ', txt)\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_triplet(final_text):\n",
    "    final_text = get_unique_text(final_text)\n",
    "    doc = nlp(final_text)\n",
    "    valid_sents=[]\n",
    "\n",
    "    for s in list(doc.sents):\n",
    "        if syntax_full(s):\n",
    "            valid_sents.append(s.text)\n",
    "    final_text = ' '.join(valid_sents)\n",
    "    return final_text\n",
    "\n",
    "def coref_res(rawtext, coref_greedn = 0.5):\n",
    "\n",
    "    neuralcoref.add_to_pipe(nlp, greedyness = coref_greedn, store_scores=False)\n",
    "    doc = nlp(rawtext)\n",
    "\n",
    "    resolved = list(tok.text_with_ws for tok in doc)\n",
    "\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        for coref in cluster:\n",
    "            if coref != cluster.main:\n",
    "                if coref.text[0].isalpha() and coref.text[0].isupper():\n",
    "\n",
    "                    main_words_list=word_tokenize(cluster.main.text)\n",
    "                    main_words_list[0]=main_words_list[0].capitalize()\n",
    "                    resolved[coref.start] = detokenizer(main_words_list) + doc[coref.end-1].whitespace_\n",
    "\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "            else:\n",
    "                resolved[coref.start] = cluster.main.text + doc[coref.end-1].whitespace_\n",
    "                for i in range(coref.start+1, coref.end):\n",
    "                    resolved[i] = \"\"\n",
    "\n",
    "    text_resolved = ''.join(resolved)\n",
    "    nlp.remove_pipe(\"neuralcoref\")\n",
    "\n",
    "    return text_resolved\n",
    "\n",
    "def compress(spacy_sents,sents_whitelist):\n",
    "    blacklist_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if token.dep_ in ['appos','advmod']:\n",
    "                    token_sub_tree=token.subtree\n",
    "                    for t in token_sub_tree:\n",
    "                        blacklist_tokens.append(t.i)\n",
    "\n",
    "        n=n+1\n",
    "    return(blacklist_tokens)\n",
    "\n",
    "def spacy_compress(rawtext):\n",
    "\n",
    "    doc1 = nlp(rawtext)\n",
    "    sents_whitelist = get_sents_ids_whitelist(doc1.sents)\n",
    "\n",
    "    tokens_blacklist = compress(doc1.sents,sents_whitelist)\n",
    "    sents_tokens = get_list_sents_tokens(doc1.sents,sents_whitelist,tokens_blacklist)\n",
    "    compressed_text_sents = []\n",
    "\n",
    "    for s in sents_tokens:\n",
    "        text=detokenizer(s)\n",
    "        compressed_text_sents.append(text)\n",
    "    compressed_text_sents=sentence_grammar_fix(compressed_text_sents)\n",
    "    text =' '.join(compressed_text_sents)\n",
    "\n",
    "    return(text)\n",
    "##### Text preprocessing #####\n",
    "\n",
    "def readingTime(mytext):\n",
    "    total_words = len(word_tokenize(mytext))\n",
    "    estimatedTime = round(total_words/200.0,1)\n",
    "    return estimatedTime\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_min_num_of_clauses(spacy_sentence, n):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
    "            result.append(token.head.text)\n",
    "    if len(result)>=n:\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_sents_ids_whitelist(spacy_sents):\n",
    "    whitelist=[]\n",
    "    i=1\n",
    "    sents_texts=[]\n",
    "    for sent in spacy_sents:\n",
    "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
    "            whitelist.append(i)\n",
    "            sents_texts.append(sent.text)\n",
    "        i=i+1\n",
    "    return(whitelist)\n",
    "\n",
    "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
    "    sents_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        sent_tokens=[]\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if (token.i not in blacklist_tokens):\n",
    "                    sent_tokens.append(token.text)\n",
    "            sents_tokens.append(sent_tokens)\n",
    "            sent_tokens=[]\n",
    "\n",
    "        n=n+1\n",
    "    return(sents_tokens)\n",
    "\n",
    "def detokenizer(list_of_tokens):\n",
    "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
    "    return(text_str)\n",
    "\n",
    "def sentence_grammar_fix(sentences):\n",
    "    fixed=[]\n",
    "    for sent in sentences:\n",
    "\n",
    "        sent=sent.strip()\n",
    "        sent=sent.replace('\\n','')\n",
    "        sent=sent.replace('()','')\n",
    "\n",
    "        sent=re.sub('\\s+',' ',sent)\n",
    "        sent=sent+'.'\n",
    "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0] in ['.',',','-','—']:\n",
    "                sent=sent[1:]\n",
    "        sent=sent.strip()\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0].isalpha():\n",
    "                sent=sent[0].upper()+sent[1:]\n",
    "        fixed.append(sent)\n",
    "\n",
    "    return(fixed)\n",
    "\n",
    "############# Parse Wiki ############# \n",
    "def parse_wiki(google_url):\n",
    "    \n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver\n",
    "    \n",
    "    # get urls  \n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    url_list = [i for i in url_list if 'https://en.wikipedia.org' in i]\n",
    "    \n",
    "    title_list = []\n",
    "    for i in url_list:\n",
    "        try:\n",
    "            if 'https://en.wikipedia.org' in i: \n",
    "                title_list.append(i.split('/')[4])  \n",
    "        except:\n",
    "            continue\n",
    "    #/ get urls\n",
    "        \n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "    \n",
    "    return title_list     \n",
    "############# Parse Wiki ############# \n",
    "\n",
    "############# Parse Arxiv #############\n",
    "def parse_arxiv(query, page_number):\n",
    "    arxivtext = ''\n",
    "    \n",
    "    urls = []\n",
    "    titles = []\n",
    "\n",
    "    closest_value = 100\n",
    "    req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value) + '&searchtype=all&source=header&start=0'\n",
    "    htmlString = get(req)\n",
    "\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib')\n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "\n",
    "    titles = list(soup.find_all('p', {'class' : 'title is-5 mathjax'}))[:page_number]\n",
    "    titles_r = [i.text.replace('\\n','').replace('  ','') for i in titles]\n",
    "    titles = ', '.join(titles_r)\n",
    "\n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])\n",
    "\n",
    "    txt = []\n",
    "    for i in tqdm(urls[:page_number]):\n",
    "        \n",
    "        time.sleep(random.randint(1,8))\n",
    "\n",
    "        soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "        abstract = soup.find('blockquote').text.replace('  ',' ')\n",
    "        parsed_pdf = parser.from_file(str(i).replace('abs','pdf'))['content']  \n",
    "\n",
    "        extended_abs = ' '.join(filter_text(parsed_pdf, abstract, threshold=0.5))    \n",
    "        txt.append(extended_abs.replace('\\n', ' ').replace('Abstract', '').replace('ABSTRACT', ''))\n",
    "\n",
    "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, urls, titles_r)), columns=['text','link', 'page'])\n",
    "\n",
    "    return arxivtext, titles, df\n",
    "#############/Parse Arxiv #############\n",
    "\n",
    "def longest_common_substring(s1, s2):\n",
    "  m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "  longest, x_longest = 0, 0\n",
    "  for x in range(1, 1 + len(s1)):\n",
    "    for y in range(1, 1 + len(s2)):\n",
    "      if s1[x - 1] == s2[y - 1]:\n",
    "        m[x][y] = m[x - 1][y - 1] + 1\n",
    "        if m[x][y] > longest:\n",
    "          longest = m[x][y]\n",
    "          x_longest = x\n",
    "      else:\n",
    "        m[x][y] = 0\n",
    "  return s1[x_longest - longest: x_longest]\n",
    "\n",
    "def longest_common_sentence(s1, s2):\n",
    "    s1_words = s1.split(' ')\n",
    "    s2_words = s2.split(' ')\n",
    "    return ' '.join(longest_common_substring(s1_words, s2_words))\n",
    "\n",
    "def css(a,b):\n",
    "    if len(a.split()) > 0:\n",
    "        score = len(longest_common_sentence(a,b).split())/len(a.split())\n",
    "    else:    \n",
    "        score = 0\n",
    "    return score \n",
    "#/common string #\n",
    "\n",
    "############# Parse Google #############\n",
    "def parse_google(system, query, keys_number, page_number, tag, cls=''):   \n",
    "    \n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver \n",
    "\n",
    "    # get urls\n",
    "    google_url = system + query + \"&num=\" + str(page_number+1)\n",
    "    google_url = google_url + '&hl=en&gl=en' + '&lr=lang_en&cr=countryGB'\n",
    "    \n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    #/ get urls\n",
    "\n",
    "    # load content \n",
    "    txt = []\n",
    "    titles = []\n",
    "    for j in tqdm(url_list):\n",
    "        delta = random.randint(1,1)\n",
    "        time.sleep(delta)\n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'): \n",
    "                file_data = parser.from_file(str(j))           \n",
    "                t = file_data['content']\n",
    "                titles.append(t[:100])\n",
    "            else:\n",
    "                t = parse_page(j,tag,cls)[0].replace('\\n','')\n",
    "                titles.append(parse_page(j,tag,cls)[1])\n",
    "            \n",
    "            txt.append(t)\n",
    "            \n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "       \n",
    "    googletext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    \n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "   \n",
    "    return googletext, errors, df, titles\n",
    "#############/Parse Google #############\n",
    "\n",
    "############# Doc preparation ##########\n",
    "def add_hyperlink(paragraph, text, url, flag):\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element and a new w:rPr element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    # Create a new Run object and add the hyperlink into it\n",
    "    r = paragraph.add_run()\n",
    "    r._r.append (hyperlink) \n",
    "\n",
    "    # A workaround for the lack of a hyperlink style (doesn't go purple after using the link)\n",
    "    # Delete this if using a template that has the hyperlink style in it\n",
    "    r.font.color.theme_color = MSO_THEME_COLOR_INDEX.HYPERLINK\n",
    "    r.font.underline = flag\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "def save_doc(final_summary, summary, query, docs_number, readtime, url_keys, url_marks, score):\n",
    "    \n",
    "    sent_list = list(final_summary.split(sep='<hr>'))\n",
    "    doc = Document()\n",
    "    style = doc.styles['Normal']\n",
    "    \n",
    "    font = style.font\n",
    "    font.name = 'Times New Roman'\n",
    "    font.size = Pt(12)\n",
    "\n",
    "    hd = doc.add_paragraph()\n",
    "    hd.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "    hd.add_run('Summary').bold = True\n",
    "\n",
    "    if query != 'none':\n",
    "        hd = doc.add_paragraph('Request: ' + \"''\" + query + \"''\")\n",
    "\n",
    "    hd = doc.add_paragraph('Read time: ' + str(readtime) + ' min')\n",
    "    hd = doc.add_paragraph('Information: ' + str(score) + '%')\n",
    "    hd = doc.add_paragraph('Documents: ' + str(docs_number))\n",
    "    \n",
    "    hd = doc.add_paragraph('')\n",
    "    \n",
    "    hd.add_run('Keys:\\n').underline = True\n",
    "    \n",
    "    for j in url_keys:\n",
    "        if j != url_keys[-1]:\n",
    "            add_hyperlink(hd, (str(j.split('/keyword/')[1]) + ', '), str(j.split('/keyword/')[0]), False)\n",
    "        else:\n",
    "            add_hyperlink(hd, (str(j.split('/keyword/')[1])), str(j.split('/keyword/')[0]), False)\n",
    "        \n",
    "    hd.add_run('\\n\\nBenchmarks:\\n').underline = True\n",
    "    \n",
    "    for j in url_marks:\n",
    "        if j != url_marks[-1]:\n",
    "            add_hyperlink(hd, str(j.split('/keyword/')[1]) + ', ', str(j.split('/keyword/')[0]), False)\n",
    "        else:\n",
    "            add_hyperlink(hd, str(j.split('/keyword/')[1]), str(j.split('/keyword/')[0]), False)\n",
    "    \n",
    "    r = hd.add_run()\n",
    "    for i in sent_list:\n",
    "        hd.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "        if query != 'none':\n",
    "            try:\n",
    "                link = re.search(r\"<a href=(.*?)target='_blank'\", str(i)).group(1).replace(' ','')\n",
    "                hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))               \n",
    "                add_hyperlink(hd, 'More', link, True).add_run()\n",
    "            except:\n",
    "                link = ''\n",
    "        if query == 'none':\n",
    "            hd = doc.add_paragraph(striphtml(str(i)).replace('<hr>','').replace('<u>','').replace('More',''))    \n",
    "         \n",
    "    doc.save('docs/' + summary + '.docx')\n",
    "    \n",
    "    return True\n",
    "#############/Doc preparation ##########\n",
    "\n",
    "############## Get summary and tags ##########\n",
    "def get_summary(rawtext, readtime):\n",
    "    sentences = int(readtime/(np.median([len(i.split()) for i in nltk.sent_tokenize(rawtext)])/200))\n",
    "\n",
    "    stemmer = Stemmer(\"english\")\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(\"english\")\n",
    "    parser = PlaintextParser.from_string(' '.join(sent_tokenize(rawtext)[6:]), Tokenizer(\"english\"))\n",
    "\n",
    "    text_list = []\n",
    "    for sentence in summarizer(parser.document, sentences):\n",
    "        text_list.append(str(sentence))\n",
    "\n",
    "    txt = ' '.join(sent_tokenize(rawtext)[:6]) + ' '+' '.join(text_list)\n",
    "\n",
    "    z = 0\n",
    "    output = []\n",
    "    pdf_output = []\n",
    "\n",
    "    for i in nltk.sent_tokenize(txt):\n",
    "        z = z+1\n",
    "        output.append('\\n\\n<hr>' + str(z) + '. ' + str(i))\n",
    "        pdf_output.append('<hr>' + str(z) + '. ' + str(i))\n",
    "\n",
    "    txt = ''.join(output) + '<hr>'\n",
    "    pdf_txt = ''.join(pdf_output) + '<hr>'\n",
    "    return txt, pdf_txt\n",
    "\n",
    "def graph_keys(final_text, top_number):\n",
    "    \n",
    "    bigrams = list(nltk.ngrams(text_normalize(final_text.lower()),2))\n",
    "    bigrams = [' '.join(i) for i in bigrams if (i[0]!=i[1])] \n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    \n",
    "    df = pd.DataFrame(bigram_counts.most_common(len(bigram_counts)), columns=['bigram', 'count'])[:top_number]\n",
    "    df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
    "    keys = ', '.join(list(df['bigram'].astype(str)))\n",
    "\n",
    "    return keys\n",
    "\n",
    "def get_entities(rawtext, tops):\n",
    "    \n",
    "    spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
    "    nlp.max_length = 1000000000000\n",
    "    doc = spacy_nlp(rawtext)\n",
    "\n",
    "    ners = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PERSON']:\n",
    "            ners.append(ent.text)\n",
    "   \n",
    "    ner_counts = collections.Counter(ners)\n",
    "\n",
    "    df = pd.DataFrame(ner_counts.most_common(len(ner_counts)), columns=['ner_names', 'count'])[:tops]\n",
    "    df['count'] = 100*df['count']/df['count'].sum().astype(int) \n",
    "    keys = ', '.join(list(df['ner_names'].astype(str)))\n",
    "    \n",
    "    return keys\n",
    "##############/Get summary and tags ##########\n",
    "\n",
    "############## Extend summary ##########\n",
    "def get_ngrams(text): \n",
    "    grams = nltk.ngrams(text.split(), 2)\n",
    "    grams_list = []\n",
    "    for i in grams:\n",
    "        grams_list.append(i)\n",
    "    \n",
    "    return grams_list \n",
    "\n",
    "def get_jaccard_sim(a,b):\n",
    "    a, b = set(get_ngrams(a)), set(get_ngrams(b)) \n",
    "    c = a.intersection(b)\n",
    "\n",
    "    return round(float(len(c)/len(a)), 2) \n",
    "\n",
    "def filter_text(page_txt, page_sum, threshold=0.5): \n",
    "    sent_list = []  \n",
    "    for j in page_txt.split('.'):\n",
    "        try:\n",
    "            sim_score = get_jaccard_sim(j, page_sum)\n",
    "        except:\n",
    "            sim_score = 0\n",
    "            \n",
    "        if sim_score > threshold:\n",
    "            sent_list.append(j + '.')\n",
    "            \n",
    "    return sent_list \n",
    "##############/Extend summary ##########\n",
    "\n",
    "############## Add keyurls ################\n",
    "def add_keyurls(final_keys, query):\n",
    "    url_keys = []\n",
    "    for i in final_keys.split(','):\n",
    "        url = 'https://www.google.com/search?q=' + '+'.join(re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip().split()) + '+' + query + '/keyword/' + i \n",
    "        url_keys.append(url)\n",
    "        \n",
    "    return url_keys     \n",
    "##############/Add urls ################\n",
    "\n",
    "############# Parse Patents #############\n",
    "def parse_patents(query, keys_number, page_number, tag, cls=''):   \n",
    "    \n",
    "    # load driver\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    #/load driver\n",
    "\n",
    "    # get urls \n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1)\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "\n",
    "    descriptions = []\n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "                titles.append(title)\n",
    "                descriptions.append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = links[:(page_number)]\n",
    "    #/ get urls\n",
    "\n",
    "    # load content \n",
    "    txt = []\n",
    "    titles = []\n",
    "    for j in tqdm(url_list):\n",
    "        delta = random.randint(1,1)\n",
    "        time.sleep(delta)\n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'): \n",
    "                file_data = parser.from_file(str(j))           \n",
    "                t = file_data['content']\n",
    "            else:\n",
    "                t = parse_page(j,tag,cls)\n",
    "            txt.append(t)\n",
    "            titles.append(''.join(sent_tokenize(t)[:3]))\n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "       \n",
    "    googletext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    \n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "   \n",
    "    return googletext, errors, df\n",
    "#############/Parse Patents #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:53:04.841399Z",
     "start_time": "2020-12-21T19:53:04.837410Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'docs/'\n",
    "\n",
    "readtime = 10\n",
    "\n",
    "page_number = 10\n",
    "keys_number = 10\n",
    "\n",
    "process_time = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature toggles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:53:08.666150Z",
     "start_time": "2020-12-21T19:53:08.660195Z"
    }
   },
   "outputs": [],
   "source": [
    "compress = True\n",
    "\n",
    "wiki_sum = True\n",
    "gogle_sum = True\n",
    "\n",
    "arxiv_sum = True\n",
    "patnt_sum = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:53:16.919472Z",
     "start_time": "2020-12-21T19:53:11.385977Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text summarization\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Wiki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:54:06.327723Z",
     "start_time": "2020-12-21T19:53:25.302758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Trying to download new driver from http://chromedriver.storage.googleapis.com/87.0.4280.88/chromedriver_win32.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\skamenshchikov\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.88]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: sentence extraction, multi document, document summarization, summary translation, extraction technique, summarization process, process set, set data, data create, create subset \n",
      "\n",
      "|Entities|: Automatic, ROUGE, Recall-Oriented Understudy \n",
      "\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wikitext = ''\n",
    "wikikeys = ''\n",
    "\n",
    "df_wiki = pd.DataFrame()\n",
    "\n",
    "if wiki_sum == True: \n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "\n",
    "    red_query = \"https://www.google.com/search?q=\" + 'site:https://en.wikipedia.org ' + query + \"&num=\" + str(page_number+1)\n",
    "    wiki_titles = parse_wiki(red_query)\n",
    "\n",
    "    txts = []\n",
    "    titles = []\n",
    "\n",
    "    for i in tqdm(wiki_titles):  \n",
    "        page_sum = wiki_wiki.page(i).summary\n",
    "        page_txt = wiki_wiki.page(i).text\n",
    "        sent_list = filter_text(page_txt, page_sum, threshold=0.5)\n",
    "       \n",
    "        titles.append(i)\n",
    "        txts.append(''.join(sent_list).replace('\\n', ''))        \n",
    "    \n",
    "    wikitext = ''.join(txts).replace('\\n','') \n",
    "\n",
    "    if compress == True:\n",
    "        wikitext = coref_res(filter_triplet(wikitext))\n",
    "\n",
    "    wikikeys = graph_keys(wikitext, keys_number)\n",
    "    wiki_entities = get_entities(wikitext, keys_number)\n",
    "\n",
    "    url_list = [str('https://en.wikipedia.org/wiki/' + i)  for i in wiki_titles] \n",
    "    df_wiki = pd.DataFrame(list(zip(txts, url_list, titles)), columns=['text','link', 'page'])\n",
    "\n",
    "    df_wiki.replace('', np.nan, inplace=True)\n",
    "    df_wiki.dropna(inplace=True) \n",
    "\n",
    "    print('|Keywords|:', wikikeys, '\\n')\n",
    "    print('|Entities|:', wiki_entities, '\\n')\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Wiki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:57:47.106577Z",
     "start_time": "2020-12-21T19:57:47.024225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Automatic summarization is the process of shor...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Automatic_summar...</td>\n",
       "      <td>Automatic_summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multi-document summarization is an automatic p...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Multi-document_s...</td>\n",
       "      <td>Multi-document_summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROUGE, or Recall-Oriented Understudy for Gisti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/ROUGE_(metric)</td>\n",
       "      <td>ROUGE_(metric)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence extraction is a technique used for au...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sentence_extraction</td>\n",
       "      <td>Sentence_extraction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Automatic summarization is the process of shor...   \n",
       "1  Multi-document summarization is an automatic p...   \n",
       "2  ROUGE, or Recall-Oriented Understudy for Gisti...   \n",
       "3  Sentence extraction is a technique used for au...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://en.wikipedia.org/wiki/Automatic_summar...   \n",
       "1  https://en.wikipedia.org/wiki/Multi-document_s...   \n",
       "2       https://en.wikipedia.org/wiki/ROUGE_(metric)   \n",
       "3  https://en.wikipedia.org/wiki/Sentence_extraction   \n",
       "\n",
       "                           page  \n",
       "0       Automatic_summarization  \n",
       "1  Multi-document_summarization  \n",
       "2                ROUGE_(metric)  \n",
       "3           Sentence_extraction  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Arxiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T20:17:18.170540Z",
     "start_time": "2020-12-21T20:15:21.983987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]2020-12-21 23:15:35,495 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.09355 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.09355.\n",
      " 10%|████████▎                                                                          | 1/10 [00:12<01:50, 12.26s/it]2020-12-21 23:15:41,745 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.07619 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.07619.\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:18<01:23, 10.40s/it]2020-12-21 23:15:50,808 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.07563 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.07563.\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:27<01:10, 10.09s/it]2020-12-21 23:15:58,173 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.07280 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.07280.\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:34<00:54,  9.03s/it]2020-12-21 23:16:06,858 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.04821 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.04821.\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:41<00:41,  8.39s/it]2020-12-21 23:16:13,660 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.04778 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.04778.\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:48<00:32,  8.11s/it]2020-12-21 23:16:25,584 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.04307 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.04307.\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [00:59<00:27,  9.03s/it]2020-12-21 23:16:36,702 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.04281 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.04281.\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [01:12<00:20, 10.05s/it]2020-12-21 23:16:46,697 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.03942 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.03942.\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [01:23<00:10, 10.39s/it]2020-12-21 23:16:57,926 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/2012.03656 to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/pdf-2012.03656.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:34<00:00, 10.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: relation extraction, text summarization, text generation, cx db, target language, model transfer, binary relation, simple binary, generation method, cross model \n",
      "\n",
      "|Entities|: CX_DB8, FACTGEN, Automatic, NIST, BERT, Email, making.kr, Transformer, CTRLsum, CNN \n",
      "\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arxivtext = ''\n",
    "arxivkeys = ''\n",
    "\n",
    "df_arxiv = pd.DataFrame()\n",
    "\n",
    "if arxiv_sum == True:\n",
    "    try:\n",
    "        df_arxiv = parse_arxiv(query, page_number)[2]  \n",
    "        arxivtext = ''.join(list(df_arxiv['text'])) \n",
    "    \n",
    "        if compress == True:\n",
    "            arxivtext = coref_res(filter_triplet(arxivtext))\n",
    "    \n",
    "        arxiv_entities = get_entities(arxivtext, keys_number)\n",
    "        arxivkeys = graph_keys(arxivtext, keys_number)\n",
    "    \n",
    "        print('|Keywords|:', arxivkeys, '\\n')\n",
    "        print('|Entities|:', arxiv_entities, '\\n')\n",
    "    \n",
    "    except:\n",
    "        print('No data')\n",
    "    \n",
    "    df_arxiv.replace('', np.nan, inplace=True)\n",
    "    df_arxiv.dropna(inplace=True)      \n",
    "    \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Arxiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T20:18:17.864582Z",
     "start_time": "2020-12-21T20:18:17.844635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edu   Information retrieval (IR) for precision...</td>\n",
       "      <td>https://arxiv.org/abs/2012.09355</td>\n",
       "      <td>Literature Retrieval for Precision Medicine wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nl   Automatic text summarization has enjoyed ...</td>\n",
       "      <td>https://arxiv.org/abs/2012.07619</td>\n",
       "      <td>What Makes a Good Summary? Reconsidering the F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the  healthcare domain, medical experts cr...</td>\n",
       "      <td>https://arxiv.org/abs/2012.07563</td>\n",
       "      <td>A Practical Approach towards Causality Mining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kr    Recently, sequence-to-sequence (seq2seq)...</td>\n",
       "      <td>https://arxiv.org/abs/2012.07280</td>\n",
       "      <td>Contrastive Learning with Adversarial Perturba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Relation extraction is very important for kno...</td>\n",
       "      <td>https://arxiv.org/abs/2012.04821</td>\n",
       "      <td>Complex Relation Extraction: Challenges and Op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  edu   Information retrieval (IR) for precision...   \n",
       "1  nl   Automatic text summarization has enjoyed ...   \n",
       "2   In the  healthcare domain, medical experts cr...   \n",
       "3  kr    Recently, sequence-to-sequence (seq2seq)...   \n",
       "4   Relation extraction is very important for kno...   \n",
       "\n",
       "                               link  \\\n",
       "0  https://arxiv.org/abs/2012.09355   \n",
       "1  https://arxiv.org/abs/2012.07619   \n",
       "2  https://arxiv.org/abs/2012.07563   \n",
       "3  https://arxiv.org/abs/2012.07280   \n",
       "4  https://arxiv.org/abs/2012.04821   \n",
       "\n",
       "                                                page  \n",
       "0  Literature Retrieval for Precision Medicine wi...  \n",
       "1  What Makes a Good Summary? Reconsidering the F...  \n",
       "2  A Practical Approach towards Causality Mining ...  \n",
       "3  Contrastive Learning with Adversarial Perturba...  \n",
       "4  Complex Relation Extraction: Challenges and Op...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arxiv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T20:46:45.020633Z",
     "start_time": "2020-12-21T20:45:17.983143Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Driver [C:\\Users\\skamenshchikov\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.88\\chromedriver.exe] found in cache\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing error: https://www.imperial.ac.uk/pls/portallive/docs/1/18619759.PDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:12<01:53, 12.64s/it]2020-12-21 23:45:39,190 [MainThread  ] [INFO ]  Retrieving https://core.ac.uk/download/pdf/11784914.pdf to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/download-pdf-11784914.pdf.\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:16<01:19,  9.97s/it]2020-12-21 23:45:42,923 [MainThread  ] [INFO ]  Retrieving https://core.ac.uk/download/pdf/81967205.pdf to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/download-pdf-81967205.pdf.\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:19<00:54,  7.84s/it]2020-12-21 23:45:45,786 [MainThread  ] [INFO ]  Retrieving https://www.fosteropenscience.eu/sites/default/files/pdf/2932.pdf to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/sites-default-files-pdf-2932.pdf.\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:22<00:38,  6.38s/it]2020-12-21 23:45:48,756 [MainThread  ] [INFO ]  Retrieving https://www.cs.kent.ac.uk/people/staff/aaf/pub_papers.dir/SBIA-2002-Joel.pdf to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/people-staff-aaf-pub_papers.dir-sbia-2002-joel.pdf.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:48<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keywords|: text summarization, et al, speech summarization, topic model, topic word, document summarization, multi document, word sentence, tf idf, feature sentence \n",
      "\n",
      "|Entities|: Automatic, LSA, Pyramid, EM, Data, ASR\n",
      "\n",
      "Acoustic Information, Greedy, Disember, Microsoft, ROUGE \n",
      "\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "googletext = ''\n",
    "googlekeys = ''\n",
    "\n",
    "df_google = pd.DataFrame()\n",
    "\n",
    "if gogle_sum == True:\n",
    "    try:\n",
    "        google_data = parse_google(\"https://www.google.com/search?q=\", query, keys_number, page_number, 'body')\n",
    "        df_google = google_data[2]\n",
    "    \n",
    "        googletext = ''.join(list(df_google['text']))\n",
    "    \n",
    "        if compress == True:\n",
    "            googletext = coref_res(filter_triplet(googletext))\n",
    "    \n",
    "        google_entities = get_entities(googletext, keys_number)\n",
    "        googlekeys = graph_keys(googletext, keys_number)\n",
    "    \n",
    "        print('|Keywords|:', googlekeys, '\\n')\n",
    "        print('|Entities|:', google_entities, '\\n')\n",
    "  \n",
    "    \n",
    "    except:\n",
    "        print('No data')\n",
    "    \n",
    "df_google.replace('', np.nan, inplace=True)\n",
    "df_google.dropna(inplace=True)      \n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T20:19:18.359212Z",
     "start_time": "2020-12-21T20:19:18.340299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>https://www.imperial.ac.uk/pls/portallive/docs...</td>\n",
       "      <td>Trends and Applications of Automatic Text Summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>https://core.ac.uk/download/pdf/11784914.pdf</td>\n",
       "      <td>403 Forbidden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>https://core.ac.uk/download/pdf/81967205.pdf</td>\n",
       "      <td>403 Forbidden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>https://www.fosteropenscience.eu/sites/default...</td>\n",
       "      <td>A Gentle Introduction to Text Summarization in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automatic Text Summarization (ATS) aims at gen...</td>\n",
       "      <td>https://www.cs.kent.ac.uk/people/staff/aaf/pub...</td>\n",
       "      <td>A Quick Introduction to Text Summarization in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "4  Automatic Text Summarization (ATS) aims at gen...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.imperial.ac.uk/pls/portallive/docs...   \n",
       "1       https://core.ac.uk/download/pdf/11784914.pdf   \n",
       "2       https://core.ac.uk/download/pdf/81967205.pdf   \n",
       "3  https://www.fosteropenscience.eu/sites/default...   \n",
       "4  https://www.cs.kent.ac.uk/people/staff/aaf/pub...   \n",
       "\n",
       "                                                page  \n",
       "0  Trends and Applications of Automatic Text Summ...  \n",
       "1                                      403 Forbidden  \n",
       "2                                      403 Forbidden  \n",
       "3  A Gentle Introduction to Text Summarization in...  \n",
       "4  A Quick Introduction to Text Summarization in ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_google.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T20:20:38.086836Z",
     "start_time": "2020-12-21T20:20:38.077894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n105 \\n\\nSan \\nAUTOMATIC TEXT SUMMARIZATION USING FEATURE-�\\n\\nBASED FUZZY EXTRACTION� 2.0\", \\n\\nKey \\nLadda Suanrnali\\n\\n1\\n, Naomie Salim\\n\\n2\\n, Mohammed Salem Binwahlan3 \\n\\nIvan \\n\\nrsity, IFaculty of Science and Technology \\nSuan Dusit Rajabhat University \\n\\n295 Rajasrima Rd, Dusit, Bangkok, Thailand 10300 \\n~ce\", \\n\\n2.3Faculty of Computer Science and Infonnation System� \\nUniversiti Teknologi Malaysia� \\n\\n81310 Skudai, Johor� \\n\\nE-mail: Iladda_sua@dusit.ac.th.2naomie@utm.my.3moham2007med@yahoo.com \\n\\nAbstract: Automatic text summarization is to compress the original text into a shorter version \\n\\nand help the user to quickly understand large volumes of infonnation. This paper focuses on the \\n\\nautomatic text summarization by sentence extraction with important features based on fuzzy \\n\\nlogic. In our experiment, we used 6 test documents in DUC2002 data set. Each document is \\n\\nprepared by preprocessing process: sentence segmentation, tokenization, renlUving Stop Word \\n\\nand stemming Word. Then, we use 8 important features and calculate their score for each \\n\\nsentence. We propose a method using fuzzy logic for sentence extraction and compare our \\n\\nresult with the baseline summarizer and Microsoft Word 2007 summarizers. The results show \\n\\nthat the highest average precision, recall, and F-mean for the summaries are conducted from \\n\\nfuzzy method. \\n\\nKeyword: Automatic text summarization, Fuzzy logic, Sentence extraction \\n\\n1. INTRODUCTION \\n\\nAutomatic text summarization is the summary of the source text by machine to present the most \\n\\nimportant infonnation in a shorter version of the original text while still keeping its main \\n\\nsemantic content and helps the user to quickly understand large volumes of information. Text \\n\\nsummarization addresses both the problem of selecting the most important portions of text and \\n\\nthe problem of generating coherent summaries. This process is significantly different from that \\n\\nJilid 20, Bit. 2 (Disember 2008) Jurnal Teknologi Maklumat \\n\\n105\\n\\nAUTOMATIC TEXT SUMMARIZATION USING FEATURE-\\n\\nBASED FUZZY EXTRACTION\\n\\nLadda Suanmali\\n1\\n, Naomie Salim\\n\\n2\\n, Mohammed Salem Binwahlan\\n\\n3\\n\\n\\'Faculty of Science and Technology\\nSuan Dusit Rajabhat University\\n\\n295 Rajasrima Rd, Dusit, Bangkok, Thailand 10300\\n\\n2,3Faculty of Computer Science and Information System\\nUniversiti Teknologi Malaysia\\n\\n81310 Skudai, lohor\\n\\nE-mail: Iladda_sua@dusit.ac.th.2naomie@utrn.my.3moham2007med@yahoo.com\\n\\nAbstract: Automatic text summarization is to compress the original text into a shorter version\\n\\nand help the user to quickly understand large volumes of information. This paper focuses on the\\n\\nautomatic text summarization by sentence extraction with important features based on fuzzy\\n\\nlogic. In our experiment, we used 6 test documents in DUC2002 data set. Each document is\\n\\nprepared by preprocessing process: sentence segmentation, tokenization, remuving Stop Word\\n\\nand stemming Word. Then, we use 8 important features and calculate their score for each\\n\\nsentence. We propose a method using fuzzy logic for sentence extraction and compare our\\n\\nresult with the baseline summarizer and Microsoft Word 2007 summarizers. The results show\\n\\nthat the highest average precision, recall, and F-mean for the summaries are conducted from\\n\\nfuzzy method.\\n\\nKeyword: Automatic text summarization, Fuzzy logic, Sentence extraction\\n\\n. 1. INTRODUCTION\\n\\nAutomatic text summarization is the summary of the source text by machine to present the most\\n\\nimportant information in a shorter version of the original text while still keeping its main\\n\\nsemantic content and helps the user to quickly understand large volumes of information. Text\\n\\nsummarization addresses both the problem of selecting the most important portions of text and\\n\\nthe problem of generating coherent summaries. This process is significantly different from that\\n\\nIilid 20, Bil. 2 (Disember 2008) Iurnal Teknologi Maklumat\\n\\n\\n\\n106 \\n\\n-----------------------\",.--\\n\\nof human based text summarization since human can capture and relate deep meanings and \\n\\nthemes of text documents while automation of such a skill is very difficult to implement. A \\n\\nnumber of researchers have proposed techniques for automatic text summarization which can \\n\\nbe classified into two categories: extraction and abstraction. Extraction summary is a selection \\n\\nof sentences or phrases from the original text with the highest score and put it together to a new \\n\\nshorter text without changing the source text. Abstraction summary method uses linguistic \\n\\nmethods to examine and interpret the text. Most of the current automated text summarization \\n\\nsystem use extraction method to produce summary. Automatic text summarization works best \\n\\non well-structured documents, such as news, reports, articles and scientific papers. \\n\\nIn this paper, we propose text summarization based on fuzzy logic aided method to \\n\\nextract important sentences as a summary of document. The rest of this paper is organized as \\n\\nfollows. Section 2 presents the summarization approach. Section 3 and 4 describes our \\n\\nproposed, followed by experimental design, experimental results and evaluation. Finally, we \\n\\nconclude and suggest future work that can be carried out in Section 5. \\n\\n2. SUMMARIZAnON APPROACH \\n\\nAutomatic text summarization dates back to the Fifties, when Luhn created the first \\n\\nsummarization system [1] in 1958. Rath et al. [2] in 1961 proposed empirical evidences for \\n\\ndifficulties inherent in the notion of ideal summary. Both studies used thematic features such as \\n\\nterm frequency, thus they characterized by surface-level approaches. In the early 1960s, new \\n\\napproaches called entity-level approaches appeared; the first approach of this kind used \\n\\nsyntactic analysis [3]. The location features were used in [4], where key phrases are used dealt \\n\\nwith three additional components: pragmatic words (cue words, i.e., words would have positive \\n\\nor negative effect on the respective sentence weight like significant, key idea, or hardly); title \\n\\nand heading words; and structural indicators (sentence location, where the sentences appearing \\n\\nin initial or final of text unit are more significant to include in the summary. \\n\\nIn this paper, we propose important sentence extraction used fuzzy rules and a set for \\n\\nselecting sentences based on their features. Fuzzy set proposed by Zadeh [10] is a mathematical \\n\\ntool for dealing with uncertainty, vagueness and ambiguity. Its application in text representation \\n\\nfor information retrieval was first proposed by Buell [11], in which a document can be \\n\\nrepresented as a fuzzy set of terms. Miyamoto [12] investigated applications of fuzzy set theory \\n\\nin information retrieval and cluster analysis. Witte and Bergler [13] presented a fuzzy-theory \\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Maklumat \\n\\n106\\n\\nof human based text summarization since human can capture and relate deep meanings and\\n\\nthemes of text documents while automation of such a skill is very difficult to implement. A\\n\\nnumber of researchers have proposed techniques for automatic text summarization which can\\n\\nbe classified into two categories: extraction and abstraction. Extraction summary is a selection\\n\\nof sentences or phrases from the original text with the highest score and put it together to a new\\n\\nshorter text without changing the source text. Abstraction summary method uses linguistic\\n\\nmethods to examine and interpret the text. Most of the current automated text summarization\\n\\nsystem use extraction method to produce summary. Automatic text summarization works best\\n\\non well-structured documents, such as news, reports, articles and scientific papers.\\n\\nIn this paper, we propose text summarization based on fuzzy logic aided method to\\n\\nextract important sentences as a summary of document. The rest of this paper is organized as\\n\\nfollows. Section 2 presents the summarization approach. Section 3 and 4 describes our\\n\\nproposed, followed by experimental design, experimental results and evaluation. Finally, we\\n\\nconclude and suggest future work that can be carried out in Section 5.\\n\\n2. SUMMARIZAnON APPROACH\\n\\nAutomatic text summarization dates back to the Fifties, when Luhn created the first\\n\\nsummarization system [1] in 1958. Rath et al. [2] in 1961 proposed empirical evidences for\\n\\ndifficulties inherent in the notion of ideal summary. Both studies used thematic features such as\\n\\nterm frequency, thus they characterized by surface-level approaches. In the early 1960s, new\\n\\napproaches called entity-level approaches appeared; the first approach of this kind used\\n\\nsyntactic analysis [3]. The location features were used in [4], where key phrases are used dealt\\n\\nwith three additional components: pragmatic words (cue words, i.e., words would have positive\\n\\nor negative effect on the respective sentence weight like significant, key idea, or hardly); title\\n\\nand heading words; and structural indicators (sentence location, where the sentences appearing\\n\\nin initial or final of text unit are more significant to include in the summary.\\n\\nIn this paper, we propose important sentence extraction used fuzzy rules and a set for\\n\\nselecting sentences based on their features. Fuzzy set proposed by Zadeh [10] is a mathematical\\n\\ntool for dealing with uncertainty, vagueness and ambiguity. Its application in text representation\\n\\nfor information retrieval was first proposed by Buell [11], in which a document can be\\n\\nrepresented as a fuzzy set of terms. Miyamoto [12] investigated applications of fuzzy set theory\\n\\nin information retrieval and cluster analysis. Witte and Bergler [13] presented a fuzzy-theory\\n\\nJilid 20, Bit 2 (Disember 2008) Jumal Teknologi Maklumat\\n\\n\\n\\n107 \\n\\nI \\n\\n, based approach to co-reference resolution and its application to text summarization. Automatic \\n\\ndetermination of co-reference between noun phrases is fraught with uncertainty. Kiani and \\n\\nAkbarzadeh [15] proposed technique for summarizing text using combination of Genetic \\n\\nAlgorithm (GA) and Genetic Programming (GP) to optimize rule sets and membership function \\n\\n; offuzzy systems. \\n\\nThe feature extraction techniques are used to locate the important sentences in the text. \\n\\n•For instance, Luhn looked at the frequency of word distributions as frequent words should \\n\\nindicate the most important concepts of the document. Some of features are used in this \\n\\nresearch such as sentence length. Some sentences are short or some sentences are long. What is \\n\\n.\\'� clear is that some of the attributes have more importance and some have less and so they should \\n\\nhave balance weight in computations and we use fuzzy logic to solve this problem by defining \\n\\nthe membership functions for each feature. \\n\\n3. EXPERIMENT \\n\\n3.1 Data Set \\n\\nWe used 6 documents from DUC2002. Each document consists of 16 to 56 sentences with an \\n\\n, average of 31 sentences. The DUC2002 collection provided [10]. Each document in DUC2002 \\n\\ncollection is supplied with a set of human-generation summaries provided by two different \\n\\nexperts. While each expert was asked to generate summaries of different length, we use only \\n\\nloo-word variants. DUC2002 for automatic single-document summarization create a generic \\n\\nCurrently, input document are of plain text format. In this paper, we use Microsoft Visual C# \\n\\n2008 for preprocessing data. There are four main activities performed in this stage: Sentence \\n\\nSegmentation, Tokenization, Removing Stop Word, and Stemming Word. Sentence \\n\\n,segmentation is boundary detection and separating source text into sentence. Tokenization is \\n\\n,separating the input document into individual words. Next, Removing Stop Words, stop words \\n\\nare the words which appear frequently in document but provide less meaning in identifying the \\n\\nimportant content of the document such as \\'a\\', \\'an\\', \\'the\\', etc.. The last step for preprocessing is \\n\\n,Stemming word; Stemming word is the process ofremoving prefixes and suffixes of each word. \\n\\n:Jilid 20, Bil. 2 (Disember 2008)� Jurnal Teknologi Maklumat \\n\\n107\\n\\nbased approach to co-reference resolution and its application to text summarization. Automatic\\n\\ndetermination of co-reference between noun phrases is fraught with uncertainty. Kiani and\\n\\nAkbarzadeh [15] proposed technique for summarizing text using combination of Genetic\\n\\nAlgorithm (GA) and Genetic Programming (GP) to optimize rule sets and membership function\\n\\nof fuzzy systems.\\n\\nThe feature extraction techniques are used to locate the important sentences in the text.\\n\\nFor instance, Luhn looked at the frequency of word distributions as frequent words should\\n\\nindicate the most important concepts of the document. Some of features are used in this\\n\\nresearch such as sentence length. Some sentences are short or some sentences are long. What is\\n\\nclear is that some of the attributes have more importance and some have less and so they should\\n\\nhave balance weight in computations and we use fuzzy logic to solve this problem by defining\\n\\nthe membership functions for each feature.\\n\\n3. EXPERIMENT\\n\\n3.1 Data Set\\n\\nWe used 6 documents from DUC2002. Each document consists of 16 to 56 sentences with an\\n\\naverage of 31 sentences. The DUC2002 collection provided [10]. Each document in DUC2002\\n\\ncollection is supplied with a set of human-generation summaries provided by two different\\n\\n.experts. While each expert was asked to generate summaries of different length, we use only\\n\\nloo-word variants. DUC2002 for automatic single-document summarization create a generic\\n\\nloo-word summary.\\n\\n3.2 Preprocessing\\n\\n~ Currently, input document are of plain text format. In this paper, we use Microsoft Visual C#\\n!\\n\\ni2008 for preprocessing data. There are four main activities performed in this stage: Sentence,\\ntSegmentation, Tokenization, Removing Stop Word, and Stemming Word. Sentence\\n\\nisegmentation is boundary detection and separating source text into sentence. Tokenization is\\n.separating the input document into individual words. Next, Removing Stop Words, stop words\\n\\nare the words which appear frequently in document but provide less meaning in identifying the\\n\\n:important content of the document such as \\'a\\', \\'an\\', \\'the\\', etc.. The last step for preprocessing is\\n\\nStemming word; Stemming word is the process of removing prefixes and suffixes of each word.\\n\\n\\'Jilid 20, Bil. 2 (Disember 2008) Jurnal Teknologi Maklumat\\n\\n\\n\\n108 \\n\\n3.3 Features in Text Summarization \\n\\nIn order to use a statistical method it is necessary to represent the sentences as vectors of \\n\\nfeatures. These features are attributes that attempt to represent the data used for the task. We \\n\\nconcentrate our presentation in eight features for each sentence. Each feature is given a value \\n\\nbetween \\'0\\' and \\'I\\'. Therefore, we can extract the appropriate number of sentences according \\n\\nto compression rate. There are eight features as follows: \\n\\n(I) Title feature: The number of title word in sentence, words in sentence that also occur in \\n\\ntitle gives high score [6]. This is detennined by counting the number of matches between the \\n\\ncontent words in a sentence and the words in the title. We calculate the score for this feature \\n\\nwhich is the ratio of the number of words in sentence that occur in the title over the number of \\n\\nword in title. \\n\\nScore (SJ = No. Title word in S1 (I) \\nNo. Word in Title \\n\\n(2) Sentence length: The number of word in sentence, this feature is useful to filtering out \\n\\nshort sentences such as datelines and author names commonly found in news articles. The short \\n\\nsentences are not expected to belong to the summary [5]. We use nonnalized length of the \\n\\nsentence, which is the ratio of the number of words occurring in the sentence over the number \\n\\nof words occurring in the longest sentence of the document. \\n\\nScore (SJ = No. Word occurring in S1 (2) \\nNo. Word occurring in longest sentence \\n\\n(3) Term weight: Calculating the average of the TF-ISF (Tennfrequency, Inverse sentence \\n\\nfrequency). The frequency of tenn occurrences within a document has often been used for \\n\\ncalculating the importance of sentence [7]. \\n\\nScore (SJ = Sum ofTF-ISF in St (3) \\nMax(Sum ofTF-ISF) \\n\\n(4) Sentence position: Whether it is the first and last Sentence in the paragraph, sentence \\n\\nposition in text gives the importance of the sentences. This feature can involve several items \\n\\nJilid 20, Bit. 2 (Disember 2008)� Jurnal Teknologi Maklumat \\n\\nsuch as the position of \\n\\nand last sentence highe \\n\\nother sentence. \\n\\nScore (SJ = 1fi \\nOJ \\n\\n(5) Sentence to sentel \\n\\nsimilarity between s an \\n\\nscore of this feature f( \\n\\nsentence similarity of Sl \\n\\nScore (SJ� = Sun \\nMax( \\n\\n(6) Proper noun: The \\n\\n(proper noun). Usually \\n\\nis most probably incl \\n\\ncalculated as the ratio c \\n\\nScore (SJ� = No \\nj \\n\\n(7) Thematic word: \\n\\nbecause terms that OCCI \\n\\nthematic words indicat \\n\\nfrequent content word \\n\\nthe ratio of the number \\n\\nScore (SJ� = Iv \\n\\n(8) Numerical data: \\n\\nnumerical data is impc \\n\\nscore for this feature i~ \\n\\nthe sentence length \\n\\nJilid 20, Bil. 2 (Disembe~ \\n\\n108\\n\\n3.3 Features in Text Summarization\\n\\nIn order to use a statistical method it is necessary to represent the sentences as vectors of\\n\\nfeatures. These features are attributes that attempt to represent the data used for the task. We\\n\\nconcentrate our presentation in eight features for each sentence. Each feature is given a value\\n\\nbetween \\'0\\' and \\'1\\'. Therefore, we can extract the appropriate number of sentences according\\n\\nto compression rate. There are eight features as follows:\\n\\n(I) Title feature: The number of title word in sentence, words in sentence that also occur in\\n\\ntitle gives high score [6]. This is determined by counting the number of matches between the\\n\\ncontent words in a sentence and the words in the title. We calculate the score for this feature\\n\\nwhich is the ratio of the number of words in sentence that occur in the title over the number of\\n\\nword in title.\\n\\nScore (SJ = No. Title word in S1\\nNo. Word in Title\\n\\n(I)\\n\\n(2) Sentence length: The number of word in sentence, this feature is useful to filtering out\\n\\nshort sentences such as datelines and author names commonly found in news articles. The short\\n\\nsentences are not expected to belong to the summary [5]. We use normalized length of the\\n\\nsentence, which is the ratio of the number of words occurring in the sentence over the number\\n\\nof words occurring in the longest sentence of the document.\\n\\nScore (SJ = No. Word occurring in SI\\nNo. Word occurring in longest sentence\\n\\n(2)\\n\\n(3) Term weight: Calculating the average of the TF-ISF (Term frequency, Inverse sentence\\n\\nfrequency). The frequency of term occurrences within a document has often been used for\\n\\ncalculating the importance of sentence [7].\\n\\nScore (SJ = Sum ofTF-ISF in Sf\\nMax(Sum ofTF-ISF)\\n\\n(3)\\n\\n(4) Sentence position: Whether it is the first and last sentence in the paragraph, sentence\\n\\nposition in text gives the importance of the sentences. This feature can involve several items\\n\\nJilid 20, Bil. 2 (Disember 2008) Jurnal Teknologi Maklumat\\n\\n\\n\\ne \\n\\ne \\n\\ng \\n\\nn \\n\\n)f \\n\\n\" \\n109 \\n\\nsuch as the position of a sentence in the document, section, paragraph, etc., [14] proposed first \\n\\nand last sentence highest ranking. The score for this feature: 1 for first and last sentence, 0 for \\n\\nother sentence. \\n\\nScore (SJ = 1 for First and Last sentence, \\nDfor other sentences (4) \\n\\n(5) Sentence. to sentence similarity: Similarity between sentences, for each sentence s, the \\n\\nsimilarity between s and each other sentence is computed by the cosine similarity measure. The \\n\\nscore of this feature for a sentence s is obtained by computing the ratio of the summary of \\n\\nsentence similarity of sentence s with each other sentence over the maximum of summary \\n\\nScore (SJ = Sum ofSentemce Similarity in S; \\nMax(Sum ofSentence Similarity) \\n\\n(5) \\n\\n(6) Proper noun: The number of proper noun in sentence, sentence inclusion of name entity \\n\\n(proper noun). Usually the sentence that contains more proper nouns is an important one and it \\n\\nis most probably included in the document summary [17]. The score for this feature is \\n\\ncalculated as the ratio of the number of proper nouns in sentence over the sentence length. \\n\\ne \\n\\nr Score (SJ = No. Proper nouns in SI \\nLength (SJ \\n\\n(6) \\n\\nr \\n\\n(7)Score (SJ = No. Thematic word in St_ \\nLength (SJ \\n\\n(7) Thematic word: The number of thematic word in sentence, this feature is important \\n\\nbecause terms that occur frequently in a document are probably related to topic. The number of \\n\\nthematic words indicates the words with maximum possible relativity. We used the top 10 most \\n\\nfrequent content word for consideration as thematic. The score for this feature is calculated as \\n\\nthe ratio of the number of thematic words in sentence over the sentence length \\n\\n(8) Numerical data: The number of numerical data in sentence, sentence that contains \\n\\nnumerical data is important and it is most probably included in the document summary [16].The \\n\\nscore for this feature is calculated as the ratio of the number of numerical data in sentence over \\n\\nthe sentence length \\n\\nJilid 20, Bil. 2 (Disember 2008) Jurnal Teknologi Maklumat \\n\\n109\\n\\nsuch as the position of a sentence in the document, section, paragraph, etc., [14] proposed first\\n\\nand last sentence highest ranking. The score for this feature: 1 for first and last sentence, 0 for\\n\\nother sentence.\\n\\nScore (SJ = 1for First and Last sentence,\\nDfor other sentences (4)\\n\\n(5) Sentence. to sentence similarity: Similarity between sentences, for each sentence s, the\\n\\nsimilarity between s and each other sentence is computed by the cosine similarity measure. The\\n\\nscore of this feature for a sentence s is obtained by computing the ratio of the summary of\\n\\nsentence similarity of sentence s with each other sentence over the maximum of summary\\n\\nScore (SJ = Sum ofSentemce Similarity in S;\\nMax(Sum ofSentence Similarity)\\n\\n(5)\\n\\n(6) Proper noun: The number of proper noun in sentence, sentence inclusion of name entity\\n\\n(proper noun). Usually the sentence that contains more proper nouns is an important one and it\\n\\nis most probably included in the document summary [17]. The score for this feature is\\n\\ncalculated as the ratio of the number of proper nouns in sentence over the sentence length.\\n\\nScore (SJ = No. Proper nouns in S!\\nLength (SJ\\n\\n(6)\\n\\n(7) Thematic word: The number of thematic word in sentence, this feature is important\\n\\nbecause terms that occur frequently in a document are probably related to topic. The number of\\n\\nthematic words indicates the words with maximum possible relativity. We used the top 10 most\\n\\nfrequent content word for consideration as thematic. The score for this feature is calculated as\\n\\nthe ratio of the number of thematic words in sentence over the sentence length\\n\\nScore (SJ = No. Thematic word in St_\\nLength (SJ\\n\\n(7)\\n\\n(8) Numerical data: The number of numerical data in sentence, sentence that contains\\n\\nnumerical data is important and it is most probably included in the document summary [1 6].The\\n\\nscore for this feature is calculated as the ratio of the number of numerical data in sentence over\\n\\nthe sentence length\\n\\nIiJid 20, BiI. 2 (Disember 2008) IurnaJ Teknologi Maklumat\\n\\n\\n\\nllO \\n\\nScore (SJ = No. Numerical data in St_ (8) \\nLength (SJ \\n\\n3.4 Text Summarization based on Fuzzy Logic \\n\\nIn order to implement text summarization based on fuzzy logic, we use MATLAB since it is \\n\\npossible to simulate fuzzy logic in this software. First, the features extracted in previous section \\n\\nare used as input to the fuzzy inference system. We used Bell membership functions. The \\n\\ngeneralized Bell membership function depends on three parameters a, b, and c as given by (9) \\n\\n1\\nf (Xi a , b,c) = IK-\\'J:b- (9) \\n\\nl+-j\\n/I • \\n\\nwhere the parameter b is usually positive. The parameter c and a, locate the center and width of \\n\\nthe curve. \\n\\nFor instance, membership function of sentence to sentence similarity is show in Figure 1. \\n\\nIeSSSupilarnv ave.~ \\n\\nI \\n- ,I \\n\\nFigure I. Membership function of sentence to sentence similarity \\n\\nAfterword, we use fuzzy logic to summarize the document. A value from zero to one is \\n\\nobtained for each sentence in the output based on sentence characteristics and the available \\n\\nrules in the knowledge base. The obtained value in the output determines the degree of the \\n\\nimportance of the sentence in the final summary. \\n\\n0-\\n\\nThe input mem1 \\n\\npIe, membership fill \\n\\nHighSimilarity}. L \\n\\nted from these rule, \\n\\nIF (NoWordIll \\nvery much) an \\nis highSimilar \\nmany) and (Nl \\n\\nALUAnON AND \\n\\nthe ROUGE, a se \\n\\nion toolkit [8] th \\n\\nof ROUGE, whic \\n\\ny, at a confidence \\n\\nwith human asses1 \\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Mak1umat \\n\\n110\\n\\nScore (SJ = No. Numerical data in St_\\nLength (SJ\\n\\n3.4 Text Summarization based on Fuzzy Logic\\n\\n(8)\\n\\nIn order to implement text summarization based on fuzzy logic, we use MATLAB since it is\\n\\npossible to simulate fuzzy logic in this software. First, the features extracted in previous section\\n\\nare used as input to the fuzzy inference system. We used Bell membership functions. The\\n\\ngeneralized Bell membership function depends on three parameters a, b, and c as given by (9)\\n\\n1f (XjQ, b,c) = .b\\nIX-\\'j\\'1+-/I\\n\\n(9)\\n\\nwhere the parameter b is usually positive. The parameter c and a. locate the center and width of\\n\\nthe curve.\\n\\nFor instance, membership function of sentence to sentence similarity is show in Figure 1.\\n\\n!essSi/hila,ny avel\\'.lge\\n\\n.·f.-\\nr\\n-~~\\\\ (\\n\\n.J\\\\~~~\\'\\n.\\' ,\\n\\nFigure 1. Membership function of sentence to sentence similarity\\n\\nAfterword, we use fuzzy logic to summarize the document. A value from zero to one is\\n\\nobtained for each sentence in the output based on sentence characteristics and the available\\n\\nrules in the knowledge base. The obtained value in the output determines the degree of the\\n\\nimportance of the sentence in the final summary.\\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Maklumat\\n\\n\\n\\nIII \\n\\nII \\n\\n1 \\n\\nimpo art \\n\\nI:\\' 01 02 (13 O~ u.s (I\\'; 0: !H; 09 \\ncdp..t Y8riable \"OuIpd\" \\n\\nFigure 2. Membership function of Output \\n\\nThe input membership function for each feature is divided into three membership \\n\\nfunctions which are composed of insignificant values, average and significant values. For \\n\\nexample, membership functions for title feature: SetenenceSimilarity {LessSimilarity, Average, \\n\\nand HighSimilarity}. Likewise, the output membership function is divided into three \\n\\nmembership functions: Output {Unimportant, Average, and Important}. The most important \\nf \\n\\npart in this procedure is the definition of fuzzy IF-THEN rules. The important sentences are \\n\\nextracted from these rules according to our features criteria. For example our rules are showed \\n\\nas follow. \\n\\nIF (No WordInTitle is many) and (SentenceLength is long) and (TermFreq is \\nvery much) and (SentencePosition is first-last position) and (SentenceSimilarity \\nis highSimilarity) and (NoProperNoun is many) and (NoThematicWord is \\nmany) and (NumbericalData is many) THEN (Sentence is important) \\n\\nFigure 3. Sample ofIF-THEN Rules \\n\\n! 4. EVALUATION AND RESULT \\n\\nWe use the ROUGE, a set of metrics called Recall-Oriented Understudy for Gisting Evaluation, \\n\\nevaluation toolkit [8] that has become standards of automatic evaluation of summaries. It \\n\\ncompares the summaries generated by the program with the human-generated (gold standard) \\n\\nsummaries. For comparison, it uses n-gram statistics. Our evaluation was done using n-gram \\n\\nsetting of ROUGE, which was found to have the highest correlation with human judgments, \\n\\nnamely, at a confidence level of 95%. It is claimed that ROUGE-I consistently correlates \\n\\nhighly with human assessments and has high recall and precision significance test with manual \\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Maklumat \\n\\nIII\\n\\n1\\n\\nitnpO artaverage\\n\\nI\\nc, ====::=::::::::=:::L:::\",~...i-\\' -=:\\':..:::-;-~-----,-----I\\n\\nc\\' 1)1 02 03 O~ u.s 0\\'3 0: IH; 09\\nrdp..t Y8riable \"Outplj\"\\n\\nFigure 2. Membership function of Output\\n\\nThe input membership function for each feature is divided into three membership\\n\\nfunctions which are composed of insignificant values, average and significant values. For\\n\\nexample, membership functions for title feature: SetenenceSimilarity {LessSimilarity, Average,\\n\\nand HighSimilarity}. Likewise, the output membership function is divided into three\\n\\nmembership functions: Output {Unimportant, Average, and Important}. The most important\\n\\npart in this procedure is the definition of fuzzy IF-THEN rules. The important sentences are\\n\\nextracted from these rules according to our features criteria. For example our rules are showed\\n\\nas follow.\\n\\nIF (No WordInTitle is many) and (SentenceLength is long) and (TermFreq is\\nvery much) and (SentencePosition is first-last position) and (SentenceSimilarity\\nis highSimilarity) and (NoProperNoun is many) and (NoThematicWord is\\nmany) and (NumbericalData is many) THEN (Sentence is important)\\n\\nFigure 3. Sample of IF-THEN Rules\\n\\n! 4. EVALUATION AND RESULT\\n\\nWe use the ROUGE, a set of metrics called Recall-Oriented Understudy for Gisting Evaluation,\\n\\nevaluation toolkit [8] that has become standards of automatic evaluation of summaries. It\\n\\ncompares the summaries generated by the program with the human-generated (gold standard)\\n\\nsummaries. For comparison, it uses n-gram statistics. Our evaluation was done using n-gram\\n\\nsetting of ROUGE, which was found to have the highest correlation with human judgments,\\n\\nnamely, at a confidence level of 95%. It is claimed that ROUGE-I consistently correlates\\n\\nhighly with human assessments and has high recall and precision significance test with manual\\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Maklumat\\n\\n\\n\\n•� \\n112 \\n\\nr--\\nevaluation results. So we choose ROUGE-I as the measurement of our experiment results. In o.n \\n\\nO.G(the table I, we compare fuzzy summarizer with baseline summarizer form DUC2002 data set \\n0.5( \\n0.4(and Microsoft Word 2007 Summarizer. \\n0.3( \\n0.2( \\nO.l( \\n0.0( \\n\\nTable I. The result of comparing Fuzzy Summarizer and other Summarizers using \\n\\nDocument set 0061 \\n\\nFu Sumarizer Baseline MS-Word Summarizer\\nDocument� \\n\\np R F P R F P R F� \\n\\nAP880911-0016 0.59223 0.60396 0.59804 0.41748 0.40952 0.41346 0.55556 0.42857 0.483 Figure 5. Rec \\nAP880912-0095 0.45484 0.48001 0.47607 0.43636 0.41379 0.42478 0.49231 0.44545 0.41 \\n\\nAP880912-0137 0.48039 0.47573 0.47805 0.46602 0.47059 0.46829 0.47525 0.47525 0.470 \\n\\nAP880915-0003 0.49038 0.48571 0.48803 0.44330 0.40952 0.42574 0.48571 0.48113 0.483 \\nI 0.70\\n\\nAP880916-0060 0.50816 0.46714 0.48095 0.32642 0.32642 0.32222 0.31148 0.33929 0.324 0.60 \\n, 0.50WSJ880912-0064 0.49524 0.51485 0.50485 0.49515 0.50495 0.50000 0.44231 0.42593 0.433 \\n\\' 0.40 \\n\\nAvera e 0.50354 0.50457 0.50433 0.43079 0.42247 0.42575 0.46044 0.43260 0.435 0.30\\\\ \\n1 0.20 \\n\\n0.10 \\nThe results are show in Table I. Baseline reaches an average precision of 0.43079, 0.00 \\n\\naverage recal1 of 0.42247 and average F-mean of 0.42575; while Microsoft Word 2007 \\n\\nsummarizer reaches an average precision 0.46044, recal1 of 0.43260 and F-mean of 0.43555. \\n\\nThe fuzzy summarizer achieves an average precision of 0.50354, recal1 of 0.50457 and F-mean \\n\\nof 0.50433. \\n\\nFigure 6. F-m \\n0.70000 \\n0.60000 \\n0.50000 \\n\\nThe results cl0.40000� \\n0.30000� _ .•. - Fuzzy better than baseline s,\\n0.20000 \\n\\n............ Baseline�0.10000 performance of the fu \\n0.00000 -Word and f-mean results. II \\n\\n~ p,\\'? ~ ,s,\\'\" !SJ &>~ \\n..rS\\' ~ ~..... ? J9 ..,p shows that the judge5 \\n\\n<;:,0,..... <;:,0, <;:,0, ~o, <;:,o,~ <;:,0,.... \\n\"-~ \\'0 \\'0 \\'0 \\'0 \\'0 \\n~v <:j,\\'O <:j,~ <:j,\\'O <:j,\\'O ....~ 0.59223, 0.60396, at \\n~ ~ ~ ~ ~~7 \\n\\nprovides strong evide \\n\\nFigure 4. Precision result under difference summarizer using Document Set 0061 \\n\\n1 \\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Maklumat Jilid 20, Bil. 2 (Disem~ \\n\\n112\\n\\nevaluation results. So we choose ROUGE-I as the measurement of our experiment results. In\\n\\nthe table I, we compare fuzzy summarizer with baseline summarizer fonn DUC2002 data set\\n\\nand Microsoft Word 2007 Summarizer.\\n\\nTable I. The result of comparing Fuzzy Summarizer and other Summarizers using\\n\\nDocument set 0061\\n\\nDocument\\nFu~ zv Sumarizer Baseline MS-Word Summarizer\\n\\np R F P R F P R F\\n\\nAP880911-0016 0.59223 0.60396 0.59804 0.41748 0.40952 0.41346 0.55556 0.42857 0.483\\n\\nAP880912-0095 0.45484 0.48001 0.47607 0.43636 0.41379 0.42478 0.49231 0.44545 0.416E\\n\\nAP880912-0137 0.48039 0.47573 0.47805 0.46602 0.47059 0.46829 0.47525 0.47525 0.470\\n\\nAP880915-0003 0.49038 0.48571 0.48803 0.44330 0.40952 0.42574 0.48571 0.48113 0.483\\n\\nAP880916-0060 0.50816 0.46714 0.48095 0.32642 0.32642 0.32222 0.31148 0.33929 0.324\\n\\nWSJ880912-0064 0.49524 0.51485 0.50485 0.49515 0.50495 0.50000 0.44231 0.42593 0.433\\n\\nAverage 0.50354 0.50457 0.50433 0.43079 0.42247 0.42575 0.46044 0.43260 0.435\\n\\nThe results are show in Table I. Baseline reaches an average precision of 0.43079,\\n\\naverage recall of 0.42247 and average F-mean of 0.42575; while Microsoft Word 2007\\n\\nsummarizer reaches an average precision 0.46044, recall of 0.43260 and F-mean of 0.43555.\\n\\nThe fuzzy summarizer achieves an average precision of 0.50354, recall of 0.50457 and F-mean\\n\\nof 0.50433.\\n\\nI\\nI\\n\\n0.70000\\n\\ni\\nI\\n\\n0.60000 \\'\"\\nI 0.50000\\n\\n~\\n. ·r·:..···· .......... ..\\'\\n\\ni 0.40000 !l......... ..... ........\\nI 0.30000 _ .•. - Fuzzy\\n\\nI\\n0.20000\\n0.10000 ............ Baseline\\n\\nI 0.00000 -Word\\nI ~ p,\"> ~ #\\'\" IS> ~I>..<S> ~ ~\\'\" {> ~,;\\'::J\\nI <::10,-\" <::10, <::10, <::10, <::Io,~ <;)0,-\"~ \\'0 ~ \\'0 \\'0 ~\\nI 9,\\'0 9.\\'0 9.\\'0 9,\\'0 9.\\'0 b-\\'O\\nI ,.,.,.,. ,.~\\nI !\\n\\nFigure 4. Precision result under difference summarizer using Document Set 0061\\n\\n..\\nJilid 20, Bit. 2 (Disember 2008) Jumal Teknologi Maklumat\\n\\n\\n\\n113 \\n\\nr----------------- ---------------------------------------------------\\n\\nI 0.70000 ------- --------.- -\\n------J \\n\\nI 0.60000 ... I \\n\\nIm~g:~ I i \\n0.20000 \\n0.10000 \\n\\n............. Baseline�0.00000 >----. \\n\\n-Word \\n\\nFigure 5. Recall result under difference summarizer using Document Set 0061 . \\n\\n0.70000 -\\n0.60000 ---,--\\n\\ng:~~~~~ -~-~~:?--\\n0.20000 Fuzzy \\n0.10000 \\n\\n-- Baseline\\n0.00000 \\n\\n--Word \\n\\nFigure 6. F-mean result under difference summarizer using Document Set 0061 \\n\\nThe results clearly show that fuzzy summarizer approach under consideration perform \\n\\nbetter than baseline summarizer and Microsoft Word 2007 summarizer. We further compare the \\n\\nperformance of the fuzzy summarizer and other summarizer by examining their precision, recall \\n\\nand f-mean results. In this case, the best precision, recall and f-mean from Figure 4, 5, and 6 \\n\\nshows that the judges from fuzzy summarizer are the highest score. The score are as followed: \\n\\n0.59223, 0.60396, and 0.59804. The significant performance improvement over fuzzy logic \\n\\nprovides strong evidence of its feasibility in text summarization applications \\n\\nJilid 20, Bil. 2 (Disember 2008) lurnal Teknologi Maklumat \\n\\n............. Baseline\\n\\n- . • . - FUllY\\n\\nr--- -\"------------- ---------------------------- --\"--------\"---\"------ ---\\nI 0.70000 -- ------- -\\nI 0.60000 ...\\nIg;~~···~···\\n\\n0.20000\\n0.10000 -.\"\\n0.00000\\n\\n\"----J\\n\\nI\\nI\\ni\\n\\n113\\n\\n-Word\\n\\nFigure 5. Recall result under difference summarizer using Document Set 0061 .\\n\\n0.70000 -----\\n0.60000 .--,--\\n\\n~:~~ggg -~~V-\"\\n0.20000\\n0.10000\\n0.00000\\n\\nFuzzy\\n\\n-- Basclirlc\\n\\n---.-- Word\\n\\nFigure 6. F-mean result under difference summarizer using Document Set 0061\\n\\nThe results clearly show that fuzzy summarizer approach under consideration perform\\n\\nbetter than baseline summarizer and Microsoft Word 2007 summarizer. We further compare the\\n\\nperformance of the fuzzy summarizer and other summarizer by examining their precision, recall\\n\\nand f-mean results. In this case, the best precision, recall and f-mean from Figure 4, 5, and 6\\n\\nshows that the judges from fuzzy summarizer are the highest score. The score are as followed:\\n\\n0.59223, 0.60396, and 0.59804. The significant performance improvement over fuzzy logic\\n\\nprovides strong evidence of its feasibility in text summarization applications\\n\\nli1id 20, Bil. 2 (Disember 2008) lurnal Teknologi Maklumat\\n\\n\\n\\n., \\n114 \\n\\n5. CONCLUSION AND FUTURE WORK [10] L. Zadeh., \"Fuz \\n\\n[11] D. Buell., \"An \\n\\nIn this paper, we propose automatic text summarization for important sentence extraction with systems,\" FUZZ) \\n\\nimportant features based on fuzzy logic; title feature, sentence length, term weight, sentence [12] S. Miyamoto., \\n\\nposition, sentence to sentence similarity, proper noun, thematic word and numerical data. We Academic Publi \\n\\nchoose 6 documents from DUC2002 data set and compare our summarizer with the baseline [13] R. Witte and \\nsummarizer and Microsoft Word 2007 summarizers. The results show that the judge gave a Proceedings of \\n\\nbetter average precision, recall and f-mean to summaries produced by fuzzy method. Our Applications to \\n\\nmethod is intent to be used for single document summarization as well as multi documents UniversitA Ca\\' I \\n\\nsummarization. We conclude that we need to extend the proposed method for multi document [14] Louisa Ferrier., \\nsummarization and combine fuzzy logic and other learning methods in a large data set. Artificial Intellij \\n\\n[15] Arman Kiani a: \\n\\nREFERENCES \\nFuzzy GA-GP; \\n\\nSystems, Shera \\n\\n983.2006. \\n\\n\\'/\\' . \\n\\n[1] H. P. Luhn., \"The Automatic Creation of Literature Abstracts,\" IBM Journal of Research \\n\\nand Development, vol. 2, pp.159-165. 1958. \\n[16] c.Y. Lin., \"Tra \\n\\ninternational cc \\n[2] G. 1. Rath, A. Resnick, and T. R. Savage., \"The formation of abstracts by the selection of \\n\\nMissouri, Unitel \\n\\n[3] \\n\\nsentences,\" American Documentation, vol. 12, pp.139- 143.1961. \\n\\nInderjeet Mani and Mark T. Maybury, editors., \"Advances \\n\\nsummarization,\" MIT Press. 1999. \\n\\nin automatic text \\n[17] 1. Kupiec. , 1. \\n\\nProceedings of \\n\\nDevelopment in \\n[4] H. P. Edmundson., \"New methods in automatic extracting,\" Journal of the Association \\n\\nfor Computing Machinery 16 (2). pp.264-285.l969. \\n\\n[5] S. D. Afantenos, V. Karkaletsis and P. Stamatopoulos., \"Summarization from Medical \\n\\nDocuments: A Survey,\" Artificial Intelligence in Medicine, vol. 33, pp.157-177. 2005. \\n\\n[6] G. Salton, C. Buckley., \"Term-weighting approaches in automatic text retrieval,\" \\n\\nInformation Processing and Management 24, 1988.513-523. Reprinted in: Sparck-Jones, \\n\\nK.; Willet, P. (eds.) Readings in I.Retrieval. Morgan Kaufmann. pp.323-328.1997. \\n\\n[7] G. Salton., \"Automatic Text Processing: The Transformation, Analysis, and Retrieval of \\n\\nInformation by Computer,\" Addison-Wesley Publishing CoJnpany. 1989. \\n\\n[8] C.Y. Lin., \"ROUGE: A Package for Automatic Evaluation of Summaries,\" In \\n\\nProceedings of Workshop on Text Summarization of ACL, .Spain. 2004. \\n\\n[9] DUC. Document understanding conference 2002 (2002), http://www-\\n\\nnlpir.nist.gov/projects/duc \\n\\nJilid 20, Bil. 2 (Disember 2008) \\n.. JJI..ntr \\n\\nlumal Teknologi Maklumat Jilid 20, Bil. 2 (Disembl \\n\\n114\\n\\n5. CONCLUSION AND FUTURE WORK\\n\\nIn this paper, we propose automatic text summarization for important sentence extraction with\\n\\nimportant features based on fuzzy logic; title feature, sentence length, term weight, sentence\\n\\nposition, sentence to sentence similarity, proper noun, thematic word and numerical data. We\\n\\nchoose 6 documents from DUC2002 data set and compare our summarizer with the baseline\\n\\nsummarizer and Microsoft Word 2007 summarizers. The results show that the judge gave a\\n\\nbetter average precision, recall and f-mean to summaries produced by fuzzy method. Our\\n\\nmethod is intent to be used for single document summarization as well as multi documents\\n\\nsummarization. We conclude that we need to extend the proposed method for multi document\\n\\nsummarization and combine fuzzy logic and other learning methods in a large data set.\\n\\nREFERENCES\\n\\n[l] H. P. Luhn., \"The Automatic Creation of Literature Abstracts,\" IBM Journal of Research\\n\\nand Development, vol. 2, pp.159-165. 1958.\\n\\n[2] G. 1. Rath, A. Resnick, and T. R. Savage., \"The formation of abstracts by the selection of\\n\\nsentences,\" American Documentation, vol. 12, pp.139- 143.1961.\\n\\n[3] Inderjeet Mani and Mark T. Maybury, editors., \"Advances in automatic text\\n\\nsummarization,\" MIT Press. 1999.\\n\\n[4] H. P. Edmundson., \"New methods in automatic extracting,\" Journal of the Association\\n\\nfor Computing Machinery 16 (2). pp.264-285.1969.\\n\\n[5] S. D. Afantenos, V. Karkaletsis and P. Stamatopoulos., \"Summarization from Medical\\n\\nDocuments: A Survey,\" Artificial Intelligence in Medicine, vol. 33, pp.157-177. 2005.\\n\\n[6] G. Salton, C. Buckley., \"Term-weighting approacbes in automatic text retrieval,\"\\n\\nInformation Processing and Management 24, 1988.513-523. Reprinted in: Sparck-Jones,\\n\\nK.; Willet, P. (eds.) Readings in LRetrieval. Morgan Kaufmann. pp.323-328.1997.\\n\\n[7] G. Salton., \"Automatic Text Processing: The Transformation, Analysis, and Retrieval of\\n\\nInformation by Computer,\" Addison-Wesley PublisbingCompany. 1989.\\n\\n[8] c.Y. Lin., \"ROUGE: A Package for Automatic Evaluation of Summaries,\" In\\n\\nProceedings of Workshop on Text Summarization of ACL, .Spain. 2004.\\n\\n[9] DUC. Document understanding conference 2002 (2002), http://www-\\n\\nnlpir.nist.gov/projects/duc\\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Maklumat\\n\\n\\n\\n115 \\n\\n[10]� L. Zadeh., \"Fuzzy sets. Information Control,\" vol. 8, pp.338-353.I965. \\n\\n[11]� D. Buell., \"An analysis of some fuzzy subsets application to information retrieval \\n\\nsystems,\" Fuzzy Sets and Systems, vol. 7, no. I, pp.35-42.I982. \\n\\n[12]� S. Miyamoto., \"Fuzzy Sets in Information Retrieval and Cluster Analysis,\" Kluwer \\n\\nAcademic Publishers, 1990. \\n\\n[13]� R. Witte and S. Bergler., \"Fuzzy coreference resolution for summarization,\" In \\n\\nProceedings of 2003 International Symposium on Reference Resolution and Its \\n\\nApplications to Question Answering and Summarization (ARQAS). Venice, Italy: \\n\\nUniversita Ca\\' Foscari. pp.43-50. 2003. http://rene-witte.net. \\n\\n[14]� Louisa Ferrier., \"A Maximum Entropy Approach to Text Summarization,\" School of \\n\\nArtificial Intelligence, Division of Informatics, University ofEdinburgh, 200 I. \\n\\n[15]� Arman Kiani and M.R. Akbarzadeh., \"Automatic Text Summarization Using: Hybrid \\n\\nFuzzy GA-GP,\" In Proceedings of 2006 IEEE International Conference on Fuzzy \\n\\nSystems, Sheraton Vancouver Wall Center Hotel, Vancouver, BC, Canada. pp.977-\\n\\n983.2006. \\n\\n[16]� C.Y. Lin., \"Training a selection function for extraction,\" In Proceedings of the eighth \\n\\ninternational conference on Information and knowledge management, Kansas City, \\n\\nMissouri, United States. pp.55-62. 1999. \\n\\n[17]� J. Kupiec. , J. Pedersen, and F. Chen., \"A Trainable Document Summarizer,\" In \\n\\nProceedings of the Eighteenth Annual International ACM Conference on Research and \\n\\nDevelopment in Information Retrieval (SIGIR), Seattle, WA, pp.68-73.1995. \\n\\nJilid 20, Bi!. 2 (Disember 2008)� Jurnal Teknologi Maklumat \\n\\n115\\n\\n[10] L. Zadeh., \"Fuzzy sets. Information Control,\" vol. 8, pp.338-353.1965.\\n\\n[11] D. Buell., \"An analysis of some fuzzy subsets application to information retrieval\\n\\nsystems,\" Fuzzy Sets and Systems, vol. 7, no. 1, pp.35--42.1982.\\n\\n[12] S. Miyamoto., \"Fuzzy Sets in Information Retrieval and Cluster Analysis,\" Kluwer\\n\\nAcademic Publishers, 1990.\\n\\n[13] R. Witte and S. Bergler., \"Fuzzy coreference resolution for summarization,\" In\\n\\nProceedings of 2003 International Symposium on Reference Resolution and Its\\n\\nApplications to Question Answering and Summarization (ARQAS). Venice, Italy:\\n\\nUniversita Ca\\' Foscari. pp.43-50. 2003. http://rene-witte.net.\\n\\n[14] Louisa Ferrier., \"A Maximum Entropy Approach to Text Summarization,\" School of\\n\\nArtificial Intelligence, Division of Informatics, University of Edinburgh, 2001.\\n\\n[15] Arman Kiani and M.R. Akbarzadeh., \"Automatic Text Summarization Using: Hybrid\\n\\nFuzzy GA-GP,\" In Proceedings of 2006 IEEE International Conference on Fuzzy\\n\\nSystems, Sheraton Vancouver Wall Center Hotel, Vancouver, BC, Canada. pp.977-\\n\\n983.2006.\\n\\n[16] c.Y. Lin., \"Training a selection function for extraction,\" In Proceedings of the eighth\\n\\ninternational conference on Information and knowledge management, Kansas City,\\n\\nMissouri, United States. pp.55-62. 1999.\\n\\n[17] 1. Kupiec. , J. Pedersen, and F. Chen., \"A Trainable Document Summarizer,\" In\\n\\nProceedings of the Eighteenth Annual International ACM Conference on Research and\\n\\nDevelopment in Information Retrieval (SIGIR), Seattle, WA, pp.68-73.1995.\\n\\nJilid 20, Bil. 2 (Disember 2008) Jumal Teknologi Maklumat\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df_google['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse patents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T14:54:16.488816Z",
     "start_time": "2020-12-20T14:54:16.468867Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "patenttext = ''\n",
    "patentkeys = ''\n",
    "\n",
    "df_patent = pd.DataFrame()\n",
    "\n",
    "# try:\n",
    "#     z = parse_patents(\"site:https://patents.google.com \" + query, keys_number, page_number, 'div', 'abstract')\n",
    "#     patent_text = z[0] \n",
    "    \n",
    "# #     patent_entities = get_entities(patent_text, keys_number)\n",
    "# #     patentkeys = graph_keys(patent_text, keys_number)\n",
    "    \n",
    "# #     print('|Keywords|:', patentkeys, '\\n')\n",
    "# #     print('|Entities|:', patent_entities, '\\n')\n",
    "    \n",
    "#     df_patent = z[2]\n",
    "# excepthgjfhgjfhgjfhjfhjhs:\n",
    "#     print('No data')\n",
    "    \n",
    "# df_patent.replace('', np.nan, inplace=True)\n",
    "# df_patent.dropna(inplace=True)  \n",
    "\n",
    "# winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Patents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T14:54:23.646357Z",
     "start_time": "2020-12-20T14:54:23.628405Z"
    }
   },
   "outputs": [],
   "source": [
    "df_patent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T14:54:59.297035Z",
     "start_time": "2020-12-20T14:54:26.589489Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "a = (arxivtext + patenttext) \n",
    "b = (wikitext) \n",
    "\n",
    "specific_keys = get_entities(a, keys_number)\n",
    "common_keys = get_entities(b, keys_number)\n",
    "\n",
    "specific_keys = [re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip() for i in specific_keys.split(',')]\n",
    "common_keys = [re.sub(r\" ?\\([^)]+\\)\", \"\", i).strip() for i in common_keys.split(',')]\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(specific_keys)), columns =['specific'])\n",
    "df2 = pd.DataFrame(list(zip(common_keys)), columns =['known'])\n",
    "\n",
    "matches = fpd.fuzzy_merge(df1, df2, left_on=['specific'], right_on=['known'], ignore_case=True, keep='match', \n",
    "                         method='levenshtein', threshold=0.8, join='inner')\n",
    "\n",
    "specific_reqs = list(set(specific_keys).difference(set(list(matches['specific']))))\n",
    "specific_reqs = [i for i in specific_reqs if i[0].isupper()] + [i for i in specific_reqs if i[0].islower()]\n",
    "specific_keys = ', '.join([i for i in specific_reqs if i])\n",
    "\n",
    "print('|Trend keys|:', specific_keys, '\\n')\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T14:55:08.336845Z",
     "start_time": "2020-12-20T14:55:08.309918Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_wiki.append(df_google).append(df_arxiv).append(df_patent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get text and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T14:55:43.046003Z",
     "start_time": "2020-12-20T14:55:12.011986Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "final_text = (wikitext + arxivtext + googletext + patenttext)\n",
    "final_entities = get_entities(final_text, keys_number)\n",
    "final_keys = graph_keys(' '.join(text_normalize(final_text)), keys_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define full reading time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T14:56:34.447360Z",
     "start_time": "2020-12-20T14:56:34.314717Z"
    }
   },
   "outputs": [],
   "source": [
    "full_readtime = len(nltk.sent_tokenize(final_text))*(np.median([len(i.split()) for i in nltk.sent_tokenize(final_text)])/200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best reading time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T17:54:34.008998Z",
     "start_time": "2020-12-20T17:39:06.803266Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "score_list = []\n",
    "read_list = []\n",
    "\n",
    "for i in range(1, int(full_readtime), 10):\n",
    "    \n",
    "    report_summary = get_summary(final_text, i)[1]\n",
    "    scores = scorer.score(report_summary, final_text)\n",
    "    scores = round(100*list(list(scores.values())[0])[2])\n",
    "    \n",
    "    print('Readtime:', i, 'min')\n",
    "    print('Rouge:', (str(scores) + '%'))\n",
    "    print('\\n')\n",
    "    \n",
    "    score_list.append(scores)\n",
    "    read_list.append(round((i/full_readtime),2))\n",
    "    \n",
    "    if scores > sc_lim:\n",
    "        break\n",
    "        \n",
    "print('|Recommended time|:', i, 'min') \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot calibration curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T18:01:35.550613Z",
     "start_time": "2020-12-20T18:01:34.773693Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11,4))\n",
    "axes[0].plot([i*full_readtime for i in read_list], score_list, '-o', markersize=9)\n",
    "axes[1].plot(read_list, score_list, '-o', markersize=9)\n",
    "\n",
    "axes[0].set_title('Rouge L, %')\n",
    "axes[0].set_xlabel('Readtime, min')\n",
    "\n",
    "axes[1].set_title('Rouge L,%')\n",
    "axes[1].set_xlabel('Compression')\n",
    "\n",
    "axes[0].grid()\n",
    "axes[1].grid()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter readtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:05:54.049204Z",
     "start_time": "2020-12-19T17:05:25.138Z"
    }
   },
   "outputs": [],
   "source": [
    "readtime = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:05:54.050202Z",
     "start_time": "2020-12-19T17:05:27.978Z"
    }
   },
   "outputs": [],
   "source": [
    "report_summary = get_summary(final_text, int(readtime))[1]\n",
    "scores = scorer.score(report_summary, final_text)\n",
    "scores = round(100*list(list(scores.values())[0])[2])\n",
    "\n",
    "print('Information extracted:', (str(scores) + ' %'))\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create keys with urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T17:05:54.054192Z",
     "start_time": "2020-12-19T17:05:30.010Z"
    }
   },
   "outputs": [],
   "source": [
    "url_keys = add_keyurls(final_keys, query)\n",
    "mark_keys = add_keyurls(final_entities, query)  \n",
    "\n",
    "ref_list = []\n",
    "pdf_list = []\n",
    "\n",
    "sent_list = list(report_summary.split(sep='<hr>'))[1:]\n",
    "\n",
    "for i in sent_list[:-1]:\n",
    "    try:\n",
    "        df_score = df.copy()\n",
    "        df_score['score'] = df_score['text'].apply(lambda x: css(i,x))\n",
    "        df_score = df_score.sort_values(by=['score'], ascending=False)\n",
    "        \n",
    "        if str(df_score['link'].iloc[0]):\n",
    "            pdf_list.append(str(i))\n",
    "            ref_list.append(str(df_score['link'].iloc[0]))\n",
    "    except:\n",
    "        pdf_list.append('')\n",
    "\n",
    "pdf_summary = ''.join(pdf_list)\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean sentences from tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-19T16:26:18.354Z"
    }
   },
   "outputs": [],
   "source": [
    "text_list = [BeautifulSoup(i, \"lxml\").text.replace('More', '').strip().capitalize() for i in pdf_list]\n",
    "text_list = [(i.replace(i.split()[0], '').strip().capitalize()) for i in text_list if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe from tags and urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-19T16:26:20.749Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(ref_list, text_list)), columns=['link', 'text'])\n",
    "df = df.sort_index(ascending=True).groupby('link', as_index=True).agg(lambda x: ' '.join(x))\n",
    "df = df.reindex(list(unique_everseen(ref_list))).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T12:28:26.416012Z",
     "start_time": "2020-12-17T12:28:26.405040Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_list = []\n",
    "pdf_list = []\n",
    "\n",
    "trc = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    trc = trc + 1\n",
    "    pdf_list.append(str(trc) + '. ...' + str(str(df['text'].iloc[i]) + \" <u><a href=\" + str(df['link'].iloc[i]) + \" target='_blank'>\" + \"More\" + \"</a></u>\" + \"<hr>\"))\n",
    "    ref_list.append(str(df['link'].iloc[i]))\n",
    "\n",
    "pdf_summary = ''.join(pdf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save docx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T12:28:29.614152Z",
     "start_time": "2020-12-17T12:28:29.467406Z"
    }
   },
   "outputs": [],
   "source": [
    "save_doc(pdf_summary, 'summary', query, len(df), readtime, url_keys, mark_keys, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T12:28:37.975238Z",
     "start_time": "2020-12-17T12:28:32.953988Z"
    }
   },
   "outputs": [],
   "source": [
    "memory_size = int(len(final_text.encode('utf-8'))/1024)\n",
    "docs_number = len(df)\n",
    "\n",
    "print('Memory size:', memory_size, 'mb')\n",
    "print('Number of documents:', docs_number)\n",
    "\n",
    "winsound.Beep(2500, 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
