{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T14:15:55.541566Z",
     "start_time": "2020-07-17T14:15:55.524609Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk import tokenize\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from googlesearch import search \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "\n",
    "import wikipediaapi\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import textrank\n",
    "import textract\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords \n",
    "from heapq import nlargest\n",
    "\n",
    "import urllib\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from requests import get\n",
    "from pattern.web import plaintext\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint as print\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization import mz_keywords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "import networkx as nx\n",
    "from collections import OrderedDict\n",
    "import trkeyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T14:15:58.375991Z",
     "start_time": "2020-07-17T14:15:58.371999Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T19:23:51.295293Z",
     "start_time": "2020-07-17T19:23:50.592172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T14:16:12.735647Z",
     "start_time": "2020-07-17T14:16:12.658818Z"
    }
   },
   "outputs": [],
   "source": [
    "def wiki_scrape(topic_name, verbose=True):\n",
    "    def wiki_link(link):\n",
    "        try:\n",
    "            page = wiki_api.page(link)\n",
    "            if page.exists():\n",
    "                d = {'page': link, 'text': page.text, 'link': page.fullurl,\n",
    "                     'categories': list(page.categories.keys())}\n",
    "                return d\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    wiki_api = wikipediaapi.Wikipedia(language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(topic_name)\n",
    "    if not page_name.exists():\n",
    "        print('page {} does not exist'.format(topic_name))\n",
    "        return\n",
    "    page_links = list(page_name.links.keys())\n",
    "    progress = tqdm(desc='Links Scraped', unit='', total=len(page_links)) if verbose else None\n",
    "    sources = [{'page': topic_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
    "                'categories': list(page_name.categories.keys())}]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_link = {executor.submit(wiki_link, link): link for link in page_links}\n",
    "        for future in concurrent.futures.as_completed(future_link):\n",
    "            data = future.result()\n",
    "            progress.update(1) if verbose else None\n",
    "            if data:\n",
    "                sources.append(data)\n",
    "    progress.close() if verbose else None\n",
    "    blacklist = ('Template', 'Help:', 'Category:', 'Portal:', 'Wikipedia:', 'Talk:')\n",
    "    sources = pd.DataFrame(sources)\n",
    "    sources = sources[(len(sources['text']) > 20)\n",
    "                      & ~(sources['page'].str.startswith(blacklist))]\n",
    "    sources['categories'] = sources.categories.apply(lambda x: [y[9:] for y in x])\n",
    "    sources['topic'] = topic_name\n",
    "    return sources\n",
    "\n",
    "def wiki_page(page_name):\n",
    "    wiki_api = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(page_name)\n",
    "    if not page_name.exists():\n",
    "        print('page does not exist')\n",
    "        return\n",
    "    page_data = {'page': page_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
    "                 'categories': [[y[9:] for y in list(page_name.categories.keys())]]}\n",
    "    page_data_df = pd.DataFrame(page_data)\n",
    "    return page_data_df\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(round(value,1)))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight\n",
    "        \n",
    "def text_summarizer(raw_docx):\n",
    "    \n",
    "    raw_text = raw_docx\n",
    "    docx = nlp(raw_text)\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    word_frequencies = {}  \n",
    "    for word in tqdm(docx):  \n",
    "        if word.text not in stopwords:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in tqdm(word_frequencies.keys()):  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "    sentence_list = [ sentence for sentence in docx.sents ]\n",
    "\n",
    "    # Calculate Sentence Score and Ranking\n",
    "    sentence_scores = {}  \n",
    "    for sent in tqdm(sentence_list):  \n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "    summary_sentences = nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "    final_sentences = [ w.text for w in summary_sentences ]\n",
    "    summary = ' '.join(final_sentences)\n",
    "\n",
    "    print(summary)\n",
    "    \n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T11:19:11.206014Z",
     "start_time": "2020-07-15T11:19:11.192055Z"
    }
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def parse_page(url):\n",
    "    htmlString = get(url).text\n",
    "\n",
    "    soup = BeautifulSoup(htmlString, 'html.parser')\n",
    "    paragraphs = soup.find_all(['p', 'article', 'section', 'title', 'h1', 'h2', 'h3'])\n",
    "    txt = text_from_html(str(paragraphs))\n",
    "\n",
    "    title = soup.find('title')\n",
    "    title_str = text_normalize(str(title.string).split('|', 1)[0])\n",
    "\n",
    "    keywords = ','.join([text_from_html(str(i)) for i in soup.find_all('a') if '/tagged' in str(i)]) \n",
    "    return keywords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T11:19:16.638724Z",
     "start_time": "2020-07-15T11:19:16.556712Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_list = pd.read_excel(f'datasets/urls.xlsx', sheet_name='Sheet1')\n",
    "# pages_list = list(pages_list['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from web pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T11:19:19.712279Z",
     "start_time": "2020-07-15T11:19:19.334293Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_list = []\n",
    "web_list = []\n",
    "\n",
    "for i in tqdm(pages_list):\n",
    "    keys_list.append(parse_page(i))\n",
    "    web_list.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T19:47:21.552186Z",
     "start_time": "2020-07-14T19:47:21.420536Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(web_list, keys_list)), columns=['url','keys'])\n",
    "df.to_excel (r'keys.xlsx', index = False, header=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web page extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docxtxt = textract.process('docs/test.docx').decode('utf-8')\n",
    "# docxtxt = re.sub('[^A-Za-z0-9]+', ' ', docxtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from Google Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T16:30:30.928525Z",
     "start_time": "2020-07-03T16:30:30.923540Z"
    }
   },
   "outputs": [],
   "source": [
    "# txt = []\n",
    "\n",
    "# exc_number = 0\n",
    "# agg_number = 0\n",
    "\n",
    "# query = 'DeepFaceDrawing'\n",
    "# url_list = list(search(query, num=100, stop=100, pause=1))\n",
    "\n",
    "# for j in tqdm(url_list):\n",
    "    \n",
    "#     agg_number += 1   \n",
    "#     if agg_number==11:\n",
    "#         break\n",
    "    \n",
    "#     try: \n",
    "#         delta = random.randint(1,8)\n",
    "#         time.sleep(delta) \n",
    "#         html = urllib.request.urlopen(j).read()\n",
    "#         txt.append(text_from_html(html))\n",
    "#     except:\n",
    "#         agg_number -= 1\n",
    "#         exc_number += 1\n",
    "        \n",
    "# summ = ''.join(txt)\n",
    "# webText = re.sub('[^A-Za-z0-9.]+', ' ', summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging - DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:22:27.667828Z",
     "start_time": "2020-07-16T15:22:27.658855Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import treebank\n",
    "import random\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:22:35.028158Z",
     "start_time": "2020-07-16T15:22:30.628914Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "tagged_sentences = treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "print('Tagged sentences: ', len(tagged_sentences)) \n",
    "print('Tagged words:', len(nltk.corpus.treebank.tagged_words())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show random sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:22:41.305382Z",
     "start_time": "2020-07-16T15:22:41.296406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Among', 'ADP'), ('its', 'PRON'), ('new', 'ADJ'), ('customers', 'NOUN'), (':', '.'), ('day-care', 'ADJ'), ('centers', 'NOUN'), ('that', 'DET'), ('*T*-1', 'X'), ('previously', 'ADV'), ('spurned', 'VERB'), ('the', 'DET'), ('service', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(tagged_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:22:45.062360Z",
     "start_time": "2020-07-16T15:22:45.042397Z"
    }
   },
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "def pos_tag(sentence): \n",
    "    tags = classifier.predict([features(sentence, index) for index in range(len(sentence))]) \n",
    "    return tags\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):  \n",
    "    X, y = [], []      \n",
    "    for tagged in tagged_sentences:         \n",
    "        for index in range(len(tagged)):    \n",
    "            X.append(features(untag(tagged), index))              \n",
    "            y.append(tagged[index][1])       \n",
    "    return X, y\n",
    "\n",
    "def features(sentence_terms, index):\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:23:10.963350Z",
     "start_time": "2020-07-16T15:23:08.878691Z"
    }
   },
   "outputs": [],
   "source": [
    "part=int(.75 * len(tagged_sentences)) \n",
    "training_sentences = tagged_sentences[:part] \n",
    "test_sentences = tagged_sentences[part:]        \n",
    "X, y = transform_to_dataset(training_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T09:46:55.726279Z",
     "start_time": "2020-07-16T09:46:53.379071Z"
    }
   },
   "source": [
    "Vectorize datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:23:19.240137Z",
     "start_time": "2020-07-16T15:23:16.487355Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "X, y = transform_to_dataset(training_sentences)\n",
    "z = dict_vectorizer.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:25:23.472773Z",
     "start_time": "2020-07-16T15:24:50.619785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.5s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  32.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='entropy',\n",
       "                                        max_depth=None, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic  = DecisionTreeClassifier(criterion='entropy')\n",
    "model = Pipeline([('vectorizer', DictVectorizer(sparse=False)), ('classifier', basic)], verbose=True)  \n",
    "model.fit(X[:10000], y[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score pipeline in test part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:27:27.101372Z",
     "start_time": "2020-07-16T15:27:20.575599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    "print('Accuracy:', round(model.score(X_test, y_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score pipeline in cross validaion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:28:19.350733Z",
     "start_time": "2020-07-16T15:27:31.218368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.5s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.2s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.4s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.2s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.3s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.4s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.7s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.3s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   4.4s\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=   0.2s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   5.8s\n",
      "Mean accuracy: 0.89\n",
      "Accuracy dispersion: 0.01\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X[:5000], y[:5000], cv=kfold)\n",
    "print('Mean accuracy:', round(results.mean(),2))\n",
    "print('Accuracy dispersion:', round(results.std(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:28:32.096670Z",
     "start_time": "2020-07-16T15:28:32.077721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON' 'VERB' 'ADJ' 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(word_tokenize('I left my religion')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging - CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries and sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T14:16:17.883865Z",
     "start_time": "2020-07-17T14:16:17.798084Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.util import untag\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    " \n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T19:02:10.513250Z",
     "start_time": "2020-07-17T19:02:10.496293Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_tag(sentence):\n",
    "    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n",
    "    return list(zip(sentence, model.predict([sentence_features])[0]))\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:43:48.377276Z",
     "start_time": "2020-07-16T15:43:43.551172Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    " \n",
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:49:59.225682Z",
     "start_time": "2020-07-16T15:48:33.849279Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CRF()\n",
    "z = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T16:03:16.798798Z",
     "start_time": "2020-07-16T16:03:16.361998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', round(metrics.flat_accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T15:46:29.630348Z",
     "start_time": "2020-07-16T15:46:29.609375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('Bob', 'NNP'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = ['I', 'am', 'Bob','!']\n",
    "print(pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:00.772671Z",
     "start_time": "2020-07-17T17:25:00.719705Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from keras import backend as K\n",
    " \n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:26:07.679478Z",
     "start_time": "2020-07-17T17:26:07.672499Z"
    }
   },
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences\n",
    "\n",
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:07.817158Z",
     "start_time": "2020-07-17T17:25:04.390214Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentences, sentence_tags =[], [] \n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:23.071287Z",
     "start_time": "2020-07-17T17:25:20.761461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['Lorillard', 'Inc.', ',', 'the', 'unit', 'of', 'New', 'York-based',\n",
      "       'Loews', 'Corp.', 'that', '*T*-2', 'makes', 'Kent', 'cigarettes',\n",
      "       ',', 'stopped', 'using', 'crocidolite', 'in', 'its', 'Micronite',\n",
      "       'cigarette', 'filters', 'in', '1956', '.'], dtype='<U11')\n",
      "array(['NNP', 'NNP', ',', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NNP', 'NNP',\n",
      "       'WDT', '-NONE-', 'VBZ', 'NNP', 'NNS', ',', 'VBD', 'VBG', 'NN',\n",
      "       'IN', 'PRP$', 'NN', 'NN', 'NNS', 'IN', 'CD', '.'], dtype='<U6')\n"
     ]
    }
   ],
   "source": [
    "sentences, sentence_tags =[], [] \n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    " \n",
    "# Let's see how a sequence looks\n",
    " \n",
    "print(sentences[5])\n",
    "print(sentence_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:28.596521Z",
     "start_time": "2020-07-17T17:25:28.576575Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:32.453316Z",
     "start_time": "2020-07-17T17:25:32.307603Z"
    }
   },
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    " \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    " \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:36.986103Z",
     "start_time": "2020-07-17T17:25:36.800594Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    " \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    " \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    "    \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:41.578933Z",
     "start_time": "2020-07-17T17:25:41.485111Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:25:48.076458Z",
     "start_time": "2020-07-17T17:25:47.211770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 271, 128)          1306496   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 271, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 271, 47)           24111     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 2,119,087\n",
      "Trainable params: 2,119,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy', ignore_class_accuracy(0)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T17:26:19.926755Z",
     "start_time": "2020-07-17T17:26:16.683413Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T18:52:42.042452Z",
     "start_time": "2020-07-17T17:26:22.927724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 128s 51ms/step - loss: 1.2172 - accuracy: 0.8577 - ignore_accuracy: 0.0172 - val_loss: 0.3565 - val_accuracy: 0.9078 - val_ignore_accuracy: 0.1176\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 118s 47ms/step - loss: 0.3307 - accuracy: 0.9070 - ignore_accuracy: 0.0651 - val_loss: 0.3185 - val_accuracy: 0.9049 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 119s 47ms/step - loss: 0.3137 - accuracy: 0.9105 - ignore_accuracy: 0.1217 - val_loss: 0.3070 - val_accuracy: 0.9172 - val_ignore_accuracy: 0.1373\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 119s 48ms/step - loss: 0.3038 - accuracy: 0.9165 - ignore_accuracy: 0.1337 - val_loss: 0.2993 - val_accuracy: 0.9175 - val_ignore_accuracy: 0.1391\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 118s 47ms/step - loss: 0.2959 - accuracy: 0.9169 - ignore_accuracy: 0.1353 - val_loss: 0.2922 - val_accuracy: 0.9176 - val_ignore_accuracy: 0.1395\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 120s 48ms/step - loss: 0.2886 - accuracy: 0.9171 - ignore_accuracy: 0.1364 - val_loss: 0.2834 - val_accuracy: 0.9178 - val_ignore_accuracy: 0.1419\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 118s 47ms/step - loss: 0.2810 - accuracy: 0.9186 - ignore_accuracy: 0.1507 - val_loss: 0.2783 - val_accuracy: 0.9209 - val_ignore_accuracy: 0.1702\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 117s 47ms/step - loss: 0.2764 - accuracy: 0.9205 - ignore_accuracy: 0.1673 - val_loss: 0.2737 - val_accuracy: 0.9211 - val_ignore_accuracy: 0.1720\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 118s 47ms/step - loss: 0.2721 - accuracy: 0.9221 - ignore_accuracy: 0.1848 - val_loss: 0.2698 - val_accuracy: 0.9272 - val_ignore_accuracy: 0.2361\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 118s 47ms/step - loss: 0.2678 - accuracy: 0.9270 - ignore_accuracy: 0.2359 - val_loss: 0.2651 - val_accuracy: 0.9315 - val_ignore_accuracy: 0.2811\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 119s 47ms/step - loss: 0.2619 - accuracy: 0.9327 - ignore_accuracy: 0.2963 - val_loss: 0.2581 - val_accuracy: 0.9389 - val_ignore_accuracy: 0.3598\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 118s 47ms/step - loss: 0.2531 - accuracy: 0.9387 - ignore_accuracy: 0.3588 - val_loss: 0.2474 - val_accuracy: 0.9411 - val_ignore_accuracy: 0.3826\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 125s 50ms/step - loss: 0.2398 - accuracy: 0.9419 - ignore_accuracy: 0.3920 - val_loss: 0.2323 - val_accuracy: 0.9422 - val_ignore_accuracy: 0.3951\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 146s 58ms/step - loss: 0.2226 - accuracy: 0.9448 - ignore_accuracy: 0.4229 - val_loss: 0.2139 - val_accuracy: 0.9466 - val_ignore_accuracy: 0.4414\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 147s 59ms/step - loss: 0.2040 - accuracy: 0.9490 - ignore_accuracy: 0.4674 - val_loss: 0.1963 - val_accuracy: 0.9505 - val_ignore_accuracy: 0.4833\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 151s 60ms/step - loss: 0.1849 - accuracy: 0.9525 - ignore_accuracy: 0.5040 - val_loss: 0.1776 - val_accuracy: 0.9544 - val_ignore_accuracy: 0.5231\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 137s 55ms/step - loss: 0.1655 - accuracy: 0.9566 - ignore_accuracy: 0.5472 - val_loss: 0.1594 - val_accuracy: 0.9569 - val_ignore_accuracy: 0.5488\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 126s 50ms/step - loss: 0.1465 - accuracy: 0.9609 - ignore_accuracy: 0.5927 - val_loss: 0.1423 - val_accuracy: 0.9620 - val_ignore_accuracy: 0.6025\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 125s 50ms/step - loss: 0.1902 - accuracy: 0.9441 - ignore_accuracy: 0.6217 - val_loss: 0.1594 - val_accuracy: 0.9591 - val_ignore_accuracy: 0.6248\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 125s 50ms/step - loss: 0.1453 - accuracy: 0.9619 - ignore_accuracy: 0.6737 - val_loss: 0.1359 - val_accuracy: 0.9658 - val_ignore_accuracy: 0.6493\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 127s 51ms/step - loss: 0.1209 - accuracy: 0.9704 - ignore_accuracy: 0.6950 - val_loss: 0.1180 - val_accuracy: 0.9717 - val_ignore_accuracy: 0.7059\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 124s 49ms/step - loss: 0.1040 - accuracy: 0.9763 - ignore_accuracy: 0.7535 - val_loss: 0.1057 - val_accuracy: 0.9765 - val_ignore_accuracy: 0.7554\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 125s 50ms/step - loss: 0.0914 - accuracy: 0.9808 - ignore_accuracy: 0.8006 - val_loss: 0.0957 - val_accuracy: 0.9796 - val_ignore_accuracy: 0.7875\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 124s 50ms/step - loss: 0.0806 - accuracy: 0.9840 - ignore_accuracy: 0.8333 - val_loss: 0.0868 - val_accuracy: 0.9817 - val_ignore_accuracy: 0.8097\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 124s 50ms/step - loss: 0.0711 - accuracy: 0.9865 - ignore_accuracy: 0.8595 - val_loss: 0.0790 - val_accuracy: 0.9834 - val_ignore_accuracy: 0.8268\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 124s 50ms/step - loss: 0.0628 - accuracy: 0.9886 - ignore_accuracy: 0.8805 - val_loss: 0.0724 - val_accuracy: 0.9845 - val_ignore_accuracy: 0.8388\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 125s 50ms/step - loss: 0.0555 - accuracy: 0.9901 - ignore_accuracy: 0.8966 - val_loss: 0.0667 - val_accuracy: 0.9857 - val_ignore_accuracy: 0.8511\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 142s 57ms/step - loss: 0.0492 - accuracy: 0.9914 - ignore_accuracy: 0.9101 - val_loss: 0.0618 - val_accuracy: 0.9867 - val_ignore_accuracy: 0.8618\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 148s 59ms/step - loss: 0.0437 - accuracy: 0.9923 - ignore_accuracy: 0.9198 - val_loss: 0.0576 - val_accuracy: 0.9874 - val_ignore_accuracy: 0.8692\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 144s 58ms/step - loss: 0.0390 - accuracy: 0.9931 - ignore_accuracy: 0.9280 - val_loss: 0.0540 - val_accuracy: 0.9880 - val_ignore_accuracy: 0.8759\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 140s 56ms/step - loss: 0.0351 - accuracy: 0.9937 - ignore_accuracy: 0.9344 - val_loss: 0.0510 - val_accuracy: 0.9883 - val_ignore_accuracy: 0.8787\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 138s 55ms/step - loss: 0.0316 - accuracy: 0.9942 - ignore_accuracy: 0.9398 - val_loss: 0.0484 - val_accuracy: 0.9888 - val_ignore_accuracy: 0.8838\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 151s 60ms/step - loss: 0.0287 - accuracy: 0.9947 - ignore_accuracy: 0.9452 - val_loss: 0.0462 - val_accuracy: 0.9892 - val_ignore_accuracy: 0.8874\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 152s 61ms/step - loss: 0.0261 - accuracy: 0.9951 - ignore_accuracy: 0.9489 - val_loss: 0.0444 - val_accuracy: 0.9895 - val_ignore_accuracy: 0.8904\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 141s 56ms/step - loss: 0.0240 - accuracy: 0.9954 - ignore_accuracy: 0.9516 - val_loss: 0.0426 - val_accuracy: 0.9897 - val_ignore_accuracy: 0.8928\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 135s 54ms/step - loss: 0.0220 - accuracy: 0.9957 - ignore_accuracy: 0.9548 - val_loss: 0.0412 - val_accuracy: 0.9899 - val_ignore_accuracy: 0.8946\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 123s 49ms/step - loss: 0.0204 - accuracy: 0.9959 - ignore_accuracy: 0.9572 - val_loss: 0.0400 - val_accuracy: 0.9901 - val_ignore_accuracy: 0.8965\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 124s 49ms/step - loss: 0.0190 - accuracy: 0.9962 - ignore_accuracy: 0.9599 - val_loss: 0.0389 - val_accuracy: 0.9903 - val_ignore_accuracy: 0.8985\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 123s 49ms/step - loss: 0.0177 - accuracy: 0.9963 - ignore_accuracy: 0.9617 - val_loss: 0.0380 - val_accuracy: 0.9904 - val_ignore_accuracy: 0.9003\n",
      "Epoch 40/40\n",
      "2504/2504 [==============================] - 127s 51ms/step - loss: 0.0166 - accuracy: 0.9965 - ignore_accuracy: 0.9639 - val_loss: 0.0371 - val_accuracy: 0.9906 - val_ignore_accuracy: 0.9017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f30aa45f08>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T18:59:42.023795Z",
     "start_time": "2020-07-17T18:59:21.224386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 20s 25ms/step\n",
      "'accuracy: 99.08385276794434'\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f'{model.metrics_names[1]}: {scores[1] * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T19:14:26.991048Z",
     "start_time": "2020-07-17T19:14:26.897151Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('models/my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
