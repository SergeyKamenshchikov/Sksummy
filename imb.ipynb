{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:18:26.899875Z",
     "start_time": "2020-11-16T17:17:53.860192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wikipediaapi\n",
    "import string\n",
    "\n",
    "import fuzzy_pandas as fpd\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime as dt\n",
    "    \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "from docx.shared import Pt\n",
    "import concurrent.futures\n",
    "from flask import Flask, render_template, request\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt\n",
    "import docx\n",
    "import time\n",
    "\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "from reportlab import platypus\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tika import parser\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "import requests\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from requests import get\n",
    "import random\n",
    "import html\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from random import randint\n",
    "import winsound\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:18:27.028527Z",
     "start_time": "2020-11-16T17:18:26.902862Z"
    }
   },
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "def text_normalize(txt):\n",
    "    processed_text = (re.sub('[^a-zA-Z]', ' ', txt)).lower()\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "\n",
    "def create_graph(text, common):\n",
    "    tokens = text_normalize(text)\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(common), columns=['bigram', 'count'])\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "    F = nx.Graph()\n",
    "    for k, v in d[0].items():\n",
    "            F.add_edge(k[0], k[1], weight=(v*10))\n",
    "            pos = nx.spring_layout(F, iterations=500)\n",
    "    return F, pos\n",
    "\n",
    "def graph_keys(final_text, top_number):\n",
    "    F,pos = create_graph(final_text.lower(), top_number)\n",
    "    nodes = []\n",
    "    degree = []\n",
    "\n",
    "    for i in F.nodes():\n",
    "        nodes.append(i)\n",
    "        degree.append(F.degree(i))\n",
    "\n",
    "    x = dict(zip(nodes, degree))\n",
    "    key_nodes = list({k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)})\n",
    "    tokens = text_normalize(final_text.lower())\n",
    "\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(100), columns=['bigram', 'count'])\n",
    "    key_bigrams = [' '.join(i) for i in list(bigram_df['bigram'])]\n",
    "\n",
    "    keys = [i for i in key_bigrams if (((i.split()[0] in key_nodes) or (i.split()[1] in key_nodes)) and (i.split()[0]!=i.split()[1]))][:top_number]\n",
    "    key_bigrams = ', '.join(keys)\n",
    "\n",
    "    return key_bigrams\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_min_num_of_clauses(spacy_sentence, n):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
    "            result.append(token.head.text)\n",
    "    if len(result)>=n:\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_sents_ids_whitelist(spacy_sents):\n",
    "    whitelist=[]\n",
    "    i=1\n",
    "    sents_texts=[]\n",
    "    for sent in spacy_sents:\n",
    "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
    "            whitelist.append(i)\n",
    "            sents_texts.append(sent.text)\n",
    "        i=i+1\n",
    "    return(whitelist)\n",
    "\n",
    "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
    "    sents_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        sent_tokens=[]\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if (token.i not in blacklist_tokens):\n",
    "                    sent_tokens.append(token.text)\n",
    "            sents_tokens.append(sent_tokens)\n",
    "            sent_tokens=[]\n",
    "\n",
    "        n=n+1\n",
    "    return(sents_tokens)\n",
    "\n",
    "def detokenizer(list_of_tokens):\n",
    "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
    "    return(text_str)\n",
    "\n",
    "def sentence_grammar_fix(sentences):\n",
    "    fixed=[]\n",
    "    for sent in sentences:\n",
    "\n",
    "        sent=sent.strip()\n",
    "        sent=sent.replace('\\n','')\n",
    "        sent=sent.replace('()','')\n",
    "\n",
    "        sent=re.sub('\\s+',' ',sent)\n",
    "        sent=sent+'.'\n",
    "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0] in ['.',',','-','—']:\n",
    "                sent=sent[1:]\n",
    "        sent=sent.strip()\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0].isalpha():\n",
    "                sent=sent[0].upper()+sent[1:]\n",
    "        fixed.append(sent)\n",
    "\n",
    "    return(fixed)\n",
    "\n",
    "def parse_arxiv(query, D1, D2): \n",
    "    arxivtext = ''\n",
    "    urls = []\n",
    "    titles = []\n",
    "\n",
    "    closest_value = 200\n",
    "    \n",
    "    req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value)+'&searchtype=all&source=header&start=0'\n",
    "    req = req + '&date-from_date=' + str(D1) + 'date-to_date=' + str(D2) + '&order=-announced_date_first' \n",
    "    htmlString = get(req)\n",
    "\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib')\n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "\n",
    "    titles = list(soup.find_all('p', {'class' : 'title is-5 mathjax'}))[:page_number]\n",
    "    titles_r = [i.text.replace('\\n','').replace('  ','') for i in titles]\n",
    "    titles = ', '.join(titles_r)\n",
    "\n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])\n",
    "\n",
    "    txt = []\n",
    "    for i in urls[:page_number]:\n",
    "        time.sleep(random.randint(1,8))\n",
    "        soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "        abstract = ' '.join(soup.find('blockquote').text.replace('  ',' ').split())\n",
    "        txt.append(abstract)\n",
    "\n",
    "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, urls, titles_r)), columns=['text','link', 'page'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_page(url,tag,cls=''): \n",
    "    \n",
    "    htmlString = get(url).text\n",
    "    soup = BeautifulSoup(htmlString, 'html.parser')\n",
    "    paragraphs = soup.find(tag,cls)\n",
    "    txt = text_from_html(str(paragraphs))\n",
    "\n",
    "    return txt\n",
    "\n",
    "def get_entities(rawtext, entities_number):\n",
    "    spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
    "    doc = spacy_nlp(rawtext)\n",
    "\n",
    "    ners = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PERSON']:\n",
    "            ners.append(ent.text)\n",
    "\n",
    "    ners = dict(Counter(ners))\n",
    "    ners = sorted(ners.items(), key=lambda x: x[1], reverse=True)\n",
    "    ners = list(frozenset([i[0] for i in ners][:entities_number]))\n",
    "\n",
    "    return ners\n",
    "\n",
    "def google_urls(query, page_number):   \n",
    "    \n",
    "    # load driver   \n",
    "    driver_location = \"chrome/chromedriver.exe\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--lang=en')\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "    driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "    #/load driver\n",
    "\n",
    "    # get urls\n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1) + '&lr=lang_en'\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []    \n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = list(frozenset(links))[:(page_number)]\n",
    "    #/ get urls\n",
    "    \n",
    "    # stop driver\n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "    #/stop driver\n",
    "    \n",
    "    return url_list \n",
    "\n",
    "def parse_urls(url_list, tag, cls_on, cls=''):\n",
    "    \n",
    "    txt = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "    \n",
    "    # load content \n",
    "    for j in tqdm(url_list):   \n",
    "        \n",
    "        time.sleep(randint(1,5))\n",
    "        \n",
    "        # load driver   \n",
    "        driver_location = \"chrome/chromedriver.exe\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--lang=en')\n",
    "        options.add_argument(\"--incognito\")\n",
    "        options.add_argument(\"--enable-javascript\")\n",
    "    \n",
    "        driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "        #/load driver\n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'): \n",
    "                file_data = parser.from_file(str(j))           \n",
    "                t = file_data['content']\n",
    "            else:   \n",
    "                # get content\n",
    "                driver.get(j)\n",
    "                time.sleep(randint(1,5))\n",
    "                #/get content\n",
    "\n",
    "                # filter content\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                #/filter content\n",
    "                 \n",
    "                if cls_on==True:\n",
    "                    t = text_from_html(str(soup.find(tag, attrs={'class': cls})))\n",
    "                else:\n",
    "                    t = text_from_html(str(soup.find(tag)))\n",
    "                    \n",
    "                txt.append(re.sub('[^A-Za-z0-9.]+', ' ', t))\n",
    "                titles.append(''.join(sent_tokenize(t)[:3]))\n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "            \n",
    "        # stop driver\n",
    "        driver.stop_client()\n",
    "        driver.close()\n",
    "        #/stop driver\n",
    "        \n",
    "        #/load content        \n",
    "    \n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:18:27.040494Z",
     "start_time": "2020-11-16T17:18:27.032517Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'docs/'\n",
    "ALLOWED_EXTENSIONS = set(['pdf', 'docx'])\n",
    "\n",
    "keys_number = 20\n",
    "process_time = 0\n",
    "delta_year = 1\n",
    "\n",
    "wiki_FT = True\n",
    "google_FT = True\n",
    "arxiv_FT = True\n",
    "patent_FT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:19:01.863163Z",
     "start_time": "2020-11-16T17:18:27.044484Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:19:19.169539Z",
     "start_time": "2020-11-16T17:19:04.712180Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_lg', disable=['tagger','parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:19:20.751682Z",
     "start_time": "2020-11-16T17:19:20.730737Z"
    }
   },
   "outputs": [],
   "source": [
    "D1 = (dt.now() - relativedelta(years=delta_year)).strftime('%Y-%m-%d')\n",
    "D2 = dt.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google - limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:27:45.280956Z",
     "start_time": "2020-11-16T17:24:22.415203Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:26<00:00, 14.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keys|: machine algorithm, io android, training data, data mining, machine model, security rule, edit article, data set, deep algorithm, cloud function, use case, decision tree, speech recognition, custom model, algorithm machine, structure data, anomaly detection, remote config, overview io, data science\n",
      "\n",
      "|Entities|: Machine, Google, AI, Andrew Ng, Firebase, Android, Computational, IBM, Deep, ISBN, Software, Microsoft, Firebase ML, ML, Semi, Cloud Firestore, Manage, API, ANN, Machine Learning \n",
      "\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "google_time = 0\n",
    "start = time.time()\n",
    "\n",
    "if google_FT == True: \n",
    "    try: \n",
    "        # parse data\n",
    "        url_list = list(set(google_urls((query + \" before:\" + str(D2) + \" after:\" + str(D1)), 10)))\n",
    "        df_google = parse_urls(url_list, 'body', False)\n",
    "        #/parse data      \n",
    "        \n",
    "        # filter links\n",
    "        exclusions = ['patents.google.com', 'angel.co']\n",
    "        df_google = df_google[~df_google['link'].str.contains('|'.join(exclusions))]\n",
    "        #/filter links\n",
    "        \n",
    "        # find ners\n",
    "        googleners = ', '.join(get_entities('; '.join(list(df_google['text'])), keys_number))\n",
    "        # find ners\n",
    "        \n",
    "        # find keys\n",
    "        googlekeys = graph_keys('; '.join(list(df_google['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)\n",
    "        #/find keys\n",
    "        \n",
    "        # print data\n",
    "        print('|Keys|:', googlekeys)\n",
    "        print('\\n|Entities|:', googleners, '\\n')\n",
    "        #/print data\n",
    "        \n",
    "    except:\n",
    "        print('No data')\n",
    "    \n",
    "    end = time.time()\n",
    "    google_time = end - start\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Angel.co - limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:02:34.695489Z",
     "start_time": "2020-11-16T18:58:37.969815Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                            | 0/9 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:37<00:00, 10.78s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:34<00:00,  9.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keys|: observe ai, enterprise software, language processing, intelligence machine, contact center, intelligence language, food tech, saas enterprise, speech recognition, machine intelligence, machine data, voice automation, product service, quality test, satis ai, address c, software intelligence, processing data, data analytics, analytics data\n",
      "\n",
      "|Entities|: AI First SaaS, Menlo Ventures, AI, Artificial Intelligence Food Tech, Nvidia, Y Combinator Observe, the EU USA, Semiconductors Pharmaceuticals Manufacturing, Root Insurance Alcon Laboratories Tripadvisor, Natural Language Processing NLP, Mobile SaaS B2B Food and Beverages Machine Learning Optimization Technology Mobile Application Food Tech Artificial Intelligence Machine Learning, Enterprise Software Artificial Intelligence Natural Language Processing, Learning Artificial Intelligence, Pearson, SaaS Enterprise Software Analytics Artificial Intelligence Natural Language Processing Speech Recognition Software Call Center Automation Deep Learning, Machine Learning Artificial Intelligence Software Computer, NGP Capital Scale Ventures Nexus Ventures, EINO, Contact Center AI, Savvie \n",
      "\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arxiv_time = 0\n",
    "start = time.time()\n",
    "\n",
    "if arxiv_FT == True:    \n",
    "    try:\n",
    "        # get urls\n",
    "        angel_query = \"site:angel.co/company/ \" + query + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "        url_list = google_urls(angel_query, 10)\n",
    "        \n",
    "        try:\n",
    "            url_list = list(set([('https://angel.co/company/' + i.split('/')[4]) for i in url_list]))\n",
    "        except:\n",
    "            print('')\n",
    "        #/get urls\n",
    "               \n",
    "        # parse data\n",
    "        df_angel_meta = parse_urls(url_list, 'dt', True, 'styles_tags__KR_s2')\n",
    "        df_angel_view = parse_urls(url_list, 'div', True, 'styles_component__2JAFO')\n",
    "        #/parse data  \n",
    "        \n",
    "        # merge data\n",
    "        merged_data = '; '.join(list(df_angel_meta['text'])) + ';' + '; '.join(list(df_angel_view['text'])) \n",
    "        #/merge data\n",
    "        \n",
    "        # find ners \n",
    "        angelners = ', '.join(get_entities(merged_data, keys_number))\n",
    "        # /find ners\n",
    "        \n",
    "        # normalize and find keys\n",
    "        angelkeys = graph_keys(', '.join(text_normalize(merged_data)), keys_number)\n",
    "        #/normalize and find keys\n",
    "        \n",
    "        # print data\n",
    "        print('|Keys|:', angelkeys)\n",
    "        print('\\n|Entities|:', angelners, '\\n')\n",
    "        #/print data\n",
    "        \n",
    "    except:\n",
    "        print('No data')\n",
    "    \n",
    "    end = time.time()\n",
    "    arxiv_time = end - start\n",
    "\n",
    "winsound.Beep(2500, 1000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google Patents - limited: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:55:30.807748Z",
     "start_time": "2020-11-16T18:52:53.075990Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:12<00:00, 12.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: odometer mileage, mobile device, disease recognition, recognition network, set negation, negation keywords, network model, network crop, non transitory, insurance policy, status mobile, ocr process, image watermark, watermark method, computer system, set question, autism classification, classification tool, phrase set, dependency parse\n",
      "\n",
      "Entities: MRI, OCR \n",
      "\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patent_time = 0\n",
    "start = time.time() \n",
    "\n",
    "if patent_FT == True:   \n",
    "    try:        \n",
    "        # get urls\n",
    "        patent_query = \"site:https://patents.google.com \" + query + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "        url_list = list(set(google_urls(patent_query, 10)))\n",
    "        #/get urls\n",
    "        \n",
    "        # parse data\n",
    "        df_patent = parse_urls(url_list, 'abstract', False)\n",
    "        #/parse data\n",
    "\n",
    "        # filter links\n",
    "        inclusions = ['https://patents.google.com']\n",
    "        df_patent = df_patent[df_patent['link'].str.contains('|'.join(inclusions))]\n",
    "        df_patent = df_patent[df_patent['link'].str.contains('/en')]\n",
    "        #/filter links\n",
    "        \n",
    "        # find ners\n",
    "        patentners = ', '.join(get_entities('; '.join(list(df_patent['text'])), keys_number))\n",
    "        #/find ners\n",
    "               \n",
    "        # normalize and find keys\n",
    "        patentkeys = graph_keys('; '.join(list(df_patent['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)\n",
    "        #/normalize and find keys\n",
    "        \n",
    "        print('Keys:', patentkeys)\n",
    "        print('\\nEntities:', patentners, '\\n')\n",
    "        \n",
    "    except:\n",
    "        print('No data')  \n",
    "       \n",
    "    end = time.time()\n",
    "    patent_time = end - start   \n",
    "    \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:59:34.043626Z",
     "start_time": "2020-11-12T19:59:34.022681Z"
    }
   },
   "source": [
    "Create specific keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:24:43.039674Z",
     "start_time": "2020-11-16T19:24:41.686322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific keys: contact center, language processing, insurance policy, watermark method, machine intelligence, set question, processing data, quality test, observe ai, recognition network, voice automation, status mobile, enterprise software, intelligence machine, odometer mileage, intelligence language, data analytics, set negation, image watermark, classification tool, dependency parse, saas enterprise, address c, autism classification, disease recognition, mobile device, analytics data, non transitory, computer system, negation keywords, phrase set, machine data, software intelligence, ocr process, satis ai, network crop, network model, product service, food tech \n",
      "\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "specific = (angelkeys + ', '  + patentkeys).split(', ')\n",
    "known = googlekeys.split(', ')\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(specific)), columns =['specific'])\n",
    "df2 = pd.DataFrame(list(zip(known)), columns =['known'])\n",
    "\n",
    "matches = fpd.fuzzy_merge(df1, df2, left_on=['specific'], right_on=['known'], ignore_case=True, keep='match', \n",
    "                         method='levenshtein', threshold=0.8, join='inner')\n",
    "\n",
    "specific_reqs = list(set(specific).difference(set(list(matches['specific']))))\n",
    "specific_keys = ', '.join(specific_reqs)\n",
    "\n",
    "print('Specific keys:', specific_keys, '\\n')\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for direct match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:37:06.572947Z",
     "start_time": "2020-11-16T19:33:05.303353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [03:58<00:00,  5.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "requests = []\n",
    "volumes = []\n",
    "\n",
    "# load driver   \n",
    "driver_location = \"chrome/chromedriver.exe\"\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--lang=en')\n",
    "options.add_argument(\"--incognito\")\n",
    "options.add_argument(\"--enable-javascript\")\n",
    "    \n",
    "driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "#/load driver\n",
    "\n",
    "for i in tqdm(specific_reqs):\n",
    "    # get urls\n",
    "    query = i + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "    #/get urls\n",
    "                \n",
    "    # get content\n",
    "    google_url = \"https://www.google.com/search?q=\" + i + \"&num=\" + str(11) + '&lr=lang_en'\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "    #/get content\n",
    "\n",
    "    # filter content\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    #/filter content\n",
    "                 \n",
    "    # count occurences\n",
    "    txt = text_from_html(str(soup.find('body')))\n",
    "    txt = re.sub('[^A-Za-z0-9.]+', ' ', txt)\n",
    "    volumes.append(txt.count(i))\n",
    "    #/count occurences\n",
    "            \n",
    "# stop driver\n",
    "driver.stop_client()\n",
    "driver.close()\n",
    "#/stop driver      \n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T16:37:03.105889Z",
     "start_time": "2020-11-13T16:37:03.098907Z"
    }
   },
   "source": [
    "Report dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:58:37.178890Z",
     "start_time": "2020-11-16T19:58:37.136003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requests</th>\n",
       "      <th>volumes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>saas enterprise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>product service</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watermark method</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>set negation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>negation keywords</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ocr process</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>intelligence language</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>address c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>status mobile</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>autism classification</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>observe ai</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>set question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>satis ai</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>voice automation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>intelligence machine</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase set</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quality test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>machine intelligence</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>software intelligence</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>machine data</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 requests  volumes\n",
       "21        saas enterprise        1\n",
       "37        product service        1\n",
       "3        watermark method        1\n",
       "17           set negation        1\n",
       "29      negation keywords        1\n",
       "33            ocr process        1\n",
       "15  intelligence language        1\n",
       "22              address c        1\n",
       "11          status mobile        2\n",
       "23  autism classification        2\n",
       "8              observe ai        2\n",
       "5            set question        2\n",
       "34               satis ai        2\n",
       "10       voice automation        3\n",
       "13   intelligence machine        3\n",
       "30             phrase set        3\n",
       "7            quality test        3\n",
       "4    machine intelligence        3\n",
       "32  software intelligence        4\n",
       "31           machine data        6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report = pd.DataFrame(list(zip(specific_reqs, volumes)), columns =['requests','volumes'])\n",
    "df_report.sort_values(by=['volumes'], ascending=True, inplace=True)\n",
    "df_report.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
