{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T11:51:34.987679Z",
     "start_time": "2020-11-20T11:51:34.982697Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pytrends --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T11:52:12.445565Z",
     "start_time": "2020-11-20T11:51:39.785481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wikipediaapi\n",
    "import string\n",
    "\n",
    "import fuzzy_pandas as fpd\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime as dt\n",
    "    \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "from docx.shared import Pt\n",
    "import concurrent.futures\n",
    "from flask import Flask, render_template, request\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt\n",
    "import docx\n",
    "import time\n",
    "\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "from reportlab import platypus\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tika import parser\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "import requests\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from requests import get\n",
    "import random\n",
    "import html\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from random import randint\n",
    "import winsound\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T11:57:37.092104Z",
     "start_time": "2020-11-20T11:57:36.955470Z"
    }
   },
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "def text_normalize(txt):\n",
    "    processed_text = (re.sub('[^a-zA-Z]', ' ', txt)).lower()\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "\n",
    "def create_graph(text, common):\n",
    "    tokens = text_normalize(text)\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(common), columns=['bigram', 'count'])\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "    F = nx.Graph()\n",
    "    for k, v in d[0].items():\n",
    "            F.add_edge(k[0], k[1], weight=(v*10))\n",
    "            pos = nx.spring_layout(F, iterations=500)\n",
    "    return F, pos\n",
    "\n",
    "def graph_keys(final_text, top_number):\n",
    "    F,pos = create_graph(final_text.lower(), top_number)\n",
    "    nodes = []\n",
    "    degree = []\n",
    "\n",
    "    for i in F.nodes():\n",
    "        nodes.append(i)\n",
    "        degree.append(F.degree(i))\n",
    "\n",
    "    x = dict(zip(nodes, degree))\n",
    "    key_nodes = list({k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)})\n",
    "    tokens = text_normalize(final_text.lower())\n",
    "\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(100), columns=['bigram', 'count'])\n",
    "    key_bigrams = [' '.join(i) for i in list(bigram_df['bigram'])]\n",
    "\n",
    "    keys = [i for i in key_bigrams if (((i.split()[0] in key_nodes) or (i.split()[1] in key_nodes)) and (i.split()[0]!=i.split()[1]))][:top_number]\n",
    "    key_bigrams = ', '.join(keys)\n",
    "\n",
    "    return key_bigrams\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_min_num_of_clauses(spacy_sentence, n):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
    "            result.append(token.head.text)\n",
    "    if len(result)>=n:\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_sents_ids_whitelist(spacy_sents):\n",
    "    whitelist=[]\n",
    "    i=1\n",
    "    sents_texts=[]\n",
    "    for sent in spacy_sents:\n",
    "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
    "            whitelist.append(i)\n",
    "            sents_texts.append(sent.text)\n",
    "        i=i+1\n",
    "    return(whitelist)\n",
    "\n",
    "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
    "    sents_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        sent_tokens=[]\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if (token.i not in blacklist_tokens):\n",
    "                    sent_tokens.append(token.text)\n",
    "            sents_tokens.append(sent_tokens)\n",
    "            sent_tokens=[]\n",
    "\n",
    "        n=n+1\n",
    "    return(sents_tokens)\n",
    "\n",
    "def detokenizer(list_of_tokens):\n",
    "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
    "    return(text_str)\n",
    "\n",
    "def sentence_grammar_fix(sentences):\n",
    "    fixed=[]\n",
    "    for sent in sentences:\n",
    "\n",
    "        sent=sent.strip()\n",
    "        sent=sent.replace('\\n','')\n",
    "        sent=sent.replace('()','')\n",
    "\n",
    "        sent=re.sub('\\s+',' ',sent)\n",
    "        sent=sent+'.'\n",
    "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0] in ['.',',','-','—']:\n",
    "                sent=sent[1:]\n",
    "        sent=sent.strip()\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0].isalpha():\n",
    "                sent=sent[0].upper()+sent[1:]\n",
    "        fixed.append(sent)\n",
    "\n",
    "    return(fixed)\n",
    "\n",
    "def parse_arxiv(query, D1, D2): \n",
    "    arxivtext = ''\n",
    "    urls = []\n",
    "    titles = []\n",
    "\n",
    "    closest_value = 200\n",
    "    \n",
    "    req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value)+'&searchtype=all&source=header&start=0'\n",
    "    req = req + '&date-from_date=' + str(D1) + 'date-to_date=' + str(D2) + '&order=-announced_date_first' \n",
    "    htmlString = get(req)\n",
    "\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib')\n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "\n",
    "    titles = list(soup.find_all('p', {'class' : 'title is-5 mathjax'}))[:page_number]\n",
    "    titles_r = [i.text.replace('\\n','').replace('  ','') for i in titles]\n",
    "    titles = ', '.join(titles_r)\n",
    "\n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])\n",
    "\n",
    "    txt = []\n",
    "    for i in urls[:page_number]:\n",
    "        time.sleep(random.randint(1,8))\n",
    "        soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "        abstract = ' '.join(soup.find('blockquote').text.replace('  ',' ').split())\n",
    "        txt.append(abstract)\n",
    "\n",
    "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, urls, titles_r)), columns=['text','link', 'page'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_page(url,tag,cls=''): \n",
    "    \n",
    "    htmlString = get(url).text\n",
    "    soup = BeautifulSoup(htmlString, 'html.parser')\n",
    "    paragraphs = soup.find(tag,cls)\n",
    "    txt = text_from_html(str(paragraphs))\n",
    "\n",
    "    return txt\n",
    "\n",
    "def get_entities(rawtext, entities_number):\n",
    "    spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
    "    doc = spacy_nlp(rawtext)\n",
    "\n",
    "    ners = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PERSON']:\n",
    "            ners.append(ent.text)\n",
    "\n",
    "    ners = dict(Counter(ners))\n",
    "    ners = sorted(ners.items(), key=lambda x: x[1], reverse=True)\n",
    "    ners = list(frozenset([i[0] for i in ners][:entities_number]))\n",
    "\n",
    "    return ners\n",
    "\n",
    "def google_urls(query, page_number):   \n",
    "    \n",
    "    # load driver   \n",
    "    driver_location = \"chrome/chromedriver.exe\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--lang=en')\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "    driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "    #/load driver\n",
    "\n",
    "    # get urls\n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1) + '&lr=lang_en'\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []    \n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = list(frozenset(links))[:(page_number)]\n",
    "    #/ get urls\n",
    "    \n",
    "    # stop driver\n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "    #/stop driver\n",
    "    \n",
    "    return url_list \n",
    "\n",
    "def parse_urls(url_list, tag, cls_on, cls=''):\n",
    "    \n",
    "    txt = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "    \n",
    "    # load content \n",
    "    for j in tqdm(url_list):   \n",
    "        \n",
    "        time.sleep(randint(1,5))\n",
    "        \n",
    "        # load driver   \n",
    "        driver_location = \"chrome/chromedriver.exe\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--lang=en')\n",
    "        options.add_argument(\"--incognito\")\n",
    "        options.add_argument(\"--enable-javascript\")\n",
    "    \n",
    "        driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "        #/load driver\n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'): \n",
    "                file_data = parser.from_file(str(j))           \n",
    "                t = file_data['content']\n",
    "            else:   \n",
    "                # get content\n",
    "                driver.get(j)\n",
    "                time.sleep(randint(1,5))\n",
    "                #/get content\n",
    "\n",
    "                # filter content\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                #/filter content\n",
    "                 \n",
    "                if cls_on==True:\n",
    "                    t = text_from_html(str(soup.find(tag, attrs={'class': cls})))\n",
    "                else:\n",
    "                    t = text_from_html(str(soup.find(tag)))\n",
    "                    \n",
    "                txt.append(re.sub('[^A-Za-z0-9.]+', ' ', t))\n",
    "                titles.append(''.join(sent_tokenize(t)[:3]))\n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "            \n",
    "        # stop driver\n",
    "        driver.stop_client()\n",
    "        driver.close()\n",
    "        #/stop driver\n",
    "        \n",
    "        #/load content        \n",
    "    \n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T11:57:45.315614Z",
     "start_time": "2020-11-20T11:57:45.309632Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'docs/'\n",
    "ALLOWED_EXTENSIONS = set(['pdf', 'docx'])\n",
    "\n",
    "keys_number = 20\n",
    "process_time = 0\n",
    "delta_year = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T11:57:51.189693Z",
     "start_time": "2020-11-20T11:57:51.184704Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query = input()\n",
    "query = \"machine learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T11:58:09.230089Z",
     "start_time": "2020-11-20T11:57:53.472181Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_lg', disable=['tagger','parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T11:58:09.240001Z",
     "start_time": "2020-11-20T11:58:09.233021Z"
    }
   },
   "outputs": [],
   "source": [
    "D1 = (dt.now() - relativedelta(years=delta_year)).strftime('%Y-%m-%d')\n",
    "D2 = dt.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google - limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:48:48.056477Z",
     "start_time": "2020-11-20T16:39:43.139025Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [05:31<00:00, 16.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keys|: machine algorithm, machine model, data set, linear regression, training data, data science, decision tree, data mining, gradient descent, data scientist, machine system, language machine, machine engineer, support vector, deep network, language processing, ai machine, amount data, machine data, vector machine\n",
      "\n",
      "|Entities|: Kera, Stanford, Fei, Arthur, Cnn, Christopher, Watson, Hinton, Fran, Ann, Ibm, Mathworks, Pmid, Zeeshan, Matlab, Vincent, Robert, Martin, Pierre, Peter \n",
      "\n",
      "Wall time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "google_time = 0\n",
    "start = time.time()\n",
    "\n",
    "googlekeys = ''\n",
    "googleners = ''\n",
    "\n",
    "try: \n",
    "    # parse data\n",
    "    url_list = list(set(google_urls(query, 20)))\n",
    "    df_google = parse_urls(url_list, 'body', False)\n",
    "    #/parse data      \n",
    "        \n",
    "    # filter links\n",
    "    exclusions = ['patents.google.com', 'angel.co']\n",
    "    df_google = df_google[~df_google['link'].str.contains('|'.join(exclusions))]\n",
    "    #/filter links\n",
    "        \n",
    "    # find ners\n",
    "    googleners = ', '.join(get_entities('; '.join(list(df_google['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)).title()\n",
    "    # find ners\n",
    "        \n",
    "    # find keys\n",
    "    googlekeys = graph_keys('; '.join(list(df_google['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)\n",
    "    #/find keys\n",
    "        \n",
    "    # print data\n",
    "    print('|Keys|:', googlekeys)\n",
    "    print('\\n|Entities|:', googleners, '\\n')\n",
    "    #/print data\n",
    "        \n",
    "except:\n",
    "    print('No data')\n",
    "    \n",
    "end = time.time()\n",
    "google_time = end - start\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Angel.co - limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:50.992182Z",
     "start_time": "2020-11-20T12:08:19.069845Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [04:15<00:00, 12.48s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [03:47<00:00, 10.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keys|: language processing, intelligence machine, machine intelligence, observe ai, enterprise software, bit ly, intelligence language, machine data, contact center, health fitness, call center, center automation, software engineering, machine saas, saas enterprise, data intelligence, semiconductor manufacturing, computer vision, intelligence software, phone call\n",
      "\n",
      "|Entities|: Stanford \n",
      "\n",
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arxiv_time = 0\n",
    "start = time.time()\n",
    "\n",
    "angelkeys = ''\n",
    "angelners = ''\n",
    " \n",
    "try:\n",
    "    # get url\n",
    "    angel_query = \"site:angel.co/company/ \" + query + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "    url_list = google_urls(angel_query, 20)\n",
    "    \n",
    "    try:\n",
    "        url_list = list(set([('https://angel.co/company/' + i.split('/')[4]) for i in url_list]))\n",
    "    except:\n",
    "        print(\"Can't simplify url\")\n",
    "    #/get urls \n",
    "        \n",
    "    # parse data\n",
    "    df_angel_meta = parse_urls(url_list, 'dt', True, 'styles_tags__KR_s2')\n",
    "    df_angel_view = parse_urls(url_list, 'div', True, 'styles_component__2JAFO')\n",
    "    #/parse data  \n",
    "        \n",
    "    # merge data\n",
    "    merged_data = '; '.join(list(df_angel_meta['text'])) + ';' + '; '.join(list(df_angel_view['text'])) \n",
    "    #/merge data\n",
    "        \n",
    "    # find ners \n",
    "    angelners = ', '.join(get_entities(', '.join(text_normalize(merged_data)), keys_number)).title()\n",
    "    # /find ners\n",
    "        \n",
    "    # normalize and find keys\n",
    "    angelkeys = graph_keys(', '.join(text_normalize(merged_data)), keys_number)\n",
    "    #/normalize and find keys\n",
    "        \n",
    "    # print data\n",
    "    print('|Keys|:', angelkeys)\n",
    "    print('\\n|Entities|:', angelners, '\\n')\n",
    "    #/print data\n",
    "        \n",
    "except:\n",
    "    print('No data')\n",
    "    \n",
    "end = time.time()\n",
    "arxiv_time = end - start\n",
    "\n",
    "winsound.Beep(2500, 1000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google Patents - limited: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:22:03.718566Z",
     "start_time": "2020-11-20T12:16:50.998165Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [04:44<00:00, 11.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Keys|: af embryo, character picture, verification code, odometer mileage, picture verification, code method, mobile device, plurality utterance, disease recognition, recognition network, method identification, identification feedback, therapy session, embryo female, network model, network crop, gray level, binaryzation image, image noise, noise image\n",
      "\n",
      "|Entities|:  \n",
      "\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patent_time = 0\n",
    "start = time.time() \n",
    " \n",
    "try:        \n",
    "    # get urls\n",
    "    patent_query = \"site:https://patents.google.com \" + query + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "    url_list = list(set(google_urls(patent_query, 20)))\n",
    "    #/get urls\n",
    "        \n",
    "    # parse data\n",
    "    df_patent = parse_urls(url_list, 'abstract', False)\n",
    "    #/parse data\n",
    "\n",
    "    # filter links\n",
    "    inclusions = ['https://patents.google.com']\n",
    "    df_patent = df_patent[df_patent['link'].str.contains('|'.join(inclusions))]\n",
    "    df_patent = df_patent[df_patent['link'].str.contains('/en')]\n",
    "    #/filter links\n",
    "        \n",
    "    # find ners\n",
    "    patentners = ', '.join(get_entities('; '.join(list(df_patent['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)).title()\n",
    "    #/find ners\n",
    "               \n",
    "    # normalize and find keys\n",
    "    patentkeys = graph_keys('; '.join(list(df_patent['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)\n",
    "    #/normalize and find keys\n",
    "        \n",
    "    print('|Keys|:', patentkeys)\n",
    "    print('\\n|Entities|:', patentners, '\\n')\n",
    "        \n",
    "except:\n",
    "    print('No data')  \n",
    "       \n",
    "    end = time.time()\n",
    "    patent_time = end - start   \n",
    "    \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create specific keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:22:05.166050Z",
     "start_time": "2020-11-20T12:22:03.724330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Specific keys|: verification code, intelligence software, saas enterprise, code method, identification feedback, embryo female, software engineering, therapy session, plurality utterance, odometer mileage, network model, data intelligence, enterprise software, health fitness, af embryo, picture verification, observe ai, bit ly, contact center, character picture, phone call, language processing, center automation, disease recognition, intelligence machine, binaryzation image, method identification, noise image, semiconductor manufacturing, image noise, machine intelligence, computer vision, network crop, mobile device, intelligence language, recognition network, call center, machine saas, gray level \n",
      "\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "specific = (angelkeys + ', '  + patentkeys).split(', ')\n",
    "known = googlekeys.split(', ')\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(specific)), columns =['specific'])\n",
    "df2 = pd.DataFrame(list(zip(known)), columns =['known'])\n",
    "\n",
    "matches = fpd.fuzzy_merge(df1, df2, left_on=['specific'], right_on=['known'], ignore_case=True, keep='match', \n",
    "                         method='levenshtein', threshold=0.8, join='inner')\n",
    "\n",
    "specific_reqs = list(set(specific).difference(set(list(matches['specific']))))\n",
    "specific_keys = ', '.join([i for i in specific_reqs if i])\n",
    "\n",
    "print('|Specific keys|:', specific_keys, '\\n')\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:57:48.031387Z",
     "start_time": "2020-11-20T16:52:52.556642Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: use options instead of chrome_options\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "  0%|                                                                                           | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+verification code machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▏                                                                                | 1/39 [00:10<06:20, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+intelligence software machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████▎                                                                              | 2/39 [00:14<05:06,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+saas enterprise machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▍                                                                            | 3/39 [00:17<04:06,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+code method machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▌                                                                          | 4/39 [00:25<04:13,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+identification feedback machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|██████████▋                                                                        | 5/39 [00:49<06:53, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+embryo female machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▊                                                                      | 6/39 [00:55<05:34, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+software engineering machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▉                                                                    | 7/39 [01:04<05:16,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+therapy session machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████████████                                                                  | 8/39 [01:11<04:40,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+plurality utterance machine learning\n",
      "No acceptable volume\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|███████████████████▏                                                               | 9/39 [01:19<04:21,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+odometer mileage machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|█████████████████████                                                             | 10/39 [01:27<04:04,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+network model machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|███████████████████████▏                                                          | 11/39 [01:32<03:32,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+data intelligence machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████████████████▏                                                        | 12/39 [01:40<03:30,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+enterprise software machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████▎                                                      | 13/39 [01:50<03:35,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+health fitness machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████▍                                                    | 14/39 [01:56<03:08,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+af embryo machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                  | 15/39 [02:01<02:48,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+picture verification machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████████████████████▋                                                | 16/39 [02:04<02:13,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+observe ai machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████▋                                              | 17/39 [02:11<02:14,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+bit ly machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████▊                                            | 18/39 [02:18<02:12,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+contact center machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|███████████████████████████████████████▉                                          | 19/39 [02:26<02:14,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+character picture machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|██████████████████████████████████████████                                        | 20/39 [02:29<01:48,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+phone call machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████████████████▏                                     | 21/39 [02:38<01:59,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+language processing machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████████▎                                   | 22/39 [02:45<01:55,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+center automation machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████▎                                 | 23/39 [02:51<01:46,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+disease recognition machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████████▍                               | 24/39 [02:58<01:39,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+intelligence machine machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|████████████████████████████████████████████████████▌                             | 25/39 [03:05<01:33,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+binaryzation image machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████▋                           | 26/39 [03:12<01:29,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+method identification machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████▊                         | 27/39 [03:19<01:23,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+noise image machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████▊                       | 28/39 [03:29<01:25,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+semiconductor manufacturing machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████▉                     | 29/39 [03:33<01:06,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+image noise machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████████████████████████████████████████████████████████████                   | 30/39 [03:42<01:06,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+machine intelligence machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████████████████████████████████████▏                | 31/39 [03:49<00:58,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+computer vision machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████▎              | 32/39 [03:59<00:55,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+network crop machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 33/39 [04:09<00:51,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+mobile device machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|███████████████████████████████████████████████████████████████████████▍          | 34/39 [04:14<00:38,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+intelligence language machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▌        | 35/39 [04:25<00:34,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+recognition network machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 36/39 [04:32<00:23,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+call center machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▊    | 37/39 [04:36<00:14,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+machine saas machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████████▉  | 38/39 [04:43<00:07,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+gray level machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [04:53<00:00,  7.84s/it]\n"
     ]
    }
   ],
   "source": [
    "volumes = []\n",
    "requests = []\n",
    "\n",
    "# load driver   \n",
    "driver_location = \"chrome/chromedriver.exe\"\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--lang=en')\n",
    "options.add_argument(\"--incognito\")\n",
    "options.add_argument(\"--enable-javascript\")\n",
    "driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "#/load driver\n",
    "\n",
    "for j in tqdm(specific_reqs):\n",
    "    \n",
    "    # get urls\n",
    "    google_url = \"https://www.google.com/search?q=\" + 'site:youtube.com ' + str(j) + ' ' + query\n",
    "    print(str('+' + j + ' ' + query))\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "    \n",
    "    requests.append(j)\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source,'html')\n",
    "        volume = re.search('<div id=\"result-stats\">(.*)<nobr>', str(soup.find_all('div', attrs={'class': 'LHJvCe'})[0]))\n",
    "        volume = volume.group(1)\n",
    "        volume = int(re.sub(\"[^0-9]\", \"\", volume))\n",
    "        \n",
    "        volumes.append(volume)\n",
    "    except:\n",
    "        print('No acceptable volume')\n",
    "        volumes.append(0)\n",
    "    \n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "# stop driver\n",
    "driver.stop_client()\n",
    "driver.close()\n",
    "#/stop driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T14:30:22.422034Z",
     "start_time": "2020-11-20T14:30:22.404083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requests</th>\n",
       "      <th>volumes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>plurality utterance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>af embryo</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>binaryzation image</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>odometer mileage</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>observe ai</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>embryo female</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>semiconductor manufacturing</td>\n",
       "      <td>2870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>picture verification</td>\n",
       "      <td>4210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saas enterprise</td>\n",
       "      <td>4440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>disease recognition</td>\n",
       "      <td>4830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identification feedback</td>\n",
       "      <td>5930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>network crop</td>\n",
       "      <td>8460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>noise image</td>\n",
       "      <td>15700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>therapy session</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>machine saas</td>\n",
       "      <td>18800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>image noise</td>\n",
       "      <td>20300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verification code</td>\n",
       "      <td>21500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>method identification</td>\n",
       "      <td>23500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>character picture</td>\n",
       "      <td>32800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>center automation</td>\n",
       "      <td>34700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       requests  volumes\n",
       "8           plurality utterance        0\n",
       "14                    af embryo       85\n",
       "25           binaryzation image      233\n",
       "9              odometer mileage     1270\n",
       "16                   observe ai     1790\n",
       "5                 embryo female     1850\n",
       "28  semiconductor manufacturing     2870\n",
       "15         picture verification     4210\n",
       "2               saas enterprise     4440\n",
       "23          disease recognition     4830\n",
       "4       identification feedback     5930\n",
       "32                 network crop     8460\n",
       "27                  noise image    15700\n",
       "7               therapy session    18000\n",
       "37                 machine saas    18800\n",
       "29                  image noise    20300\n",
       "0             verification code    21500\n",
       "26        method identification    23500\n",
       "19            character picture    32800\n",
       "22            center automation    34700"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report = pd.DataFrame(list(zip(requests, volumes)), columns =['requests','volumes'])\n",
    "df_report.sort_values(by=['volumes'], ascending=True, inplace=True)\n",
    "df_report.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Google trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:56:08.384263Z",
     "start_time": "2020-11-20T15:52:44.371117Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [03:23<00:00,  6.49s/it]\n"
     ]
    }
   ],
   "source": [
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "ques = []\n",
    "rates = []\n",
    "\n",
    "for i in tqdm(list(df_report['requests'])[:10]):\n",
    "    \n",
    "    time.sleep(randint(1,5))\n",
    "    ques.append(i) \n",
    "    pytrends.build_payload([i], cat=0, timeframe='today 3-m',  gprop='')\n",
    "    \n",
    "    try:\n",
    "        df = pytrends.interest_over_time()\n",
    "        del df['isPartial']\n",
    "        df.columns = ['tag']\n",
    "        rates.append(df['tag'][-1] - df['tag'][0])\n",
    "    except:\n",
    "        rates.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show high request tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:57:44.313184Z",
     "start_time": "2020-11-20T15:57:44.292242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requests</th>\n",
       "      <th>rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>image noise</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>noise image</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>recognition network</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>embryo female</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>center automation</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>semiconductor manufacturing</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>network model</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>intelligence language</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>method identification</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gray level</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>health fitness</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>character picture</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>computer vision</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>data intelligence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>intelligence machine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>binaryzation image</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af embryo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plurality utterance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>observe ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>machine saas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       requests  rates\n",
       "15                  image noise     58\n",
       "12                  noise image     58\n",
       "20          recognition network     27\n",
       "5                 embryo female     24\n",
       "19            center automation     19\n",
       "6   semiconductor manufacturing     19\n",
       "38                network model     19\n",
       "26        intelligence language     18\n",
       "17        method identification     15\n",
       "30                   gray level     13\n",
       "23               health fitness     11\n",
       "18            character picture     11\n",
       "29              computer vision      6\n",
       "31            data intelligence      1\n",
       "34         intelligence machine      0\n",
       "2            binaryzation image      0\n",
       "1                     af embryo      0\n",
       "0           plurality utterance      0\n",
       "4                    observe ai      0\n",
       "14                 machine saas      0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report = pd.DataFrame(list(zip(ques, rates)), columns = ['requests','rates'])\n",
    "df_report.sort_values(by=['rates'], ascending=False, inplace=True)\n",
    "df_report.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
