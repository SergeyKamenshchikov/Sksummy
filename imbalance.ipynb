{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:17:48.784639Z",
     "start_time": "2020-11-15T15:17:16.744289Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skamenshchikov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wikipediaapi\n",
    "import string\n",
    "\n",
    "import fuzzy_pandas as fpd\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime as dt\n",
    "    \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "from docx.shared import Pt\n",
    "import concurrent.futures\n",
    "from flask import Flask, render_template, request\n",
    "from docx.enum.dml import MSO_THEME_COLOR_INDEX\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import Pt\n",
    "import docx\n",
    "import time\n",
    "\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import Image\n",
    "from reportlab import platypus\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tika import parser\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "import requests\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from requests import get\n",
    "import random\n",
    "import html\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from random import randint\n",
    "import winsound\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 50000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:18:02.682379Z",
     "start_time": "2020-11-15T15:18:02.529786Z"
    }
   },
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "def text_normalize(txt):\n",
    "    processed_text = (re.sub('[^a-zA-Z]', ' ', txt)).lower()\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    tokens = [i for i in tokens if (tags(i) in ['NN', 'NNP', 'NNS', 'NNPS'])]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def connected_component_subgraphs(G):\n",
    "    for c in nx.connected_components(G):\n",
    "        yield G.subgraph(c)\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    fetched_text = ' '.join(map(lambda p:p.text,soup.find_all('p')))\n",
    "    return fetched_text\n",
    "\n",
    "def create_graph(text, common):\n",
    "    tokens = text_normalize(text)\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(common), columns=['bigram', 'count'])\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "    F = nx.Graph()\n",
    "    for k, v in d[0].items():\n",
    "            F.add_edge(k[0], k[1], weight=(v*10))\n",
    "            pos = nx.spring_layout(F, iterations=500)\n",
    "    return F, pos\n",
    "\n",
    "def graph_keys(final_text, top_number):\n",
    "    F,pos = create_graph(final_text.lower(), top_number)\n",
    "    nodes = []\n",
    "    degree = []\n",
    "\n",
    "    for i in F.nodes():\n",
    "        nodes.append(i)\n",
    "        degree.append(F.degree(i))\n",
    "\n",
    "    x = dict(zip(nodes, degree))\n",
    "    key_nodes = list({k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)})\n",
    "    tokens = text_normalize(final_text.lower())\n",
    "\n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(100), columns=['bigram', 'count'])\n",
    "    key_bigrams = [' '.join(i) for i in list(bigram_df['bigram'])]\n",
    "\n",
    "    keys = [i for i in key_bigrams if (((i.split()[0] in key_nodes) or (i.split()[1] in key_nodes)) and (i.split()[0]!=i.split()[1]))][:top_number]\n",
    "    key_bigrams = ', '.join(keys)\n",
    "\n",
    "    return key_bigrams\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and token.head.pos == VERB:\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_min_num_of_clauses(spacy_sentence, n):\n",
    "    result=[]\n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep_ in ['nsubj','nsubjpass','csubj','expl']) and (token.head.pos_ == 'VERB' or token.head.pos_ == 'AUX'):\n",
    "            result.append(token.head.text)\n",
    "    if len(result)>=n:\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_sents_ids_whitelist(spacy_sents):\n",
    "    whitelist=[]\n",
    "    i=1\n",
    "    sents_texts=[]\n",
    "    for sent in spacy_sents:\n",
    "        if (sent.text not in sents_texts) and check_min_num_of_clauses(sent,1):\n",
    "            whitelist.append(i)\n",
    "            sents_texts.append(sent.text)\n",
    "        i=i+1\n",
    "    return(whitelist)\n",
    "\n",
    "def get_list_sents_tokens(spacy_sents,sents_whitelist,blacklist_tokens):\n",
    "    sents_tokens=[]\n",
    "    n=1\n",
    "    for sent in spacy_sents:\n",
    "        sent_tokens=[]\n",
    "        if (n in sents_whitelist):\n",
    "            for token in sent:\n",
    "                if (token.i not in blacklist_tokens):\n",
    "                    sent_tokens.append(token.text)\n",
    "            sents_tokens.append(sent_tokens)\n",
    "            sent_tokens=[]\n",
    "\n",
    "        n=n+1\n",
    "    return(sents_tokens)\n",
    "\n",
    "def detokenizer(list_of_tokens):\n",
    "    text_str=\"\".join([\" \"+w if not w.startswith(\"'\") and not w.startswith(\"’\") and w!='' and w not in string.punctuation else w for w in list_of_tokens]).strip()\n",
    "    return(text_str)\n",
    "\n",
    "def sentence_grammar_fix(sentences):\n",
    "    fixed=[]\n",
    "    for sent in sentences:\n",
    "\n",
    "        sent=sent.strip()\n",
    "        sent=sent.replace('\\n','')\n",
    "        sent=sent.replace('()','')\n",
    "\n",
    "        sent=re.sub('\\s+',' ',sent)\n",
    "        sent=sent+'.'\n",
    "        sent=re.sub(r'([,.\\-—:])+',r'\\1',sent)\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0] in ['.',',','-','—']:\n",
    "                sent=sent[1:]\n",
    "        sent=sent.strip()\n",
    "\n",
    "        if len(sent)>1:\n",
    "            if sent[0].isalpha():\n",
    "                sent=sent[0].upper()+sent[1:]\n",
    "        fixed.append(sent)\n",
    "\n",
    "    return(fixed)\n",
    "\n",
    "def parse_arxiv(query, D1, D2): \n",
    "    arxivtext = ''\n",
    "    urls = []\n",
    "    titles = []\n",
    "\n",
    "    closest_value = 200\n",
    "    \n",
    "    req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value)+'&searchtype=all&source=header&start=0'\n",
    "    req = req + '&date-from_date=' + str(D1) + 'date-to_date=' + str(D2) + '&order=-announced_date_first' \n",
    "    htmlString = get(req)\n",
    "\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib')\n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "\n",
    "    titles = list(soup.find_all('p', {'class' : 'title is-5 mathjax'}))[:page_number]\n",
    "    titles_r = [i.text.replace('\\n','').replace('  ','') for i in titles]\n",
    "    titles = ', '.join(titles_r)\n",
    "\n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])\n",
    "\n",
    "    txt = []\n",
    "    for i in urls[:page_number]:\n",
    "        time.sleep(random.randint(1,8))\n",
    "        soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "        abstract = ' '.join(soup.find('blockquote').text.replace('  ',' ').split())\n",
    "        txt.append(abstract)\n",
    "\n",
    "    arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', '; '.join(txt))\n",
    "    df = pd.DataFrame(list(zip(txt, urls, titles_r)), columns=['text','link', 'page'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_page(url,tag,cls=''): \n",
    "    \n",
    "    htmlString = get(url).text\n",
    "    soup = BeautifulSoup(htmlString, 'html.parser')\n",
    "    paragraphs = soup.find(tag,cls)\n",
    "    txt = text_from_html(str(paragraphs))\n",
    "\n",
    "    return txt\n",
    "\n",
    "def get_entities(rawtext, entities_number):\n",
    "    spacy_nlp = spacy.load('en_core_web_lg', disable=[\"tagger\",\"parser\"])\n",
    "    doc = spacy_nlp(rawtext)\n",
    "\n",
    "    ners = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PERSON']:\n",
    "            ners.append(ent.text)\n",
    "\n",
    "    ners = dict(Counter(ners))\n",
    "    ners = sorted(ners.items(), key=lambda x: x[1], reverse=True)\n",
    "    ners = list(frozenset([i[0] for i in ners][:entities_number]))\n",
    "\n",
    "    return ners\n",
    "\n",
    "def google_urls(query, page_number):   \n",
    "    \n",
    "    # load driver   \n",
    "    driver_location = \"chrome/chromedriver.exe\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--lang=en')\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "    driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "    #/load driver\n",
    "\n",
    "    # get urls\n",
    "    google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(page_number+1) + '&lr=lang_en'\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "    links = []    \n",
    "    for r in result_div:\n",
    "        try:\n",
    "            link = r.find('a', href=True)\n",
    "            title = None\n",
    "            title = r.find('h3')\n",
    "\n",
    "            if isinstance(title,Tag):\n",
    "                title = title.get_text()\n",
    "\n",
    "            description = None\n",
    "            description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "            if isinstance(description, Tag):\n",
    "                description = description.get_text()\n",
    "\n",
    "            if link != '' and title != '' and description != '':\n",
    "                links.append(link['href'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    url_list = list(frozenset(links))[:(page_number)]\n",
    "    #/ get urls\n",
    "    \n",
    "    # stop driver\n",
    "    driver.stop_client()\n",
    "    driver.close()\n",
    "    #/stop driver\n",
    "    \n",
    "    return url_list \n",
    "\n",
    "def parse_urls(url_list, tag, cls_on, cls=''):\n",
    "    \n",
    "    txt = []\n",
    "    titles = []\n",
    "    errors = []\n",
    "    \n",
    "    # load content \n",
    "    for j in tqdm(url_list):   \n",
    "        \n",
    "        time.sleep(randint(1,5))\n",
    "        \n",
    "        # load driver   \n",
    "        driver_location = \"chrome/chromedriver.exe\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--lang=en')\n",
    "        options.add_argument(\"--incognito\")\n",
    "        options.add_argument(\"--enable-javascript\")\n",
    "    \n",
    "        driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "        #/load driver\n",
    "        \n",
    "        try:  \n",
    "            if str(j).endswith('.pdf'): \n",
    "                file_data = parser.from_file(str(j))           \n",
    "                t = file_data['content']\n",
    "            else:   \n",
    "                # get content\n",
    "                driver.get(j)\n",
    "                time.sleep(randint(1,5))\n",
    "                #/get content\n",
    "\n",
    "                # filter content\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                #/filter content\n",
    "                 \n",
    "                if cls_on==True:\n",
    "                    t = text_from_html(str(soup.find(tag, attrs={'class': cls})))\n",
    "                else:\n",
    "                    t = text_from_html(str(soup.find(tag)))\n",
    "                    \n",
    "                txt.append(re.sub('[^A-Za-z0-9.]+', ' ', t))\n",
    "                titles.append(''.join(sent_tokenize(t)[:3]))\n",
    "        except:\n",
    "            print('Parsing error:',str(j))\n",
    "            errors.append(str(j))\n",
    "            \n",
    "        # stop driver\n",
    "        driver.stop_client()\n",
    "        driver.close()\n",
    "        #/stop driver\n",
    "        \n",
    "        #/load content        \n",
    "    \n",
    "    df = pd.DataFrame(list(zip(txt, url_list, titles)), columns=['text','link', 'page'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:18:24.316267Z",
     "start_time": "2020-11-15T15:18:24.310281Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'docs/'\n",
    "ALLOWED_EXTENSIONS = set(['pdf', 'docx'])\n",
    "\n",
    "page_number = 20\n",
    "keys_number = 20\n",
    "\n",
    "process_time = 0\n",
    "delta_year = 1\n",
    "\n",
    "wiki_FT = True\n",
    "google_FT = True\n",
    "arxiv_FT = True\n",
    "patent_FT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:18:32.035966Z",
     "start_time": "2020-11-15T15:18:27.353580Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:18:56.088549Z",
     "start_time": "2020-11-15T15:18:37.442982Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_lg', disable=['tagger','parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:19:00.415798Z",
     "start_time": "2020-11-15T15:19:00.401834Z"
    }
   },
   "outputs": [],
   "source": [
    "D1 = (dt.now() - relativedelta(years=delta_year)).strftime('%Y-%m-%d')\n",
    "D2 = dt.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google - limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:25:11.237375Z",
     "start_time": "2020-11-15T15:19:07.736945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [04:32<00:00, 13.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: data science, machine algorithm, use case, machine model, training data, machine application, data set, data scientist, interview question, learn machine, course machine, decision tree, video lecture, linear regression, computer science, towards data, ml algorithm, machine course, ai machine, specialization course\n",
      "\n",
      "Entities: Semi, Coursera, IBM, Andrew Ng, AI Course, Big Data, DL, ML, Deutsch Espa, Google, YouTube Stores, Deep, AI, Machine, Machine Learning, MATLAB, RL, NXP, Fran, Artificial Intelligence \n",
      "\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "google_time = 0\n",
    "start = time.time()\n",
    "\n",
    "if google_FT == True: \n",
    "    try: \n",
    "        # parse data\n",
    "        url_list = list(set(google_urls((query + \" before:\" + str(D2) + \" after:\" + str(D1)), page_number)))\n",
    "        df_google = parse_urls(url_list, 'body', False)\n",
    "        #/parse data      \n",
    "        \n",
    "        # filter links\n",
    "        exclusions = ['patents.google.com', 'angel.co']\n",
    "        df_google = df_google[~df_google['link'].str.contains('|'.join(exclusions))]\n",
    "        #/filter links\n",
    "        \n",
    "        # find ners\n",
    "        googleners = ', '.join(get_entities('; '.join(list(df_google['text'])), keys_number))\n",
    "        # find ners\n",
    "        \n",
    "        # find keys\n",
    "        googlekeys = graph_keys('; '.join(list(df_google['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)\n",
    "        #/find keys\n",
    "        \n",
    "        # print data\n",
    "        print('Keys:', googlekeys)\n",
    "        print('\\nEntities:', googleners, '\\n')\n",
    "        #/print data\n",
    "        \n",
    "    except:\n",
    "        print('No data')\n",
    "    \n",
    "    end = time.time()\n",
    "    google_time = end - start\n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Angel.co - limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:45:59.786076Z",
     "start_time": "2020-11-15T15:26:00.027066Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                           | 0/91 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [19:30<00:00, 12.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: computer vision, language processing, ai operating, operating system, platform data, ai ml, vision platform, platform use, use deep, deep train, train computer, computer play, play game, game marketing, marketing economy, economy predict, predict order, order pickup, pickup delivery, delivery pizzeria\n",
      "\n",
      "Entities: AI ML, Natural Language Processing to Automate Routine Work, Climate Informatics, Quality Prediciton in Process Industry, Powering Intelligent Industrial Workspaces, Ecommerce, Making Factories Smarter, The Automation Company, Forestry ML, Deep Ge, Listnr, ML, Simplifying Healthcare for Patients and Doctors, AI and Machine Learning, Deep, Revolutions, AI, Machine Learning, AC, Intelligent Natural Language Processing \n",
      "\n",
      "Wall time: 19min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arxiv_time = 0\n",
    "start = time.time()\n",
    "\n",
    "if arxiv_FT == True:    \n",
    "    try:\n",
    "        # get urls\n",
    "        angel_query = \"site:angel.co/company/ \" + query + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "        url_list = google_urls(angel_query, page_number*10)\n",
    "        url_list = list(set([('https://angel.co/company/' + i.split('/')[4]) for i in url_list]))\n",
    "        #/get urls\n",
    "               \n",
    "        # parse data\n",
    "        df_angel = parse_urls(url_list, 'h3', True, 'styles_component__1kg4S styles_flow__3_K06 styles_highConcept__1eLSa __halo_fontSizeMap_size--xl __halo_fontWeight_medium')\n",
    "        #/parse data  \n",
    "        \n",
    "        # filter links\n",
    "        inclusions = ['angel.co']\n",
    "        df_angel = df_angel[df_angel['link'].str.contains('|'.join(inclusions))]\n",
    "        #/filter links\n",
    "        \n",
    "        # find ners\n",
    "        angelners = ', '.join(get_entities('; '.join(list(df_angel['text'])), keys_number))\n",
    "        # /find ners\n",
    "        \n",
    "        # normalize and find keys\n",
    "        angelkeys = graph_keys('; '.join(list(df_angel['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)\n",
    "        #/normalize and find keys\n",
    "        \n",
    "        print('Keys:', angelkeys)\n",
    "        print('\\nEntities:', angelners, '\\n')\n",
    "        \n",
    "    except:\n",
    "        print('No data')\n",
    "    \n",
    "    end = time.time()\n",
    "    arxiv_time = end - start\n",
    "\n",
    "winsound.Beep(2500, 1000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google Patents - limited: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T16:12:15.046087Z",
     "start_time": "2020-11-15T15:48:31.282653Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: DeprecationWarning: use options instead of chrome_options\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:298: DeprecationWarning: use options instead of chrome_options\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [23:17<00:00, 15.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: generate link, link comment, system method, method generate, comment method, system generate, comment generate, comment system, comment kind, comment character, data generate, thereof generate, error detection, function generate, pronunciation generate, comment device, risk generate, information processing, comment management, management channel\n",
      "\n",
      "Entities: Improved PSNR Peak Signal, Pyramid, brca1, LTE, VMOS, Multimodal, ARPA Auto Radar, Shaped, Method, Program, Device, IPTV, Compositions, OCR, Target, Phenotypic, Slotted CSMA CA, System \n",
      "\n",
      "Wall time: 23min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patent_time = 0\n",
    "start = time.time() \n",
    "\n",
    "if patent_FT == True:   \n",
    "    try:        \n",
    "        # get urls\n",
    "        patent_query = \"site:https://patents.google.com \" + query + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "        url_list = list(set(google_urls(patent_query, page_number*10)))\n",
    "        #/get urls\n",
    "        \n",
    "        # parse data\n",
    "        df_patent = parse_urls(url_list, 'h1', True, 'scroll-target style-scope patent-result')\n",
    "        #/parse data\n",
    "\n",
    "        # filter links\n",
    "        inclusions = ['https://patents.google.com']\n",
    "        df_patent = df_patent[df_patent['link'].str.contains('|'.join(inclusions))]\n",
    "        df_patent = df_patent[df_patent['link'].str.contains('/en')]\n",
    "        #/filter links\n",
    "        \n",
    "        # find ners\n",
    "        patentners = ', '.join(get_entities('; '.join(list(df_patent['text'])), keys_number))\n",
    "        #/find ners\n",
    "               \n",
    "        # normalize and find keys\n",
    "        patentkeys = graph_keys('; '.join(list(df_patent['text'].apply(lambda x: ', '.join(text_normalize(x))))), keys_number)\n",
    "        #/normalize and find keys\n",
    "        \n",
    "        print('Keys:', patentkeys)\n",
    "        print('\\nEntities:', patentners, '\\n')\n",
    "        \n",
    "    except:\n",
    "        print('No data')  \n",
    "       \n",
    "    end = time.time()\n",
    "    patent_time = end - start   \n",
    "    \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T19:59:34.043626Z",
     "start_time": "2020-11-12T19:59:34.022681Z"
    }
   },
   "source": [
    "Create specific keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T16:15:31.725554Z",
     "start_time": "2020-11-15T16:15:30.375900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific keys: information processing, operating system, predict order, comment management, use deep, order pickup, economy predict, generate link, train computer, language processing, comment generate, comment device, comment method, pronunciation generate, system generate, platform use, risk generate, pickup delivery, vision platform, platform data, delivery pizzeria, thereof generate, error detection, method generate, computer play, play game, marketing economy, function generate, link comment, ai operating, deep train, data generate, management channel, ai ml, system method, computer vision, comment system, comment character, game marketing, comment kind \n",
      "\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "specific = (angelkeys + ', '  + patentkeys).split(', ')\n",
    "known = googlekeys.split(', ')\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(specific)), columns =['specific'])\n",
    "df2 = pd.DataFrame(list(zip(known)), columns =['known'])\n",
    "\n",
    "matches = fpd.fuzzy_merge(df1, df2, left_on=['specific'], right_on=['known'], ignore_case=True, keep='match', \n",
    "                         method='levenshtein', threshold=0.8, join='inner')\n",
    "\n",
    "specific_reqs = list(set(specific).difference(set(list(matches['specific']))))\n",
    "specific_keys = ', '.join(specific_reqs)\n",
    "\n",
    "print('Specific keys:', specific_keys, '\\n')\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for direct match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-15T16:16:09.765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      " 18%|██████████████▌                                                                    | 7/40 [00:39<03:12,  5.84s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "requests = []\n",
    "volumes = []\n",
    "\n",
    "# load driver   \n",
    "driver_location = \"chrome/chromedriver.exe\"\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--lang=en')\n",
    "options.add_argument(\"--incognito\")\n",
    "options.add_argument(\"--enable-javascript\")\n",
    "    \n",
    "driver = webdriver.Chrome(executable_path=driver_location, chrome_options=options)\n",
    "#/load driver\n",
    "\n",
    "for i in tqdm(specific_reqs):\n",
    "    # get urls\n",
    "    query = i + \" before:\" + str(D2) + \" after:\" + str(D1)\n",
    "    #/get urls\n",
    "                \n",
    "    # get content\n",
    "    google_url = \"https://www.google.com/search?q=\" + i + \"&num=\" + str(page_number+1) + '&lr=lang_en'\n",
    "    driver.get(google_url)\n",
    "    time.sleep(randint(1,5))\n",
    "    #/get content\n",
    "\n",
    "    # filter content\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    #/filter content\n",
    "                 \n",
    "    txt = text_from_html(str(soup.find('body')))\n",
    "    txt = re.sub('[^A-Za-z0-9.]+', ' ', txt)\n",
    "    volumes.append(txt.count(i))\n",
    "            \n",
    "# stop driver\n",
    "driver.stop_client()\n",
    "driver.close()\n",
    "#/stop driver      \n",
    "    \n",
    "winsound.Beep(2500, 1000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T16:37:03.105889Z",
     "start_time": "2020-11-13T16:37:03.098907Z"
    }
   },
   "source": [
    "Report dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-13T19:57:57.561Z"
    }
   },
   "outputs": [],
   "source": [
    "df_report = pd.DataFrame(list(zip(requests, volumes)), columns =['requests','volumes'])\n",
    "df_report.sort_values(by=['volumes'], ascending=True, inplace=True)\n",
    "df_report.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
