{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T11:54:53.368587Z",
     "start_time": "2020-08-29T11:54:31.256993Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from random import randint\n",
    "from wordcloud import WordCloud\n",
    "from tika import parser\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import Cm\n",
    "\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "from reportlab.platypus import Image\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import Table, TableStyle\n",
    "\n",
    "from gensim.summarization import mz_keywords\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from bs4.element import Tag\n",
    "\n",
    "from pathlib import Path\n",
    "import wikipediaapi\n",
    "import concurrent.futures\n",
    "\n",
    "import winsound\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from requests import get\n",
    "from googlesearch import search\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "from bs4.element import Comment\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "import time\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.symbols import nsubj, nsubjpass, VERB, AUX \n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T11:55:03.807581Z",
     "start_time": "2020-08-29T11:55:03.722806Z"
    }
   },
   "outputs": [],
   "source": [
    "def wiki_scrape(topic_name, verbose=True):\n",
    "    def wiki_link(link):\n",
    "        try:\n",
    "            page = wiki_api.page(link)\n",
    "            if page.exists():\n",
    "                d = {'page': link, 'text': page.text, 'link': page.fullurl,\n",
    "                     'categories': list(page.categories.keys())}\n",
    "                return d\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    wiki_api = wikipediaapi.Wikipedia(language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(topic_name)\n",
    "    if not page_name.exists():\n",
    "        return\n",
    "    page_links = list(page_name.links.keys())\n",
    "    progress = tqdm(desc='Links Scraped', unit='', total=len(page_links)) if verbose else None\n",
    "    sources = [{'page': topic_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
    "                'categories': list(page_name.categories.keys())}]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_link = {executor.submit(wiki_link, link): link for link in page_links}\n",
    "        for future in concurrent.futures.as_completed(future_link):\n",
    "            data = future.result()\n",
    "            progress.update(1) if verbose else None\n",
    "            if data:\n",
    "                sources.append(data)\n",
    "    progress.close() if verbose else None\n",
    "    blacklist = ('Template', 'Help:', 'Category:', 'Portal:', 'Wikipedia:', 'Talk:')\n",
    "    sources = pd.DataFrame(sources)\n",
    "    sources = sources[(len(sources['text']) > 20)\n",
    "                      & ~(sources['page'].str.startswith(blacklist))]\n",
    "    sources['categories'] = sources.categories.apply(lambda x: [y[9:] for y in x])\n",
    "    sources['topic'] = topic_name\n",
    "    return sources\n",
    "\n",
    "def wiki_page(page_name):\n",
    "    wiki_api = wikipediaapi.Wikipedia(language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(page_name)\n",
    "    if not page_name.exists():\n",
    "        print('page does not exist')\n",
    "        return\n",
    "    page_data = {'page': page_name, 'text': page_name.text, 'link': page_name.fullurl,\n",
    "                 'categories': [[y[9:] for y in list(page_name.categories.keys())]]}\n",
    "    page_data_df = pd.DataFrame(page_data)\n",
    "    return page_data_df\n",
    "\n",
    "def tags(x):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(x))[0][1]\n",
    "\n",
    "def scoring(a,b):\n",
    "    try:\n",
    "        a = a.replace(',', '').split()\n",
    "        b = b.replace(',', '').split()\n",
    "        score = int(100*len(set(a).intersection(b))/(len(set(a))))\n",
    "    \n",
    "    except:\n",
    "        score = 0    \n",
    "        \n",
    "    return score\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_normalize(txt, nouns=True):\n",
    "    processed_text = re.sub('[^a-zA-Z]', ' ', txt)\n",
    "    processed_text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",processed_text)\n",
    "    processed_text=re.sub(\"(\\\\d|\\\\W)+\",\" \",processed_text)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(processed_text) \n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n",
    "    \n",
    "    if nouns==True: \n",
    "        tokens = [i for i in tokens if ((tags(i) in ['NN', 'NNP', 'NNS', 'NNPS']) and len(i) > 3)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "\n",
    "def word_cloud(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    bigrams_list = list(nltk.bigrams(tokens))\n",
    "    dictionary = [' '.join(tup) for tup in bigrams_list]\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "    bag_of_words = vectorizer.fit_transform(dictionary)\n",
    "    words_freq = [(word, bag_of_words.sum(axis=0)[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    wordcloud = WordCloud(stopwords=stop_words, width=300, height=300, random_state=42, max_words=50, prefer_horizontal=1, \n",
    "                          background_color='white', max_font_size=256)\n",
    "    wordcloud.generate_from_frequencies(dict(words_freq))  \n",
    "    keys = wordcloud.words_\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig('reports/wordcloud.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return keys \n",
    "    \n",
    "def create_graph(text, common=200):\n",
    "    tokens = text_normalize(text)\n",
    "    \n",
    "    bigrams=list(nltk.ngrams(tokens, 2))\n",
    "    bigram_counts = collections.Counter(bigrams)\n",
    "    bigram_df = pd.DataFrame(bigram_counts.most_common(common), columns=['bigram', 'count'])\n",
    "    d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "    F = nx.Graph()\n",
    "    for k, v in d[0].items():\n",
    "            F.add_edge(k[0], k[1], weight=(v*10))\n",
    "\n",
    "            pos = nx.spring_layout(F, iterations=500)\n",
    "    return F, pos\n",
    "\n",
    "def parse_page(url):\n",
    "    htmlString = get(url).text\n",
    "    soup = BeautifulSoup(htmlString, 'html.parser')\n",
    "    paragraphs = soup.find_all(['a'])\n",
    "    txt = text_from_html(str(paragraphs))\n",
    "\n",
    "    return txt\n",
    "\n",
    "def get_unique_text(document):\n",
    "    unique_sentences = []\n",
    "    for sentence in [sent.raw for sent in TextBlob(document).sentences]:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def syntax_full(spacy_sentence):\n",
    "    result=[] \n",
    "    for token in spacy_sentence:\n",
    "        if (token.dep == nsubj or token.dep == nsubjpass) and (token.head.pos == VERB or token.head.pos == AUX):\n",
    "            result.append(token.head)\n",
    "    if result:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def tree_graph(d):\n",
    "    valency_list=[d.get(element) for element in d] #list of valency values\n",
    "    unique_v_list=list(set(valency_list)) #unique valency values\n",
    "\n",
    "    unique_v_list.reverse()\n",
    "\n",
    "    N=5 #number of rows\n",
    "    T=5 #top words\n",
    "\n",
    "    v_step=[1/N*step for step in range(0,N)] #list of borders for v values\n",
    "    v_step.reverse()\n",
    "\n",
    "    d={k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)} #sort d\n",
    "    levels=[]\n",
    "    used=[]\n",
    "    for step in v_step:\n",
    "        level=[]\n",
    "        for k in d:\n",
    "            if d[k]/max(unique_v_list)>step and k not in used:\n",
    "                level.append(k)\n",
    "                used.append(k)\n",
    "        levels.append(level)\n",
    "    levels=[level for level in levels if level]\n",
    "\n",
    "    top_levels=[]\n",
    "    for level in levels:\n",
    "        if len(level)>T:\n",
    "            top_levels.append(level[:T])\n",
    "        else:\n",
    "            top_levels.append(level)\n",
    "\n",
    "    levels=top_levels\n",
    "\n",
    "    gaps=[len(level)-1 for level in levels] #count number of gaps between two words on each level\n",
    "\n",
    "    lines=[] #get list of strings of every level\n",
    "    for level in levels:\n",
    "        level_line=''\n",
    "        for w in level:\n",
    "            level_line=level_line+w\n",
    "        lines.append(level_line)\n",
    "\n",
    "    longest_symbol_row=max([len(line) for line in lines])+max(gaps)+2 \n",
    "    ax_step=1/longest_symbol_row #X pos step in the graph\n",
    "\n",
    "    s=5\n",
    "    fig = plt.figure(facecolor='#EBF5FB')\n",
    "    fig.set_size_inches((s+3), s) #set size in inches\n",
    "\n",
    "    ay_step=1/(len(levels)) #Y pos step in the graph\n",
    "    i=0\n",
    "    h_lines_y=[]\n",
    "    colors=['#D6EAF8','#AED6F1','#85C1E9','#5DADE2','#3498DB']\n",
    "    \n",
    "    for level in levels:\n",
    "        level_line='' #get length of level\n",
    "        for w in level:\n",
    "            level_line=level_line+w\n",
    "        l_step=(longest_symbol_row-len(level_line))/2-gaps[i]/2  #step from the left to start placing words\n",
    "\n",
    "        h_lines_y.append(1-(i*ay_step))\n",
    "        color=colors.pop()\n",
    "        for w in level:\n",
    "            plt.text(l_step*ax_step+len(w)/2*ax_step, 1-ay_step/2-(i*ay_step), w, fontdict=dict(fontsize=12), \n",
    "                     family='monospace', horizontalalignment='center', verticalalignment='center', \n",
    "                     bbox=dict(facecolor=color, edgecolor='black', boxstyle='round,pad=0.2'))\n",
    "            l_step=l_step+len(w)+1\n",
    "        i=i+1\n",
    "\n",
    "    h_lines_y.append(0)\n",
    "    for y in h_lines_y:\n",
    "      if y==max(h_lines_y) or y==min(h_lines_y):\n",
    "        plt.axhline(y=y, color='black')\n",
    "      else:\n",
    "        plt.axhline(y=y, color='black', ls='dashed')\n",
    "   \n",
    "    plt.axvline(x=0,ymin=min(h_lines_y), ymax=max(h_lines_y), color='black')\n",
    "    plt.axvline(x=1,ymin=min(h_lines_y), ymax=max(h_lines_y), color='black')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig('reports/graph.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T11:55:59.726258Z",
     "start_time": "2020-08-29T11:55:03.809573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp\\tmpuvz68amb\\config.json as plain json\n",
      "Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n"
     ]
    }
   ],
   "source": [
    "en_stopwords = stopwords.words('english') #load stopwords\n",
    "predictor = Predictor.from_path('./coref-spanbert-large-2020.02.27.tar.gz') #predictor for clustering\n",
    "top_number = 20 #keywords\n",
    "number = 20 #pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:10:06.466328Z",
     "start_time": "2020-08-29T15:10:04.727453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text summarization\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "\n",
    "wikitext = ''\n",
    "googletext = ''\n",
    "arxivtext = ''\n",
    "\n",
    "urls = []\n",
    "url_list = []\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:10:40.779668Z",
     "start_time": "2020-08-29T15:10:18.173720Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Links Scraped: 100%|███████████████████████████████████████████████████████████████████| 130/130 [00:20<00:00,  6.24/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Wiki documents: 20\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = wiki_scrape(query, verbose=True)[:number]\n",
    "    df = df[['text','link']]\n",
    "    wikitext = '; '.join(list(df['text'])) \n",
    "except:\n",
    "    wikitext = 'no text found'\n",
    "    print('Text not found')\n",
    "    \n",
    "print('Number of Wiki documents:', len(df)) \n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:12:01.518562Z",
     "start_time": "2020-08-29T15:11:09.976776Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skamenshchikov\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: use options instead of chrome_options\n",
      "  after removing the cwd from sys.path.\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [00:37<00:04,  2.15s/it]2020-08-29 18:11:55,737 [MainThread  ] [INFO ]  Retrieving https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/lecture-notes/lec25.pdf to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/courses-electrical-engineering-and-computer-science-6-864-advanced-natural-language-processing-fall-2005-lecture-notes-lec25.pdf.\n",
      "INFO:tika.tika:Retrieving https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/lecture-notes/lec25.pdf to C:\\Users\\SKAMEN~1\\AppData\\Local\\Temp/courses-electrical-engineering-and-computer-science-6-864-advanced-natural-language-processing-fall-2005-lecture-notes-lec25.pdf.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:43<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of web pages: 20\n",
      "Errors: []\n"
     ]
    }
   ],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(number+1)\n",
    "driver.get(google_url)\n",
    "time.sleep(randint(1,5))\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "result_div = soup.find_all('div', attrs={'class': 'g'})\n",
    "\n",
    "links = []\n",
    "titles = []\n",
    "errors = []\n",
    "\n",
    "descriptions = []\n",
    "for r in result_div:\n",
    "    try:\n",
    "        link = r.find('a', href=True)\n",
    "        title = None\n",
    "        title = r.find('h3')\n",
    "\n",
    "        if isinstance(title,Tag):\n",
    "            title = title.get_text()\n",
    "\n",
    "        description = None\n",
    "        description = r.find('span', attrs={'class': 'st'})\n",
    "\n",
    "        if isinstance(description, Tag):\n",
    "            description = description.get_text()\n",
    "\n",
    "        if link != '' and title != '' and description != '':\n",
    "            links.append(link['href'])\n",
    "            titles.append(title)\n",
    "            descriptions.append(description)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "url_list = links[:(number)]\n",
    "\n",
    "txt = []\n",
    "for j in tqdm(url_list):\n",
    "    delta = random.randint(1,1)\n",
    "    time.sleep(delta)\n",
    "    try:  \n",
    "        if str(j).endswith('.pdf'): \n",
    "            file_data = parser.from_file(str(j))           \n",
    "            t = file_data['content']\n",
    "        else:\n",
    "        t = parse_page(j)\n",
    "        txt.append(t)\n",
    "    except:\n",
    "        print('Parsing error:',str(j))\n",
    "        errors.append(str(j))\n",
    "       \n",
    "summ = '; '.join(txt)\n",
    "googletext = re.sub('[^A-Za-z0-9.]+', ' ', summ)\n",
    "\n",
    "print('Number of web pages:', len(url_list))\n",
    "print('Errors:', errors)\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Arxiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:59.581856Z",
     "start_time": "2020-08-29T12:05:09.667557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:45<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of web pages: 20\n"
     ]
    }
   ],
   "source": [
    "closest_value = 200\n",
    "req = 'https://arxiv.org/search/?query='+query+'&size='+str(closest_value)+'&searchtype=all&source=header&start=0'\n",
    "htmlString = get(req)\n",
    "\n",
    "try:\n",
    "    soup = BeautifulSoup(htmlString.content, 'html5lib') \n",
    "    hrefs = soup.find_all('a', {'href': re.compile(r'arxiv.org/abs/')})\n",
    "    \n",
    "    if (len(hrefs) > 0):\n",
    "        for i in hrefs:\n",
    "            urls.append(i['href'])    \n",
    "\n",
    "except:\n",
    "    print ('Connection error')\n",
    "    \n",
    "txt = []\n",
    "for i in tqdm(urls[:number]):\n",
    "    time.sleep(random.randint(1,8))\n",
    "    soup = BeautifulSoup(get(str(i)).content, 'html5lib')\n",
    "    abstract = soup.find('blockquote').text.replace('\\n','').replace('  ',' ')\n",
    "    abstract = ' '.join(abstract.split())\n",
    "    txt.append(abstract)\n",
    "         \n",
    "summ = '; '.join(txt)\n",
    "arxivtext = re.sub('[^A-Za-z0-9.]+', ' ', summ)\n",
    "print('Number of web pages:', len(urls[:number]))\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:12:22.134064Z",
     "start_time": "2020-08-29T15:12:22.128080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 40\n"
     ]
    }
   ],
   "source": [
    "docs_length = len(df) + len(url_list) + len(urls)\n",
    "print('Number of documents:',docs_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:12:24.538222Z",
     "start_time": "2020-08-29T15:12:24.533233Z"
    }
   },
   "outputs": [],
   "source": [
    "final_text = (wikitext + ' ' + googletext + ' ' + arxivtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:12:27.813928Z",
     "start_time": "2020-08-29T15:12:27.563469Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_length = [len(i.split()) for i in nltk.sent_tokenize(final_text)] \n",
    "lb = q1 -(1.5 * (np.percentile(sent_length, [75]) - np.percentile(sent_length, [25]))) \n",
    "ub = q3 +(1.5 * (np.percentile(sent_length, [75]) - np.percentile(sent_length, [25]))) \n",
    "\n",
    "filtered_text = [i for i in nltk.sent_tokenize(final_text) if len(i.split()) < lb or len(i.split()) > ub] \n",
    "final_text = ' '.join(filtered_text).replace('\\n','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacement of reference links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T10:16:39.430071Z",
     "start_time": "2020-08-29T10:16:39.426082Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "span_dict=predictor.predict(document=final_text)\n",
    "words=span_dict['document']\n",
    "original_document=span_dict['document']\n",
    "clusters=span_dict['clusters']\n",
    "\n",
    "for cluster in clusters:\n",
    "    first_element=cluster[0]\n",
    "    head=original_document[first_element[0]:first_element[1]+1] #get head of cluster\n",
    "    if len(head)==1 and head[0].lower() in en_stopwords: #check if pronoun\n",
    "        pass\n",
    "    else:\n",
    "        for element in cluster:\n",
    "            for index in range(element[0],element[1]+1):\n",
    "                words[index]='' #make all elements empty to save indexes of document words\n",
    "        for element in cluster:\n",
    "            words[element[0]]=\"\".join([\" \"+w if not w.startswith(\"'\") and w not in string.punctuation else w for w in head]).strip() #add head as a first word of element\n",
    "\n",
    "final_text=\"\".join([\" \"+w if not w.startswith(\"'\") and w!='' and w not in string.punctuation else w for w in words]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show text size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:12:32.013007Z",
     "start_time": "2020-08-29T15:12:32.008010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text size: 119 mb\n"
     ]
    }
   ],
   "source": [
    "print('Text size:', int(len(final_text.encode('utf-8'))/1024),'mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:12:43.528007Z",
     "start_time": "2020-08-29T15:12:35.234561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1476/1476 [00:00<00:00, 30202.97it/s]\n"
     ]
    }
   ],
   "source": [
    "final_text = get_unique_text(final_text)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 50000000\n",
    "\n",
    "doc = nlp(final_text)\n",
    "valid_sents=[]\n",
    "\n",
    "for s in tqdm(list(doc.sents)):\n",
    "    if syntax_full(s):\n",
    "        valid_sents.append(s.text)\n",
    "\n",
    "final_text = ' '.join(valid_sents)\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph based keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:15:47.496974Z",
     "start_time": "2020-08-29T15:12:50.286496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "displaystyle number (19%), cluster distance (13%), data science (12%), article deep (8%), sentence compression (8%), summarization text (5%), word model (4%), index bouldin (4%), algorithm produce (4%), centroid triangle (4%), june reply (3%), language processing (3%), cream child (3%), question test (3%), matrix entry (3%), recognition speech (3%), object belongs (3%), measure evaluation (3%), machine translation (3%), analysis component (3%)\n"
     ]
    }
   ],
   "source": [
    "F,pos = create_graph(final_text.lower())\n",
    "\n",
    "nodes = []\n",
    "degree = []\n",
    "\n",
    "for i in F.nodes():   \n",
    "    nodes.append(i)\n",
    "    degree.append(F.degree(i))\n",
    "\n",
    "key_bigrams = []\n",
    "birams_score = []\n",
    "bigram_vocab = []\n",
    "\n",
    "x = dict(zip(nodes, degree))\n",
    "major_nodes = list({k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)})\n",
    "\n",
    "for i in major_nodes:\n",
    "    if i!=list(F.neighbors(i))[0] and (i not in bigram_vocab and list(F.neighbors(i))[0] not in bigram_vocab):\n",
    "        key_bigrams.append(str(i+' '+list(F.neighbors(i))[0]))\n",
    "        birams_score.append(F.degree[i])\n",
    "        bigram_vocab = set((' '.join(key_bigrams)).split())\n",
    "        \n",
    "key_bigrams = key_bigrams[:top_number]\n",
    "birams_score = birams_score[:top_number]\n",
    "\n",
    "bigram_dict = dict(zip(key_bigrams, birams_score))\n",
    "normed_keys = []\n",
    "\n",
    "for i in range(len(bigram_dict.items())):\n",
    "    normed_keys.append(''.join(str(list(bigram_dict.keys())[i])+' ('+str(list(bigram_dict.values())[i])+'%)'))\n",
    "\n",
    "key_bigrams = ', '.join(normed_keys)\n",
    "print('Keywords:\\n')\n",
    "print(key_bigrams)\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:18:00.607684Z",
     "start_time": "2020-08-29T15:17:50.922933Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 3.7 %\n",
      "\n",
      "* Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n",
      "* Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.\n",
      "* For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n",
      "* Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express.\n",
      "* Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n",
      "* The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).\n",
      "* Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
      "* An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
      "* Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).\n",
      "* At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.\n",
      "* Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\n",
      "* For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\n",
      "* In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\".\n",
      "* They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.\n",
      "* For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words.\n",
      "* Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc.\n",
      "* While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.\n",
      "* Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.\n",
      "* For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.\n",
      "* Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases.\n",
      "* This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references.\n",
      "* Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.\n",
      "* Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.\n",
      "* The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\n",
      "* While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths).\n",
      "* It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\n",
      "* Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.\n",
      "* While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\n",
      "* Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.\n",
      "* There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks.\n",
      "* Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.A new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed.\n",
      "* This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\n",
      "* The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.\n",
      "* Another example of a submodular optimization problem is using a determinantal point process to model diversity.\n",
      "* For example, work by Lin and Bilmes, 2012 shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.\n",
      "* For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.\n",
      "* The main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models.\n",
      "* Some of this knowledge is in the form of facts that can be explicitly represented, but some knowledge is unconscious and closely tied to the human body: for example, the machine may need to understand how an ocean makes one feel to accurately translate a specific metaphor in the text.\n",
      "* In a second, narrower sense, the term anaphora denotes the act of referring backwards in a dialog or text, such as referring to the left when an anaphor points to its left toward its antecedent in languages that are written from left to right.\n",
      "* Similarly, in discussing 'The Mayor' (of a city), the Mayor's identity must be understood broadly through the context which the speech references as general 'object' of understanding; is a particular human person meant, a current or future or past office-holder, the office in a strict legal sense, or the office in a general sense which includes activities a mayor might conduct, might even be expected to conduct, while they may not be explicitly defined for this office.\n",
      "* Usually between 100 and 200 words, the informative abstract summarizes the paper's structure, its major topics and key points.\n",
      "* Some of the major criticisms of the study have been that five of the eight datasets consisted of paragraphs rather than essays, four of the eight data sets were graded by human readers for content only rather than for writing ability, and that rather than measuring human readers and the AES machines against the \"true score\", the average of the two readers' scores, the study employed an artificial construct, the \"resolved score\", which in four datasets consisted of the higher of the two human scores if there was a disagreement.\n",
      "* The program evaluates surface features of the text of each essay, such as the total number of words, the number of subordinate clauses, or the ratio of uppercase to lowercase letters—quantities that can be measured without any human insight.\n",
      "* The various AES programs differ in what specific surface features they measure, how many essays are required in the training set, and most significantly in the mathematical modeling technique.\n",
      "* The bag-of-words model has also been used for computer vision.The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.An early reference to \"bag of words\" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.\n",
      "* The most common type of characteristics, or features calculated from the Bag-of-words model is term frequency, namely, the number of times a term appears in the text.\n",
      "* The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.\n",
      "* The creation and implementation of chatbots is still a developing area, heavily related to artificial intelligence and machine learning, so the provided solutions, while possessing obvious advantages, have some important limitations in terms of functionalities and use cases.\n",
      "* Cluster analysis itself is not one specific algorithm, but the general task to be solved.\n",
      "* Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions.\n",
      "* The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results.\n",
      "* Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure.\n",
      "* Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς \"grape\"), typological analysis, and community detection.\n",
      "* Centroid models: for example, the k-means algorithm represents each cluster by a single mean vector.\n",
      "* Density models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space.\n",
      "* These algorithms connect \"objects\" to form \"clusters\" based on their distance.\n",
      "* Connectivity-based clustering is a whole family of methods that differ by the way distances are computed.\n",
      "* Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance) to use.\n",
      "* These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters.\n",
      "* In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set.\n",
      "* When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the k cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.\n",
      "* Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (k-means++) or allowing a fuzzy cluster assignment (fuzzy c-means).\n",
      "* Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid.\n",
      "* Centroid-based clustering problems such as k-means and k-medoids are special cases of the uncapacitated, metric facility location problem, a canonical problem in the operations research and computational geometry communities.\n",
      "* This makes it possible to apply the well-developed algorithmic solutions from the facility location literature to the presently considered centroid-based clustering problem.\n",
      "* A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution.\n",
      "* On data sets with, for example, overlapping Gaussian distributions – a common use case in artificial data – the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously.\n",
      "* On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.\n",
      "* With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing.\n",
      "* Popular approaches involve \"internal\" evaluation, where the clustering is summarized to a single quality score, \"external\" evaluation, where the clustering is compared to an existing \"ground truth\" classification, \"manual\" evaluation by a human expert, and \"indirect\" evaluation by evaluating the utility of the clustering in its intended application.Internal evaluation measures suffer from the problem that they represent functions that themselves can be seen as a clustering objective.\n",
      "* For example, one could cluster the data set by the Silhouette coefficient; except that there is no known efficient algorithm for this.\n",
      "* By using such an internal measure for evaluation, one rather compares the similarity of the optimization problems, and not necessarily how useful the clustering is.\n",
      "* External evaluation has similar problems: if we have such \"ground truth\" labels, then we would not need to cluster; and in practical applications we usually do not have such labels.\n",
      "* One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications.\n",
      "* Additionally, this evaluation is biased towards algorithms that use the same cluster model.\n",
      "* For example, k-means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering.\n",
      "* On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.\n",
      "* For example, the following methods can be used to assess the quality of clustering algorithms based on internal criterion:\n",
      "In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster.As with internal evaluation, several external evaluation measures exist, for example:\n",
      "Now take the sum over all clusters and divide by the total number of data points.\n",
      "* So for example, a purity score of 1 is possible by putting each data point in its own cluster.\n",
      "* Market researchers use cluster analysis to partition the general population of consumers into market segments and to better understand the relationships between different groups of consumers/potential customers, and for use in market segmentation, product positioning, new product development and selecting test markets.\n",
      "* In the process of intelligent grouping of the files and websites, clustering may be used to create a more relevant set of search results compared to normal search engines like Google.\n",
      "* Determining the number of clusters in a data set\n",
      "NPO(V): :The class of NPO problems with polynomial-time algorithms approximating the optimal solution by a ratio bounded by some function on n.\n"
     ]
    }
   ],
   "source": [
    "readtime = 1\n",
    "sent_num = int(readtime/(np.median([len(i.split()) for i in nltk.sent_tokenize(final_text)])*0.005))\n",
    "\n",
    "ratio = sent_num/len(nltk.sent_tokenize(final_text))\n",
    "txt = summarize(wikitext, ratio=ratio)\n",
    "\n",
    "print('Compression:', round(100*ratio,1), '%\\n')\n",
    "[print('* ' + i) for i in nltk.sent_tokenize(txt)]\n",
    "\n",
    "winsound.Beep(2500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PDF report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:18:08.793648Z",
     "start_time": "2020-08-29T15:18:08.423635Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_name = 'reports/summary.pdf'\n",
    "\n",
    "short_summary = '<br/><br/>'.join([('* ' + i) for i in nltk.sent_tokenize(txt)])\n",
    "\n",
    "sp = ParagraphStyle('parrafos', alignment=TA_JUSTIFY, fontSize=12, fontName=\"Times-Roman\")\n",
    "headers = ParagraphStyle('parrafos', alignment=TA_CENTER, fontSize=14, fontName=\"Times-Roman\")\n",
    "doc = SimpleDocTemplate(pdf_name, pagesize=letter, bottomMargin=.4 * inch, topMargin=.6 * inch, rightMargin=.8 * inch, leftMargin=.8 * inch)\n",
    "\n",
    "hd = Paragraph('<b>Summary</b>', headers)\n",
    "P = Paragraph(short_summary, sp)\n",
    "kw = Paragraph(('\\n<b>Keywords</b>: ' + str(key_bigrams) + '.'), sp)\n",
    "rq = Paragraph(('\\n<br /><b>Request</b>: ' + str(query)), sp)\n",
    "pn = Paragraph(('\\n<br /><b>Number of documents:</b>: ' + str(docs_length)), sp)\n",
    "ts = Paragraph(('\\n<br /><b>Text size:</b> ' + str(int(len(final_text.encode('utf-8'))/1024))) + ' mb', sp)\n",
    "rq = Paragraph(('\\n<br /><b>Number of pages</b>: ' + str(query)), sp)\n",
    "cmp = Paragraph(('\\n<br /><b>Number of sentences</b>: ' + str(sent_num)), sp)\n",
    "\n",
    "rs = Paragraph(('\\n<br /><b>Resources</b>: ' + str('Wiki, Arxiv, Google')), sp)\n",
    "space = Paragraph('<br /><br />', sp)\n",
    "\n",
    "catalog = []\n",
    "img_hist = Image('reports/hist.png', 3.25*inch, 6.3*inch)\n",
    "\n",
    "catalog.append(hd)\n",
    "catalog.append(rq)\n",
    "catalog.append(rs)\n",
    "catalog.append(pn)\n",
    "catalog.append(cmp)\n",
    "catalog.append(ts)\n",
    "catalog.append(space)\n",
    "catalog.append(kw)\n",
    "catalog.append(space)\n",
    "catalog.append(P)\n",
    "\n",
    "doc.build(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DOCX report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T15:18:13.570858Z",
     "start_time": "2020-08-29T15:18:13.486940Z"
    }
   },
   "outputs": [],
   "source": [
    "short_summary = '\\n'.join([('* ' + i + '\\n') for i in nltk.sent_tokenize(txt)])\n",
    "\n",
    "doc = Document()\n",
    "hd = doc.add_paragraph()\n",
    "hd.add_run('Summary').bold = True\n",
    "hd = doc.add_paragraph(re.sub(r'[^\\x00-\\x7F]+|\\x0c',' ', ('Request: ' + str(query))))\n",
    "hd = doc.add_paragraph(re.sub(r'[^\\x00-\\x7F]+|\\x0c',' ', ('Resources: ' + str('Wiki, ' + 'Arxiv, ' + 'Google'))))\n",
    "hd = doc.add_paragraph(re.sub(r'[^\\x00-\\x7F]+|\\x0c',' ', ('Number of documents: ' + str(docs_length))))\n",
    "hd = doc.add_paragraph(re.sub(r'[^\\x00-\\x7F]+|\\x0c',' ', ('Text size: ' + str(int(len(final_text.encode('utf-8'))/1024)) + ' mb')))\n",
    "hd = doc.add_paragraph(re.sub(r'[^\\x00-\\x7F]+|\\x0c',' ', ('Keywords: ' + str(key_bigrams))))\n",
    "\n",
    "hd.alignment = 3\n",
    "\n",
    "r = hd.add_run()\n",
    "par = doc.add_paragraph(re.sub(r'[^\\x00-\\x7F]+|\\x0c',' ', (short_summary.replace('.','. '))))\n",
    "par.alignment = 0\n",
    "\n",
    "doc.save('reports/summary.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
